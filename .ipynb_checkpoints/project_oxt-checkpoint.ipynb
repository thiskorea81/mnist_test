{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************처음에 한 번만 실행해야 함.********************************#\n",
    "# 이미지를 하나씩 잘라서 폴더(img)에 저장                                 #\n",
    "# 형식: %s/oxt_000  : 0폴더, 1폴더, 2폴더 데이터를 각 폴더에 저장         #\n",
    "# 데이터 뻥튀기(좌우대칭, 상하대칭, 좌우상하대칭, +-5도, +-3도 회전) 8배   #\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from dataset.oxt import load_oxt\n",
    "\n",
    "# 디렉토리가 없으면 생성하는 함수\n",
    "def createDirectory(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")\n",
    "\n",
    "# img 디렉토리 생성        \n",
    "createDirectory('./img')\n",
    "\n",
    "# img 내 0, 1, 2 디렉토리 생성\n",
    "for i in range(3):\n",
    "    dir='./img/%d'%(i)\n",
    "    createDirectory(dir)\n",
    "\n",
    "# 노멀라이즈되지 않은 이미지를 불러오기\n",
    "(x_train, t_train), (x_test, t_test)=load_oxt(normalize=False)\n",
    "\n",
    "train_size=len(x_train)\n",
    "test_size=len(x_test)\n",
    "total=train_size+test_size\n",
    "\n",
    "# 원본 트레인, 테스트 사이즈 출력\n",
    "print(train_size, test_size, total)\n",
    "\n",
    "for i in range(train_size):\n",
    "    img = x_train[i]\n",
    "    img = img.reshape(28, 28)  # 형상을 원래 이미지의 크기로 변형\n",
    "    img = ~img\n",
    "    fname=\"img/%s/oxt_%04d.jpg\"%(t_train[i], i)\n",
    "    cv2.imwrite(fname, img)\n",
    "\n",
    "for i in range(test_size):\n",
    "    img = x_test[i]\n",
    "    img = img.reshape(28, 28)  # 형상을 원래 이미지의 크기로 변형\n",
    "    img = ~img\n",
    "    fname=\"img/%s/oxt_%04d.jpg\"%(t_test[i], train_size+i)\n",
    "    cv2.imwrite(fname, img)\n",
    "    \n",
    "\n",
    "# 좌우대칭    \n",
    "for i in range(train_size):\n",
    "    img = x_train[i]\n",
    "    img = img.reshape(28, 28)  # 형상을 원래 이미지의 크기로 변형\n",
    "    img = cv2.flip(img, 1)\n",
    "    img = ~img\n",
    "    fname=\"img/%s/oxt_flip_lr_%04d.jpg\"%(t_train[i], i)\n",
    "    cv2.imwrite(fname, img)\n",
    "\n",
    "for i in range(test_size):\n",
    "    img = x_test[i]\n",
    "    img = img.reshape(28, 28)  # 형상을 원래 이미지의 크기로 변형\n",
    "    img = cv2.flip(img, 1)\n",
    "    img = ~img\n",
    "    fname=\"img/%s/oxt_flip_lr_%04d.jpg\"%(t_test[i], train_size+i)\n",
    "    cv2.imwrite(fname, img)\n",
    "\n",
    "# 상하대칭    \n",
    "for i in range(train_size):\n",
    "    img = x_train[i]\n",
    "    img = img.reshape(28, 28)  # 형상을 원래 이미지의 크기로 변형\n",
    "    img = cv2.flip(img, 0)\n",
    "    img = ~img\n",
    "    fname=\"img/%s/oxt_flip_td_%04d.jpg\"%(t_train[i], i)\n",
    "    cv2.imwrite(fname, img)\n",
    "\n",
    "for i in range(test_size):\n",
    "    img = x_test[i]\n",
    "    img = img.reshape(28, 28)  # 형상을 원래 이미지의 크기로 변형\n",
    "    img = cv2.flip(img, 0)\n",
    "    img = ~img\n",
    "    fname=\"img/%s/oxt_flip_td_%04d.jpg\"%(t_test[i], train_size+i)\n",
    "    cv2.imwrite(fname, img)\n",
    "\n",
    "# 좌우상하대칭    \n",
    "for i in range(train_size):\n",
    "    img = x_train[i]\n",
    "    img = img.reshape(28, 28)  # 형상을 원래 이미지의 크기로 변형\n",
    "    img = cv2.flip(img, -1)\n",
    "    img = ~img\n",
    "    fname=\"img/%s/oxt_flip_ld_%04d.jpg\"%(t_train[i], i)\n",
    "    cv2.imwrite(fname, img)\n",
    "\n",
    "for i in range(test_size):\n",
    "    img = x_test[i]\n",
    "    img = img.reshape(28, 28)  # 형상을 원래 이미지의 크기로 변형\n",
    "    img = cv2.flip(img, -1)\n",
    "    img = ~img\n",
    "    fname=\"img/%s/oxt_flip_ld_%04d.jpg\"%(t_test[i], train_size+i)\n",
    "    cv2.imwrite(fname, img)\n",
    "    \n",
    "# 회전(5도)\n",
    "rot_l = cv2.getRotationMatrix2D((14,14), 5, 1)\n",
    "rot_r = cv2.getRotationMatrix2D((14,14), -5, 1)\n",
    "\n",
    "for i in range(train_size):\n",
    "    img = x_train[i]\n",
    "    img = img.reshape(28, 28)  # 형상을 원래 이미지의 크기로 변형\n",
    "    img = cv2.warpAffine(img, rot_l, (0,0))\n",
    "    img = ~img\n",
    "    fname=\"img/%s/oxt_rot_l5_%04d.jpg\"%(t_train[i], i)\n",
    "    cv2.imwrite(fname, img)\n",
    "\n",
    "for i in range(test_size):\n",
    "    img = x_test[i]\n",
    "    img = img.reshape(28, 28)  # 형상을 원래 이미지의 크기로 변형\n",
    "    img = cv2.warpAffine(img, rot_l, (0,0))\n",
    "    img = ~img\n",
    "    fname=\"img/%s/oxt_rot_l5_%04d.jpg\"%(t_test[i], train_size+i)\n",
    "    cv2.imwrite(fname, img)\n",
    "    \n",
    "\n",
    "# 회전(-5도)\n",
    "for i in range(train_size):\n",
    "    img = x_train[i]\n",
    "    img = img.reshape(28, 28)  # 형상을 원래 이미지의 크기로 변형\n",
    "    img = cv2.warpAffine(img, rot_r, (0,0))\n",
    "    img = ~img\n",
    "    fname=\"img/%s/oxt_rot_r5_%04d.jpg\"%(t_train[i], i)\n",
    "    cv2.imwrite(fname, img)\n",
    "\n",
    "for i in range(test_size):\n",
    "    img = x_test[i]\n",
    "    img = img.reshape(28, 28)  # 형상을 원래 이미지의 크기로 변형\n",
    "    img = cv2.warpAffine(img, rot_r, (0,0))\n",
    "    img = ~img\n",
    "    fname=\"img/%s/oxt_rot_r5_%04d.jpg\"%(t_test[i], train_size+i)\n",
    "    cv2.imwrite(fname, img)\n",
    "    \n",
    "# 회전(3도)\n",
    "rot_l = cv2.getRotationMatrix2D((14,14), 3, 1)\n",
    "rot_r = cv2.getRotationMatrix2D((14,14), -3, 1)\n",
    "\n",
    "for i in range(train_size):\n",
    "    img = x_train[i]\n",
    "    img = img.reshape(28, 28)  # 형상을 원래 이미지의 크기로 변형\n",
    "    img = cv2.warpAffine(img, rot_l, (0,0))\n",
    "    img = ~img\n",
    "    fname=\"img/%s/oxt_rot_l3_%04d.jpg\"%(t_train[i], i)\n",
    "    cv2.imwrite(fname, img)\n",
    "\n",
    "for i in range(test_size):\n",
    "    img = x_test[i]\n",
    "    img = img.reshape(28, 28)  # 형상을 원래 이미지의 크기로 변형\n",
    "    img = cv2.warpAffine(img, rot_l, (0,0))\n",
    "    img = ~img\n",
    "    fname=\"img/%s/oxt_rot_l3_%04d.jpg\"%(t_test[i], train_size+i)\n",
    "    cv2.imwrite(fname, img)\n",
    "    \n",
    "\n",
    "# 회전(-3도)\n",
    "for i in range(train_size):\n",
    "    img = x_train[i]\n",
    "    img = img.reshape(28, 28)  # 형상을 원래 이미지의 크기로 변형\n",
    "    img = cv2.warpAffine(img, rot_r, (0,0))\n",
    "    img = ~img\n",
    "    fname=\"img/%s/oxt_rot_r3_%04d.jpg\"%(t_train[i], i)\n",
    "    cv2.imwrite(fname, img)\n",
    "\n",
    "for i in range(test_size):\n",
    "    img = x_test[i]\n",
    "    img = img.reshape(28, 28)  # 형상을 원래 이미지의 크기로 변형\n",
    "    img = cv2.warpAffine(img, rot_r, (0,0))\n",
    "    img = ~img\n",
    "    fname=\"img/%s/oxt_rot_r3_%04d.jpg\"%(t_test[i], train_size+i)\n",
    "    cv2.imwrite(fname, img)\n",
    "\n",
    "\n",
    "# 각 폴더에 이미지 불러오기    \n",
    "img_0=glob.glob('./img/0/*.jpg')\n",
    "img_1=glob.glob('./img/1/*.jpg')\n",
    "img_2=glob.glob('./img/2/*.jpg')\n",
    "\n",
    "# 뻥튀기 된 이미지 갯수 세기\n",
    "total_img = len(img_0)+len(img_1)+len(img_2)\n",
    "print(\"total: \", total_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44160, 784)\n",
      "(44160, 3)\n",
      "(11040, 784)\n",
      "(11040, 3)\n",
      "train acc, test acc | 0.3333333333333333, 0.3333333333333333\n",
      "train acc, test acc | 0.5326766304347826, 0.5393115942028985\n",
      "train acc, test acc | 0.5703577898550725, 0.5746376811594203\n",
      "train acc, test acc | 0.638971920289855, 0.6350543478260869\n",
      "train acc, test acc | 0.6627717391304347, 0.6590579710144927\n",
      "train acc, test acc | 0.669134963768116, 0.6635869565217392\n",
      "train acc, test acc | 0.681046195652174, 0.6745471014492753\n",
      "train acc, test acc | 0.6874773550724638, 0.679981884057971\n",
      "train acc, test acc | 0.6894701086956522, 0.6798913043478261\n",
      "train acc, test acc | 0.6976675724637681, 0.6894927536231884\n",
      "train acc, test acc | 0.7037364130434782, 0.6933876811594203\n",
      "train acc, test acc | 0.7127264492753623, 0.7021739130434783\n",
      "train acc, test acc | 0.7276721014492754, 0.7185688405797102\n",
      "train acc, test acc | 0.7423233695652174, 0.7313405797101449\n",
      "train acc, test acc | 0.7552536231884058, 0.7446557971014492\n",
      "train acc, test acc | 0.7716711956521739, 0.7617753623188406\n",
      "train acc, test acc | 0.7863903985507247, 0.7773550724637681\n",
      "train acc, test acc | 0.8083333333333333, 0.796286231884058\n",
      "train acc, test acc | 0.8257019927536232, 0.8145833333333333\n",
      "train acc, test acc | 0.8425045289855072, 0.8302536231884058\n",
      "train acc, test acc | 0.8555253623188406, 0.8446557971014492\n",
      "train acc, test acc | 0.867776268115942, 0.8585144927536232\n",
      "train acc, test acc | 0.8765398550724638, 0.8676630434782608\n",
      "train acc, test acc | 0.886028079710145, 0.8794384057971014\n",
      "train acc, test acc | 0.8941802536231884, 0.8871376811594203\n",
      "train acc, test acc | 0.8997282608695653, 0.8931159420289855\n",
      "train acc, test acc | 0.9045516304347826, 0.8977355072463769\n",
      "train acc, test acc | 0.9104619565217391, 0.9016304347826087\n",
      "train acc, test acc | 0.9151041666666667, 0.9085144927536232\n",
      "train acc, test acc | 0.9194067028985508, 0.9125905797101449\n",
      "train acc, test acc | 0.9235054347826087, 0.9154891304347826\n",
      "train acc, test acc | 0.9259510869565217, 0.9181159420289855\n",
      "train acc, test acc | 0.9286458333333333, 0.9221014492753623\n",
      "train acc, test acc | 0.9303668478260869, 0.9233695652173913\n",
      "train acc, test acc | 0.9332653985507247, 0.9252717391304348\n",
      "train acc, test acc | 0.9360280797101449, 0.9280797101449275\n",
      "train acc, test acc | 0.9393795289855073, 0.9304347826086956\n",
      "train acc, test acc | 0.9405570652173914, 0.9321557971014492\n",
      "train acc, test acc | 0.9427762681159421, 0.9344202898550724\n",
      "train acc, test acc | 0.943953804347826, 0.9347826086956522\n",
      "train acc, test acc | 0.9463541666666667, 0.9375\n",
      "train acc, test acc | 0.9475090579710145, 0.9378623188405797\n",
      "train acc, test acc | 0.9493659420289855, 0.9392210144927536\n",
      "train acc, test acc | 0.9499320652173913, 0.9393115942028986\n",
      "train acc, test acc | 0.952536231884058, 0.9426630434782609\n",
      "train acc, test acc | 0.952241847826087, 0.9428442028985508\n",
      "train acc, test acc | 0.9540987318840579, 0.9447463768115942\n",
      "train acc, test acc | 0.9559103260869565, 0.9456521739130435\n",
      "train acc, test acc | 0.956634963768116, 0.9465579710144928\n",
      "train acc, test acc | 0.9581521739130435, 0.947463768115942\n",
      "train acc, test acc | 0.958491847826087, 0.9482789855072464\n",
      "train acc, test acc | 0.9600996376811595, 0.9495471014492753\n",
      "train acc, test acc | 0.9605978260869565, 0.9496376811594203\n",
      "train acc, test acc | 0.9615036231884058, 0.9520833333333333\n",
      "train acc, test acc | 0.9625, 0.951268115942029\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## common.two_layer_net 참조.\n",
    "## 투레이어넷 모델(chap03)\n",
    "\n",
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.img_oxt import load_oxt\n",
    "from common.two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_oxt(one_hot_label=True)\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_test.shape)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=3, weight_init_std=0.01)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 240   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.08604408184391\n",
      "=== epoch:1, train acc:0.39121376811594205, test acc:0.39202898550724635 ===\n",
      "train loss:1.0975094915799863\n",
      "train loss:1.0931172648678484\n",
      "train loss:1.079741123158623\n",
      "train loss:1.087446816386158\n",
      "train loss:1.0766950903807502\n",
      "train loss:1.0814746345318815\n",
      "train loss:1.060058507410045\n",
      "train loss:1.0741507847136988\n",
      "train loss:1.077412281807042\n",
      "train loss:1.0500276777485533\n",
      "train loss:1.0506813029286843\n",
      "train loss:1.038116564683016\n",
      "train loss:1.0329280334488151\n",
      "train loss:1.045886888524662\n",
      "train loss:1.0390954320079824\n",
      "train loss:0.9794344492804666\n",
      "train loss:1.0511454074432878\n",
      "train loss:0.9916197073989896\n",
      "train loss:0.9673027840018238\n",
      "train loss:1.009763331297949\n",
      "train loss:0.9594871101824803\n",
      "train loss:0.9705181894424662\n",
      "train loss:0.945590856909402\n",
      "train loss:0.9218486941195894\n",
      "train loss:0.9755756563877315\n",
      "train loss:0.9313561527098344\n",
      "train loss:0.9082175414840089\n",
      "train loss:0.8545812988654783\n",
      "train loss:0.8584770140688607\n",
      "train loss:0.923945545997988\n",
      "train loss:0.8703690976726618\n",
      "train loss:0.8529686412700556\n",
      "train loss:0.7925962452884403\n",
      "train loss:0.8866377642081318\n",
      "train loss:0.8203299851641782\n",
      "train loss:0.7741153327010578\n",
      "train loss:0.7905780401216467\n",
      "train loss:0.7932578100456052\n",
      "train loss:0.7590223462286892\n",
      "train loss:0.7323411950811919\n",
      "train loss:0.794335996936441\n",
      "train loss:0.7027250888447711\n",
      "train loss:0.7816194021516668\n",
      "train loss:0.7763148756265416\n",
      "train loss:0.7137482096281772\n",
      "train loss:0.7634892510540198\n",
      "train loss:0.7509738590768291\n",
      "train loss:0.7771437843682537\n",
      "train loss:0.7679222264662994\n",
      "train loss:0.8054211428023523\n",
      "train loss:0.8050746860143179\n",
      "train loss:0.650496359203625\n",
      "train loss:0.7513327765765719\n",
      "train loss:0.6288935879016158\n",
      "train loss:0.7019381465129492\n",
      "train loss:0.7446985629669141\n",
      "train loss:0.6808706409374478\n",
      "train loss:0.7332968062160783\n",
      "train loss:0.6835758653481835\n",
      "train loss:0.6096358850999065\n",
      "train loss:0.6887676563747733\n",
      "train loss:0.5943426193535056\n",
      "train loss:0.6341360114264876\n",
      "train loss:0.5853916920147516\n",
      "train loss:0.5397840277540418\n",
      "train loss:0.580682296255038\n",
      "train loss:0.606319411866992\n",
      "train loss:0.6078148413912707\n",
      "train loss:0.7295857040577565\n",
      "train loss:0.5257590587660197\n",
      "train loss:0.5760254529922388\n",
      "train loss:0.6285385849414399\n",
      "train loss:0.5367752221631903\n",
      "train loss:0.5628395791312559\n",
      "train loss:0.5609260813351064\n",
      "train loss:0.5075861541470367\n",
      "train loss:0.6576834236524889\n",
      "train loss:0.5304285133096538\n",
      "train loss:0.46980199178350673\n",
      "train loss:0.5962787881464019\n",
      "train loss:0.5401189831941725\n",
      "train loss:0.5032952216270675\n",
      "train loss:0.5063982067853897\n",
      "train loss:0.5254591640761771\n",
      "train loss:0.44639566994004304\n",
      "train loss:0.5924222357161846\n",
      "train loss:0.5000904461071533\n",
      "train loss:0.47617917991597003\n",
      "train loss:0.5228289888201209\n",
      "train loss:0.4946141433257442\n",
      "train loss:0.559854743616322\n",
      "train loss:0.5456800738149333\n",
      "train loss:0.4136949663066446\n",
      "train loss:0.46600969435858974\n",
      "train loss:0.3752157552652716\n",
      "train loss:0.4536825838475474\n",
      "train loss:0.37139143537471825\n",
      "train loss:0.41280636585606406\n",
      "train loss:0.4226687359781542\n",
      "train loss:0.47034972567785155\n",
      "train loss:0.5327031709458553\n",
      "train loss:0.3803064093272311\n",
      "train loss:0.39943854817845914\n",
      "train loss:0.39883450652519575\n",
      "train loss:0.46046268048886946\n",
      "train loss:0.36768349060453454\n",
      "train loss:0.3442444681113797\n",
      "train loss:0.30700784025519906\n",
      "train loss:0.3535404849796866\n",
      "train loss:0.38355557389627204\n",
      "train loss:0.3963943597484019\n",
      "train loss:0.3954483975041022\n",
      "train loss:0.39596918471525\n",
      "train loss:0.31745438778282614\n",
      "train loss:0.41532473486631005\n",
      "train loss:0.38743779322131144\n",
      "train loss:0.2870946999523279\n",
      "train loss:0.4418190301943081\n",
      "train loss:0.39783320756761525\n",
      "train loss:0.37885466802633055\n",
      "train loss:0.3622531994182659\n",
      "train loss:0.3431022816038226\n",
      "train loss:0.3715007281164819\n",
      "train loss:0.3034371024550733\n",
      "train loss:0.2743977421322323\n",
      "train loss:0.35757043724191306\n",
      "train loss:0.25022347774891435\n",
      "train loss:0.31742112852486276\n",
      "train loss:0.27927083264646685\n",
      "train loss:0.2750475575861968\n",
      "train loss:0.33765688480486383\n",
      "train loss:0.2525039560848536\n",
      "train loss:0.27037810454255473\n",
      "train loss:0.3546864555879177\n",
      "train loss:0.22267927158051248\n",
      "train loss:0.3512153622733452\n",
      "train loss:0.3725910141695031\n",
      "train loss:0.23219724951817047\n",
      "train loss:0.3207590383601622\n",
      "train loss:0.2436607863316907\n",
      "train loss:0.33515275590313975\n",
      "train loss:0.27199016799321635\n",
      "train loss:0.31993831549306\n",
      "train loss:0.22592835136492845\n",
      "train loss:0.2814976085143611\n",
      "train loss:0.24154510483419145\n",
      "train loss:0.3157096469850451\n",
      "train loss:0.31204417608734997\n",
      "train loss:0.2182097482323154\n",
      "train loss:0.26613965959519137\n",
      "train loss:0.3046802760711388\n",
      "train loss:0.3204911200674129\n",
      "train loss:0.2854177825155757\n",
      "train loss:0.19447599970103535\n",
      "train loss:0.23999693817474502\n",
      "train loss:0.29631060667663206\n",
      "train loss:0.3340450553924603\n",
      "train loss:0.2781280735495986\n",
      "train loss:0.31810372698680517\n",
      "train loss:0.26363848469074563\n",
      "train loss:0.2826745721806281\n",
      "train loss:0.2052643663158634\n",
      "train loss:0.32445373209282063\n",
      "train loss:0.260220171296965\n",
      "train loss:0.2770155824187467\n",
      "train loss:0.2611568722536813\n",
      "train loss:0.25023996640060614\n",
      "train loss:0.17289016545316382\n",
      "train loss:0.20673091085399828\n",
      "train loss:0.21569317081901698\n",
      "train loss:0.15207953114255804\n",
      "train loss:0.28150971456389506\n",
      "train loss:0.19994769524683637\n",
      "train loss:0.20583445853719506\n",
      "train loss:0.17974197461358918\n",
      "train loss:0.23756732378589213\n",
      "train loss:0.19934136656659546\n",
      "train loss:0.2271732008974616\n",
      "train loss:0.1748713953956532\n",
      "train loss:0.2601419455082881\n",
      "train loss:0.11401732795443147\n",
      "train loss:0.245083836175517\n",
      "train loss:0.193063839216036\n",
      "train loss:0.15022329098451018\n",
      "train loss:0.17562403977009478\n",
      "train loss:0.1649629076672019\n",
      "train loss:0.20521113099562263\n",
      "train loss:0.16065608618861313\n",
      "train loss:0.20572882537992473\n",
      "train loss:0.2566753292946221\n",
      "train loss:0.3292637746834469\n",
      "train loss:0.19738461090453188\n",
      "train loss:0.15813334597914802\n",
      "train loss:0.32179305136337616\n",
      "train loss:0.19375758122111975\n",
      "train loss:0.2255424414534394\n",
      "train loss:0.16281975885214692\n",
      "train loss:0.23816365434405673\n",
      "train loss:0.1772535328431478\n",
      "train loss:0.19952762329696616\n",
      "train loss:0.18879609527871283\n",
      "train loss:0.2453315562021015\n",
      "train loss:0.17417843726616403\n",
      "train loss:0.2154531046477511\n",
      "train loss:0.16973621722020843\n",
      "train loss:0.1396818361084563\n",
      "train loss:0.19244502827275267\n",
      "train loss:0.1887001210205654\n",
      "train loss:0.28061963715121746\n",
      "train loss:0.2904471544603275\n",
      "train loss:0.21318110081106903\n",
      "train loss:0.2123768749111859\n",
      "train loss:0.18289392141965932\n",
      "train loss:0.23238831722618852\n",
      "train loss:0.14248294616167487\n",
      "train loss:0.26105934330792596\n",
      "train loss:0.1724474873300719\n",
      "train loss:0.14784767329098614\n",
      "train loss:0.2004282580822854\n",
      "train loss:0.11932191031460546\n",
      "train loss:0.1723548179312673\n",
      "train loss:0.19913100090719166\n",
      "train loss:0.25961253443013305\n",
      "train loss:0.12298171919593096\n",
      "train loss:0.24709590314142227\n",
      "train loss:0.13715092022307365\n",
      "train loss:0.2166301587210672\n",
      "train loss:0.284114617931568\n",
      "train loss:0.16600271648797824\n",
      "train loss:0.11847176444964291\n",
      "train loss:0.17091554584096566\n",
      "train loss:0.17756863152268262\n",
      "train loss:0.23487206154629453\n",
      "train loss:0.14868344889928486\n",
      "train loss:0.17886303388501304\n",
      "train loss:0.2185745461694359\n",
      "train loss:0.2810713344207657\n",
      "train loss:0.1290199019723495\n",
      "train loss:0.1883395182089415\n",
      "train loss:0.1787704855646153\n",
      "train loss:0.18710760341909918\n",
      "train loss:0.12384616131559939\n",
      "train loss:0.17697343123804446\n",
      "train loss:0.15516770120587778\n",
      "train loss:0.25720091897673725\n",
      "train loss:0.12318864414100283\n",
      "train loss:0.1703103530101797\n",
      "train loss:0.15759826673162786\n",
      "train loss:0.13134017799628198\n",
      "train loss:0.13288984959640757\n",
      "train loss:0.1230033288813204\n",
      "train loss:0.11131046967071452\n",
      "train loss:0.10511483648882194\n",
      "train loss:0.14369314924838925\n",
      "train loss:0.0994017886751075\n",
      "train loss:0.20675665766836945\n",
      "train loss:0.13643125831766312\n",
      "train loss:0.1272650825152146\n",
      "train loss:0.21897327970152916\n",
      "train loss:0.11285709354917924\n",
      "train loss:0.10619544245090197\n",
      "train loss:0.17987092287668313\n",
      "train loss:0.16192357803557608\n",
      "train loss:0.14070143640773564\n",
      "train loss:0.10919386332078299\n",
      "train loss:0.19978835141844975\n",
      "train loss:0.20362476052294157\n",
      "train loss:0.1333039290016696\n",
      "train loss:0.11453352580006225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.18621706547360722\n",
      "train loss:0.07877164353429776\n",
      "train loss:0.11420188178087519\n",
      "train loss:0.21296243018426472\n",
      "train loss:0.14439728508334107\n",
      "train loss:0.08824365044487\n",
      "train loss:0.2173892390981422\n",
      "train loss:0.1454934313614136\n",
      "train loss:0.10797405743378584\n",
      "train loss:0.10414883084398702\n",
      "train loss:0.11795174246188166\n",
      "train loss:0.14140906734869815\n",
      "train loss:0.08419468831411402\n",
      "train loss:0.2564691082249689\n",
      "train loss:0.10300753755255516\n",
      "train loss:0.16538443130001187\n",
      "train loss:0.15454894661842555\n",
      "train loss:0.14452581816705778\n",
      "train loss:0.14983263659140217\n",
      "train loss:0.22268418920592808\n",
      "train loss:0.1624519335834073\n",
      "train loss:0.14553169945161679\n",
      "train loss:0.1247658974303658\n",
      "train loss:0.07464402732886727\n",
      "train loss:0.08559176594835818\n",
      "train loss:0.20497408554388824\n",
      "train loss:0.1775767307527199\n",
      "train loss:0.07042995755384464\n",
      "train loss:0.18102175419398867\n",
      "train loss:0.14198151188139949\n",
      "train loss:0.07937575544814163\n",
      "train loss:0.0891530707800826\n",
      "train loss:0.121476650455691\n",
      "train loss:0.1138298932315954\n",
      "train loss:0.07227827777286437\n",
      "train loss:0.1303298550156541\n",
      "train loss:0.16776797394197462\n",
      "train loss:0.10377776707899873\n",
      "train loss:0.15738510106895234\n",
      "train loss:0.19136406879395806\n",
      "train loss:0.0551063338158057\n",
      "train loss:0.10526013529216956\n",
      "train loss:0.13428699446113418\n",
      "train loss:0.08668681350880673\n",
      "train loss:0.11508899192732411\n",
      "train loss:0.1516185250124096\n",
      "train loss:0.1208061979443925\n",
      "train loss:0.09903637861620404\n",
      "train loss:0.08488447763861569\n",
      "train loss:0.08851376826508292\n",
      "train loss:0.154897175179089\n",
      "train loss:0.04491259710229138\n",
      "train loss:0.2014181530454074\n",
      "train loss:0.0948105435973033\n",
      "train loss:0.10434705822420834\n",
      "train loss:0.16210430214391933\n",
      "train loss:0.10801582705123372\n",
      "train loss:0.09124545760010767\n",
      "train loss:0.11553530828555288\n",
      "train loss:0.13901697783398087\n",
      "train loss:0.1702136072060089\n",
      "train loss:0.08030326506301251\n",
      "train loss:0.06965623479435055\n",
      "train loss:0.10403800542565035\n",
      "train loss:0.1167981786785285\n",
      "train loss:0.18770405770212922\n",
      "train loss:0.1615252152472695\n",
      "train loss:0.1560689614859759\n",
      "train loss:0.17834206790337184\n",
      "train loss:0.19419736737594126\n",
      "train loss:0.07835329359421306\n",
      "train loss:0.10625197275202164\n",
      "train loss:0.1029561368051613\n",
      "train loss:0.16320843360912837\n",
      "train loss:0.060778839378494626\n",
      "train loss:0.09590777611805595\n",
      "train loss:0.1596594005499332\n",
      "train loss:0.21007861247664236\n",
      "train loss:0.11997183300391683\n",
      "train loss:0.15202105952735365\n",
      "train loss:0.1948305901316691\n",
      "train loss:0.07259654971923062\n",
      "train loss:0.10960732476239168\n",
      "train loss:0.09186687149204689\n",
      "train loss:0.08135412403356003\n",
      "train loss:0.11578299576736212\n",
      "train loss:0.1040304902590155\n",
      "train loss:0.17318212326944812\n",
      "train loss:0.10458816245677843\n",
      "train loss:0.10962943237815924\n",
      "train loss:0.15914102969547775\n",
      "train loss:0.12140840835599838\n",
      "train loss:0.15056115294194208\n",
      "train loss:0.10367861031933114\n",
      "train loss:0.09383942945607059\n",
      "train loss:0.09268707955869131\n",
      "train loss:0.13056021491184377\n",
      "train loss:0.12869115021946834\n",
      "train loss:0.08914830326568864\n",
      "=== epoch:2, train acc:0.9590126811594203, test acc:0.9520833333333333 ===\n",
      "train loss:0.1040889230128019\n",
      "train loss:0.10036254031296261\n",
      "train loss:0.10709181665051716\n",
      "train loss:0.12011750561858496\n",
      "train loss:0.09246706292266431\n",
      "train loss:0.16040180566977383\n",
      "train loss:0.1465937337104423\n",
      "train loss:0.1251064189247393\n",
      "train loss:0.0964940415270879\n",
      "train loss:0.08504587985308752\n",
      "train loss:0.08392311577887034\n",
      "train loss:0.05312560548732866\n",
      "train loss:0.16625009820706108\n",
      "train loss:0.13736080972353654\n",
      "train loss:0.11230445810679254\n",
      "train loss:0.07469352090358898\n",
      "train loss:0.07367985627427019\n",
      "train loss:0.18262855526957342\n",
      "train loss:0.14074730799641763\n",
      "train loss:0.10002680092895513\n",
      "train loss:0.236633721248967\n",
      "train loss:0.04155075332710032\n",
      "train loss:0.09366773296107263\n",
      "train loss:0.13408496676631554\n",
      "train loss:0.12456467005370452\n",
      "train loss:0.08762330990069528\n",
      "train loss:0.0964765705945491\n",
      "train loss:0.07318510225382777\n",
      "train loss:0.1688560287239143\n",
      "train loss:0.033684835674964386\n",
      "train loss:0.12328684440323466\n",
      "train loss:0.09570407711083621\n",
      "train loss:0.0668628638059344\n",
      "train loss:0.18765685673162377\n",
      "train loss:0.07012759644066656\n",
      "train loss:0.10293194696701093\n",
      "train loss:0.09254118710632073\n",
      "train loss:0.11878008247561424\n",
      "train loss:0.14396156755352033\n",
      "train loss:0.03798129969379071\n",
      "train loss:0.1499199814554069\n",
      "train loss:0.06581370453871381\n",
      "train loss:0.07855007852439816\n",
      "train loss:0.10006891107779256\n",
      "train loss:0.11785377454739722\n",
      "train loss:0.1858126351802938\n",
      "train loss:0.057086808384537044\n",
      "train loss:0.167311785703006\n",
      "train loss:0.0691537983914347\n",
      "train loss:0.05713158165584821\n",
      "train loss:0.10933967057890076\n",
      "train loss:0.03696969514147301\n",
      "train loss:0.11090482746759443\n",
      "train loss:0.06681315658798453\n",
      "train loss:0.10170099073434272\n",
      "train loss:0.09013649001106054\n",
      "train loss:0.2029048548004533\n",
      "train loss:0.04421346399060357\n",
      "train loss:0.10570562843070459\n",
      "train loss:0.0778391624159218\n",
      "train loss:0.09590593925207545\n",
      "train loss:0.11606206760147363\n",
      "train loss:0.05683874649110909\n",
      "train loss:0.12796377445114335\n",
      "train loss:0.1346519943600176\n",
      "train loss:0.1438792820341998\n",
      "train loss:0.11995331877016749\n",
      "train loss:0.10665110968719145\n",
      "train loss:0.0574717974996356\n",
      "train loss:0.13203394163748353\n",
      "train loss:0.1229727766773448\n",
      "train loss:0.1290688041076025\n",
      "train loss:0.11291049876768865\n",
      "train loss:0.0869983051250207\n",
      "train loss:0.09363627789991116\n",
      "train loss:0.1153974424761978\n",
      "train loss:0.12495773018339963\n",
      "train loss:0.1312166114756653\n",
      "train loss:0.09667434652332135\n",
      "train loss:0.08567350879310584\n",
      "train loss:0.08197525379687676\n",
      "train loss:0.07193922872406724\n",
      "train loss:0.05357864083090071\n",
      "train loss:0.09878442998831698\n",
      "train loss:0.1414318286609843\n",
      "train loss:0.06100074675121287\n",
      "train loss:0.04179497225057613\n",
      "train loss:0.08330668283058766\n",
      "train loss:0.08533516951880228\n",
      "train loss:0.04749877634458096\n",
      "train loss:0.058588350015420204\n",
      "train loss:0.06872823556892502\n",
      "train loss:0.11817153784509607\n",
      "train loss:0.12320357037460107\n",
      "train loss:0.07391202741254727\n",
      "train loss:0.07267904290555333\n",
      "train loss:0.05921962139196298\n",
      "train loss:0.06469467943273248\n",
      "train loss:0.13489995580210604\n",
      "train loss:0.10535337574630835\n",
      "train loss:0.10493439180760936\n",
      "train loss:0.19967417000453752\n",
      "train loss:0.08676434992378704\n",
      "train loss:0.16343510462879987\n",
      "train loss:0.09084214614284528\n",
      "train loss:0.1075911730761465\n",
      "train loss:0.10005731549266485\n",
      "train loss:0.18227624443000778\n",
      "train loss:0.06894388871959625\n",
      "train loss:0.10570667682658456\n",
      "train loss:0.07607189415402442\n",
      "train loss:0.11081690720999\n",
      "train loss:0.09481842518451475\n",
      "train loss:0.0777743062077662\n",
      "train loss:0.16802279252385302\n",
      "train loss:0.0710804453504094\n",
      "train loss:0.08836704639544818\n",
      "train loss:0.0647437410978132\n",
      "train loss:0.10993469872101064\n",
      "train loss:0.08358336280906903\n",
      "train loss:0.10228250843433788\n",
      "train loss:0.06386204920639324\n",
      "train loss:0.051684581351675946\n",
      "train loss:0.1354362926367568\n",
      "train loss:0.09273558037169846\n",
      "train loss:0.09235720478901571\n",
      "train loss:0.07080073547909087\n",
      "train loss:0.05451146428631374\n",
      "train loss:0.05727086435178221\n",
      "train loss:0.067343962849799\n",
      "train loss:0.07832755223411586\n",
      "train loss:0.04347029612226343\n",
      "train loss:0.17105663486176476\n",
      "train loss:0.14590213261640286\n",
      "train loss:0.14666517068163093\n",
      "train loss:0.04016865730720103\n",
      "train loss:0.11019293778938932\n",
      "train loss:0.1293055430887585\n",
      "train loss:0.10198973974976733\n",
      "train loss:0.043275437611090435\n",
      "train loss:0.12023125159813523\n",
      "train loss:0.07994927351672794\n",
      "train loss:0.0861200524550818\n",
      "train loss:0.08474440332362213\n",
      "train loss:0.11508537696176292\n",
      "train loss:0.09671037631023965\n",
      "train loss:0.06456355263969248\n",
      "train loss:0.12597303456009412\n",
      "train loss:0.1380531008358571\n",
      "train loss:0.1631091504807812\n",
      "train loss:0.08067058401619683\n",
      "train loss:0.12495716903769577\n",
      "train loss:0.06105444371420923\n",
      "train loss:0.09613222914885637\n",
      "train loss:0.06751086620587451\n",
      "train loss:0.07416167076146123\n",
      "train loss:0.06394586669793871\n",
      "train loss:0.08317426851656494\n",
      "train loss:0.17562774797244593\n",
      "train loss:0.05773826875589424\n",
      "train loss:0.13299634750147876\n",
      "train loss:0.1033766698838809\n",
      "train loss:0.05390742091333521\n",
      "train loss:0.06102692017706876\n",
      "train loss:0.07308208246147038\n",
      "train loss:0.06465353065489761\n",
      "train loss:0.05601090749084203\n",
      "train loss:0.06667427474954737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.03624787645947771\n",
      "train loss:0.16443741795436947\n",
      "train loss:0.09031981587761848\n",
      "train loss:0.13899296856957133\n",
      "train loss:0.07860654250296435\n",
      "train loss:0.050358981400622525\n",
      "train loss:0.0309888049964518\n",
      "train loss:0.15222864184113066\n",
      "train loss:0.19966080851050727\n",
      "train loss:0.10422016690048899\n",
      "train loss:0.06374010496009726\n",
      "train loss:0.08888765113141464\n",
      "train loss:0.0480699195684392\n",
      "train loss:0.04375579772617563\n",
      "train loss:0.02259536940906143\n",
      "train loss:0.09916527872788096\n",
      "train loss:0.06460496037530322\n",
      "train loss:0.08961229083988004\n",
      "train loss:0.1149425106238128\n",
      "train loss:0.07460403176451749\n",
      "train loss:0.0659720396016538\n",
      "train loss:0.12195933892362373\n",
      "train loss:0.06364953067684206\n",
      "train loss:0.05628135279016886\n",
      "train loss:0.10004698582941332\n",
      "train loss:0.08043755841008059\n",
      "train loss:0.07903490756461912\n",
      "train loss:0.12327980478227422\n",
      "train loss:0.10240244970980351\n",
      "train loss:0.022464793529932938\n",
      "train loss:0.062043263672158844\n",
      "train loss:0.08507411081449619\n",
      "train loss:0.08813035128486804\n",
      "train loss:0.10814631595722594\n",
      "train loss:0.08170845297885905\n",
      "train loss:0.09519460932669135\n",
      "train loss:0.025802987181982533\n",
      "train loss:0.1273406291514449\n",
      "train loss:0.14347219620180537\n",
      "train loss:0.046786608505592665\n",
      "train loss:0.04573645923330534\n",
      "train loss:0.027254232153872845\n",
      "train loss:0.051105175040104436\n",
      "train loss:0.055235940495051865\n",
      "train loss:0.09853775110874832\n",
      "train loss:0.0950741244478461\n",
      "train loss:0.05926539280203872\n",
      "train loss:0.05841815431577049\n",
      "train loss:0.06534559496337558\n",
      "train loss:0.05683754839772202\n",
      "train loss:0.14513505203262428\n",
      "train loss:0.06904705179635283\n",
      "train loss:0.08440706192134989\n",
      "train loss:0.04827540443534455\n",
      "train loss:0.11536956103142507\n",
      "train loss:0.14458877812125798\n",
      "train loss:0.1689618821575737\n",
      "train loss:0.06263848443479594\n",
      "train loss:0.060204149389612306\n",
      "train loss:0.0975479381634243\n",
      "train loss:0.11160605311593234\n",
      "train loss:0.036111065574342814\n",
      "train loss:0.05981060632838396\n",
      "train loss:0.10284483410718952\n",
      "train loss:0.04973732847824996\n",
      "train loss:0.1040352518411982\n",
      "train loss:0.05674009204496763\n",
      "train loss:0.17284601178355943\n",
      "train loss:0.065896783149528\n",
      "train loss:0.05457999943875871\n",
      "train loss:0.1003052216287965\n",
      "train loss:0.05217018274347073\n",
      "train loss:0.06204681390127963\n",
      "train loss:0.07609612827825618\n",
      "train loss:0.05232396859957844\n",
      "train loss:0.11026058319689754\n",
      "train loss:0.02751113597229903\n",
      "train loss:0.06476538169757617\n",
      "train loss:0.05737696588778648\n",
      "train loss:0.13194079721250504\n",
      "train loss:0.07951158837505876\n",
      "train loss:0.09001375829756858\n",
      "train loss:0.1127837819133228\n",
      "train loss:0.11328942431696393\n",
      "train loss:0.06871217756222063\n",
      "train loss:0.02376870066545705\n",
      "train loss:0.01893011804858485\n",
      "train loss:0.09238958459144898\n",
      "train loss:0.10044331448012137\n",
      "train loss:0.08278273848832614\n",
      "train loss:0.10211573848020192\n",
      "train loss:0.11868261451886271\n",
      "train loss:0.03605826491949439\n",
      "train loss:0.0735040432957586\n",
      "train loss:0.05550059194108747\n",
      "train loss:0.08982024080256822\n",
      "train loss:0.142655318739033\n",
      "train loss:0.1102861763427758\n",
      "train loss:0.11813267165679081\n",
      "train loss:0.06555012176625089\n",
      "train loss:0.027051846133895812\n",
      "train loss:0.12431292806132624\n",
      "train loss:0.10939514066895571\n",
      "train loss:0.05234735061111727\n",
      "train loss:0.11495243373873434\n",
      "train loss:0.04229361008885164\n",
      "train loss:0.08325400499133352\n",
      "train loss:0.08480910382041373\n",
      "train loss:0.08582955982276395\n",
      "train loss:0.04268244071471031\n",
      "train loss:0.08706738807274957\n",
      "train loss:0.03567124930796986\n",
      "train loss:0.13567251670418182\n",
      "train loss:0.0603159989794448\n",
      "train loss:0.08621490111785997\n",
      "train loss:0.0321501500457679\n",
      "train loss:0.05757693653679576\n",
      "train loss:0.11447316771013462\n",
      "train loss:0.0716424169708432\n",
      "train loss:0.17499972156045937\n",
      "train loss:0.09749160691504999\n",
      "train loss:0.07508647859897413\n",
      "train loss:0.058634232985260416\n",
      "train loss:0.02683143536932361\n",
      "train loss:0.07203635472703292\n",
      "train loss:0.04545082745879731\n",
      "train loss:0.0673655880169853\n",
      "train loss:0.08397299910470624\n",
      "train loss:0.22410887712701036\n",
      "train loss:0.06893807807344261\n",
      "train loss:0.09874146489682946\n",
      "train loss:0.11250064652947604\n",
      "train loss:0.0978165228344398\n",
      "train loss:0.07011373053821639\n",
      "train loss:0.04869669946800101\n",
      "train loss:0.04468969385616896\n",
      "train loss:0.03764523559594902\n",
      "train loss:0.10357567645992241\n",
      "train loss:0.07000512917943154\n",
      "train loss:0.058362934739655446\n",
      "train loss:0.09659607027984819\n",
      "train loss:0.16887676542167832\n",
      "train loss:0.08482104112242535\n",
      "train loss:0.07442259588549736\n",
      "train loss:0.04396374590589035\n",
      "train loss:0.06624666910142266\n",
      "train loss:0.08745046913467158\n",
      "train loss:0.03796841408932001\n",
      "train loss:0.03386428206131786\n",
      "train loss:0.09886348438752487\n",
      "train loss:0.09577037328881348\n",
      "train loss:0.03111772085456995\n",
      "train loss:0.08768209559388877\n",
      "train loss:0.05620969818532396\n",
      "train loss:0.05816139845461851\n",
      "train loss:0.03825200475208497\n",
      "train loss:0.0883182883808297\n",
      "train loss:0.08921193995839737\n",
      "train loss:0.16425091578694312\n",
      "train loss:0.032544118661439705\n",
      "train loss:0.046072238539391255\n",
      "train loss:0.08788176866036125\n",
      "train loss:0.04141976587807069\n",
      "train loss:0.11689841307845664\n",
      "train loss:0.05651961532963415\n",
      "train loss:0.1374017207590825\n",
      "train loss:0.07556048449147733\n",
      "train loss:0.09214349733899921\n",
      "train loss:0.07528829428071313\n",
      "train loss:0.06480450812079214\n",
      "train loss:0.11803895692700855\n",
      "train loss:0.04559848622580256\n",
      "train loss:0.09130399809028472\n",
      "train loss:0.08724115669973857\n",
      "train loss:0.03887631907272265\n",
      "train loss:0.04713923845500761\n",
      "train loss:0.14863262302773392\n",
      "train loss:0.035153895841080786\n",
      "train loss:0.035380682766521995\n",
      "train loss:0.02422817880245737\n",
      "train loss:0.17151570851939937\n",
      "train loss:0.06437516231484941\n",
      "train loss:0.08311091308827054\n",
      "train loss:0.05388728557053015\n",
      "train loss:0.08773406644095533\n",
      "train loss:0.08716990046791466\n",
      "train loss:0.08411737054203566\n",
      "train loss:0.05185520229152989\n",
      "train loss:0.05574161481372499\n",
      "train loss:0.07519109466146104\n",
      "train loss:0.048617420213293586\n",
      "train loss:0.08547945427646289\n",
      "train loss:0.05336799272362831\n",
      "train loss:0.14902646751507534\n",
      "train loss:0.060309684383574674\n",
      "train loss:0.05878102567258092\n",
      "train loss:0.06117562688889103\n",
      "train loss:0.03538381727774725\n",
      "train loss:0.107254078826117\n",
      "train loss:0.08853623872392251\n",
      "=== epoch:3, train acc:0.9733016304347826, test acc:0.9651268115942029 ===\n",
      "train loss:0.10132210741149379\n",
      "train loss:0.04791377708638704\n",
      "train loss:0.09143783853296197\n",
      "train loss:0.060318578667541284\n",
      "train loss:0.1590392270565167\n",
      "train loss:0.04167909940376486\n",
      "train loss:0.10564637935285096\n",
      "train loss:0.05745741494926951\n",
      "train loss:0.07290744633047212\n",
      "train loss:0.06598630025240275\n",
      "train loss:0.06191710439674454\n",
      "train loss:0.07857858306647675\n",
      "train loss:0.04455606093184137\n",
      "train loss:0.08163422589349002\n",
      "train loss:0.06997683318721518\n",
      "train loss:0.03449365949444343\n",
      "train loss:0.0733906231765004\n",
      "train loss:0.0922601561184797\n",
      "train loss:0.04096834573508335\n",
      "train loss:0.04610592877027469\n",
      "train loss:0.024443653321609457\n",
      "train loss:0.05757631190221748\n",
      "train loss:0.07423259413953404\n",
      "train loss:0.07354710074180981\n",
      "train loss:0.09547510036017419\n",
      "train loss:0.11332176164516666\n",
      "train loss:0.09576305298637797\n",
      "train loss:0.07799901572915084\n",
      "train loss:0.02176652115773164\n",
      "train loss:0.056999313117720454\n",
      "train loss:0.012655723859508496\n",
      "train loss:0.06682263499905776\n",
      "train loss:0.027737660454072124\n",
      "train loss:0.016660928819082158\n",
      "train loss:0.01791123821464131\n",
      "train loss:0.07902385494200391\n",
      "train loss:0.029501754337082034\n",
      "train loss:0.05298983532307391\n",
      "train loss:0.03784654040674075\n",
      "train loss:0.045941309878477006\n",
      "train loss:0.01316411546050666\n",
      "train loss:0.09051930711763842\n",
      "train loss:0.03480576650206911\n",
      "train loss:0.019203437430037315\n",
      "train loss:0.1200586465452201\n",
      "train loss:0.12060882121666161\n",
      "train loss:0.07024621983525448\n",
      "train loss:0.05734308675027865\n",
      "train loss:0.08008836294259686\n",
      "train loss:0.08794450327935203\n",
      "train loss:0.047038351014488375\n",
      "train loss:0.024106326793564236\n",
      "train loss:0.07353767769511424\n",
      "train loss:0.04184110843274103\n",
      "train loss:0.03470372931776175\n",
      "train loss:0.035512258685163146\n",
      "train loss:0.04365358247042026\n",
      "train loss:0.06967825527695097\n",
      "train loss:0.10651884069990646\n",
      "train loss:0.06618593566687295\n",
      "train loss:0.059095160335677945\n",
      "train loss:0.04939342715259188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.02897656902068023\n",
      "train loss:0.027135467560895868\n",
      "train loss:0.058974671358571124\n",
      "train loss:0.02748707161803173\n",
      "train loss:0.02320664533167131\n",
      "train loss:0.03276915933311359\n",
      "train loss:0.022985737880257616\n",
      "train loss:0.04645461874031563\n",
      "train loss:0.0471437402832155\n",
      "train loss:0.14517284933552818\n",
      "train loss:0.08826539999025422\n",
      "train loss:0.07931304359669764\n",
      "train loss:0.09462374899216965\n",
      "train loss:0.02371423931735298\n",
      "train loss:0.06245830185047545\n",
      "train loss:0.09318089622311462\n",
      "train loss:0.11635846173422036\n",
      "train loss:0.027056369244097808\n",
      "train loss:0.05287700075725997\n",
      "train loss:0.04047342298783229\n",
      "train loss:0.07687527166681779\n",
      "train loss:0.04995104091470233\n",
      "train loss:0.07123499068985385\n",
      "train loss:0.03294117505091427\n",
      "train loss:0.09801016298185111\n",
      "train loss:0.016878435314826676\n",
      "train loss:0.1499960468004483\n",
      "train loss:0.026663986429405994\n",
      "train loss:0.1576751335325662\n",
      "train loss:0.06507179443421195\n",
      "train loss:0.11888328428198026\n",
      "train loss:0.12323256805397023\n",
      "train loss:0.07071592887252319\n",
      "train loss:0.09130786311125517\n",
      "train loss:0.04196269272991242\n",
      "train loss:0.016967099990098537\n",
      "train loss:0.05517771113863897\n",
      "train loss:0.0479218920973629\n",
      "train loss:0.08423043541197285\n",
      "train loss:0.07579611778467292\n",
      "train loss:0.06664819582459888\n",
      "train loss:0.11045070320339234\n",
      "train loss:0.08678523782153243\n",
      "train loss:0.06684664990649458\n",
      "train loss:0.05174632424220936\n",
      "train loss:0.07653188783562784\n",
      "train loss:0.03393331927461217\n",
      "train loss:0.15096431572690727\n",
      "train loss:0.06558670264314224\n",
      "train loss:0.07206620646369576\n",
      "train loss:0.030677912918159408\n",
      "train loss:0.057509393763816447\n",
      "train loss:0.10901875069941698\n",
      "train loss:0.04859184127934483\n",
      "train loss:0.06500241831622242\n",
      "train loss:0.11406804568659643\n",
      "train loss:0.032352035466788595\n",
      "train loss:0.09683988246641659\n",
      "train loss:0.026326706597980577\n",
      "train loss:0.03402605800690795\n",
      "train loss:0.017361624188621142\n",
      "train loss:0.08946995315614616\n",
      "train loss:0.029780405785815707\n",
      "train loss:0.04452875063352539\n",
      "train loss:0.022795039856411677\n",
      "train loss:0.042731390793436366\n",
      "train loss:0.025732030731302525\n",
      "train loss:0.06436488567616076\n",
      "train loss:0.07321853457137836\n",
      "train loss:0.042733845258684934\n",
      "train loss:0.089291617490761\n",
      "train loss:0.07643291533459831\n",
      "train loss:0.07193615745296987\n",
      "train loss:0.026598848425773283\n",
      "train loss:0.06120322079525118\n",
      "train loss:0.03967689954991659\n",
      "train loss:0.04864087517215453\n",
      "train loss:0.07769178746461163\n",
      "train loss:0.06239000436593626\n",
      "train loss:0.04244140533743949\n",
      "train loss:0.03714614580999932\n",
      "train loss:0.012044643149315627\n",
      "train loss:0.10775461109715263\n",
      "train loss:0.10211267577419289\n",
      "train loss:0.0957362355294318\n",
      "train loss:0.018681108491590822\n",
      "train loss:0.03686598026984985\n",
      "train loss:0.054585391174829155\n",
      "train loss:0.04080330638743211\n",
      "train loss:0.02758967443933043\n",
      "train loss:0.06676724668637313\n",
      "train loss:0.11758947524287007\n",
      "train loss:0.029777172359914676\n",
      "train loss:0.02228536702467753\n",
      "train loss:0.030364898912216158\n",
      "train loss:0.025215425307045677\n",
      "train loss:0.024141707722739245\n",
      "train loss:0.018235682391879768\n",
      "train loss:0.02083573560742143\n",
      "train loss:0.010727698455833812\n",
      "train loss:0.11179379552504817\n",
      "train loss:0.05237303736289837\n",
      "train loss:0.053590126058952345\n",
      "train loss:0.06241768723137668\n",
      "train loss:0.13178862992809884\n",
      "train loss:0.04762487324778411\n",
      "train loss:0.036601256216240094\n",
      "train loss:0.04619845132055859\n",
      "train loss:0.07963198835361919\n",
      "train loss:0.04728293919581999\n",
      "train loss:0.09863057140992697\n",
      "train loss:0.018699883752638837\n",
      "train loss:0.016466221117775924\n",
      "train loss:0.05182363232592288\n",
      "train loss:0.09918471685862539\n",
      "train loss:0.018903348280089112\n",
      "train loss:0.024353369197509536\n",
      "train loss:0.03774463782272029\n",
      "train loss:0.06794702436352366\n",
      "train loss:0.024004247602548188\n",
      "train loss:0.04236725101403328\n",
      "train loss:0.013165813139700892\n",
      "train loss:0.03596372561289445\n",
      "train loss:0.059312781611400474\n",
      "train loss:0.03104329702248025\n",
      "train loss:0.05750891714230724\n",
      "train loss:0.035108165477964505\n",
      "train loss:0.10309406477935862\n",
      "train loss:0.06587500881283787\n",
      "train loss:0.019419228857431272\n",
      "train loss:0.053918842532321806\n",
      "train loss:0.0636652490611401\n",
      "train loss:0.03835115053774997\n",
      "train loss:0.023948768756673622\n",
      "train loss:0.07629787509201286\n",
      "train loss:0.10067693999352026\n",
      "train loss:0.061739068722882456\n",
      "train loss:0.06490997266080963\n",
      "train loss:0.06662237999411415\n",
      "train loss:0.03907771922628161\n",
      "train loss:0.0511773329142164\n",
      "train loss:0.03486712605037598\n",
      "train loss:0.019456817431979755\n",
      "train loss:0.03770352101663377\n",
      "train loss:0.03366167754608974\n",
      "train loss:0.03380339564636053\n",
      "train loss:0.057560488166515135\n",
      "train loss:0.046894202992396404\n",
      "train loss:0.07766105639834922\n",
      "train loss:0.027099086315626613\n",
      "train loss:0.01673954584458654\n",
      "train loss:0.042185678385402575\n",
      "train loss:0.020007581805218562\n",
      "train loss:0.035311927836659485\n",
      "train loss:0.0645892780293554\n",
      "train loss:0.06386231300492831\n",
      "train loss:0.02915489048832327\n",
      "train loss:0.04788656832334563\n",
      "train loss:0.060763085001144294\n",
      "train loss:0.03210993371172518\n",
      "train loss:0.03316844866362883\n",
      "train loss:0.08772718225743312\n",
      "train loss:0.04025526858655499\n",
      "train loss:0.056115305858775805\n",
      "train loss:0.031993662402229\n",
      "train loss:0.027916875637444674\n",
      "train loss:0.06790166418404171\n",
      "train loss:0.051203151666785\n",
      "train loss:0.08001880437555652\n",
      "train loss:0.05212011664316031\n",
      "train loss:0.0645447813741555\n",
      "train loss:0.0335778909234789\n",
      "train loss:0.033694101219211445\n",
      "train loss:0.11867841691965099\n",
      "train loss:0.09143080204282115\n",
      "train loss:0.0473353388419629\n",
      "train loss:0.11096039098570479\n",
      "train loss:0.046112314532456705\n",
      "train loss:0.029367398452226565\n",
      "train loss:0.02642202181877571\n",
      "train loss:0.03364611460922606\n",
      "train loss:0.04012356555227856\n",
      "train loss:0.06886699097377515\n",
      "train loss:0.042876749205082616\n",
      "train loss:0.0909646921885478\n",
      "train loss:0.06105323308555442\n",
      "train loss:0.11220571563799232\n",
      "train loss:0.05658607256265332\n",
      "train loss:0.03160707174135337\n",
      "train loss:0.01019458429347831\n",
      "train loss:0.055107569705542626\n",
      "train loss:0.08792186792330617\n",
      "train loss:0.08091432008239022\n",
      "train loss:0.0353454730822255\n",
      "train loss:0.017211528112621558\n",
      "train loss:0.03313064388307466\n",
      "train loss:0.02078109189490169\n",
      "train loss:0.0289545795362585\n",
      "train loss:0.03332802286586883\n",
      "train loss:0.09685568762993599\n",
      "train loss:0.034248645256468514\n",
      "train loss:0.06685681597433443\n",
      "train loss:0.04387248762371843\n",
      "train loss:0.039237284077795725\n",
      "train loss:0.033929153972155256\n",
      "train loss:0.04608287691883057\n",
      "train loss:0.09739862082142\n",
      "train loss:0.046511865267073293\n",
      "train loss:0.03181764719097042\n",
      "train loss:0.032109333302212835\n",
      "train loss:0.030463252894106875\n",
      "train loss:0.030466884311084498\n",
      "train loss:0.03634510873733367\n",
      "train loss:0.04376307779777893\n",
      "train loss:0.05947323564278458\n",
      "train loss:0.03737883121963624\n",
      "train loss:0.1305953336642206\n",
      "train loss:0.04671460734472566\n",
      "train loss:0.020502996682133064\n",
      "train loss:0.09148639435333333\n",
      "train loss:0.02801789737033961\n",
      "train loss:0.10054050330491518\n",
      "train loss:0.045663576456295686\n",
      "train loss:0.03110022842113358\n",
      "train loss:0.036207964332620905\n",
      "train loss:0.11445061242253732\n",
      "train loss:0.11845565834935104\n",
      "train loss:0.05875205106339942\n",
      "train loss:0.04761128382763869\n",
      "train loss:0.12346817249086971\n",
      "train loss:0.09790155980740188\n",
      "train loss:0.06285959341217157\n",
      "train loss:0.049961806000572755\n",
      "train loss:0.10116129736596156\n",
      "train loss:0.024296395025460178\n",
      "train loss:0.1114469242453489\n",
      "train loss:0.04608588462624127\n",
      "train loss:0.01840850421182481\n",
      "train loss:0.020311361748894356\n",
      "train loss:0.04298500025950942\n",
      "train loss:0.08240405641725443\n",
      "train loss:0.1255235513939836\n",
      "train loss:0.057051292058814436\n",
      "train loss:0.029899122314707\n",
      "train loss:0.04624329744789811\n",
      "train loss:0.0535172256640284\n",
      "train loss:0.15384209556373182\n",
      "train loss:0.06804441747729374\n",
      "train loss:0.049503099939311804\n",
      "train loss:0.028190172351231023\n",
      "train loss:0.07669186796748481\n",
      "train loss:0.04027945213138816\n",
      "train loss:0.040145354284564415\n",
      "train loss:0.023511051078736512\n",
      "train loss:0.07459028076386814\n",
      "train loss:0.03758750990468845\n",
      "train loss:0.05271155682749593\n",
      "train loss:0.01342547121506898\n",
      "train loss:0.023413121721100143\n",
      "train loss:0.06313754940077893\n",
      "train loss:0.061477219825537666\n",
      "train loss:0.06500612096807841\n",
      "train loss:0.006968290314269933\n",
      "train loss:0.051056285686786024\n",
      "train loss:0.010241101093885784\n",
      "train loss:0.07048811185933358\n",
      "train loss:0.07512046501442972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.07412815260709694\n",
      "train loss:0.009595224107609535\n",
      "train loss:0.02608753075429761\n",
      "train loss:0.05681760101621503\n",
      "train loss:0.12102517649880291\n",
      "train loss:0.061607782000329764\n",
      "train loss:0.020005250507233105\n",
      "train loss:0.046319915977150135\n",
      "train loss:0.02128759420511737\n",
      "train loss:0.08252867202216961\n",
      "train loss:0.024109390175178026\n",
      "train loss:0.019929576589291283\n",
      "train loss:0.038007765390336484\n",
      "train loss:0.008596302172953723\n",
      "train loss:0.05297322605911351\n",
      "train loss:0.037958136427091976\n",
      "train loss:0.044634801406009644\n",
      "train loss:0.05800490013422429\n",
      "train loss:0.049412714748672246\n",
      "train loss:0.07015396784254294\n",
      "train loss:0.05224361528942164\n",
      "train loss:0.03988294576074057\n",
      "train loss:0.008975872745968724\n",
      "train loss:0.05428155337077696\n",
      "train loss:0.051810906121860144\n",
      "train loss:0.04193702306592385\n",
      "train loss:0.019893374142188013\n",
      "train loss:0.017676347295084997\n",
      "train loss:0.025132020007298746\n",
      "train loss:0.07305922336401854\n",
      "train loss:0.06471333288643276\n",
      "train loss:0.09293179983474248\n",
      "train loss:0.03452330865166679\n",
      "train loss:0.12721060326945138\n",
      "train loss:0.07464607475176414\n",
      "train loss:0.06740739119496106\n",
      "train loss:0.03968322714503276\n",
      "train loss:0.03264766964112921\n",
      "train loss:0.07602907148673842\n",
      "=== epoch:4, train acc:0.9820425724637681, test acc:0.9743659420289855 ===\n",
      "train loss:0.04274477969104647\n",
      "train loss:0.043914288396283495\n",
      "train loss:0.01923631596243324\n",
      "train loss:0.041276087888091235\n",
      "train loss:0.03395926585871175\n",
      "train loss:0.2033935049521057\n",
      "train loss:0.049950296029228194\n",
      "train loss:0.03350140814682683\n",
      "train loss:0.059969744351987805\n",
      "train loss:0.012914535378628024\n",
      "train loss:0.04761047296138451\n",
      "train loss:0.07983740152584422\n",
      "train loss:0.03536305558733009\n",
      "train loss:0.03301456571243115\n",
      "train loss:0.043325977303117304\n",
      "train loss:0.09721116277438129\n",
      "train loss:0.07635394854347374\n",
      "train loss:0.04681180024216553\n",
      "train loss:0.04076561010678971\n",
      "train loss:0.08076141068193683\n",
      "train loss:0.08661595047722619\n",
      "train loss:0.03510370675778688\n",
      "train loss:0.06673065789029048\n",
      "train loss:0.04980269274464639\n",
      "train loss:0.036072857819073004\n",
      "train loss:0.058733837951103395\n",
      "train loss:0.030553461169658416\n",
      "train loss:0.027985496151782037\n",
      "train loss:0.10556048360605254\n",
      "train loss:0.0802044884959054\n",
      "train loss:0.03654871315791109\n",
      "train loss:0.03744769577832547\n",
      "train loss:0.01534756033602193\n",
      "train loss:0.05735674226056827\n",
      "train loss:0.03384103690240701\n",
      "train loss:0.06827619483888266\n",
      "train loss:0.12403633279809116\n",
      "train loss:0.057897105608330834\n",
      "train loss:0.06670734014785491\n",
      "train loss:0.08831870460204932\n",
      "train loss:0.035715786225983454\n",
      "train loss:0.03446204563079818\n",
      "train loss:0.043621982287008795\n",
      "train loss:0.11892995023776204\n",
      "train loss:0.03899662505572324\n",
      "train loss:0.02554186012460231\n",
      "train loss:0.04328100103197923\n",
      "train loss:0.07159025018101152\n",
      "train loss:0.04104529380414238\n",
      "train loss:0.06751876042913298\n",
      "train loss:0.012502049500966938\n",
      "train loss:0.04598446843553784\n",
      "train loss:0.04920582043045117\n",
      "train loss:0.053783729009374635\n",
      "train loss:0.03620526065994111\n",
      "train loss:0.030107769094197205\n",
      "train loss:0.04008337096900614\n",
      "train loss:0.06020250053168061\n",
      "train loss:0.05155133655947937\n",
      "train loss:0.012756286303460549\n",
      "train loss:0.023784867331467775\n",
      "train loss:0.03545613088055748\n",
      "train loss:0.034676180025288356\n",
      "train loss:0.01723471262087619\n",
      "train loss:0.05550256862484562\n",
      "train loss:0.01080136406243287\n",
      "train loss:0.01713649244337303\n",
      "train loss:0.039497723591395174\n",
      "train loss:0.05743134970898187\n",
      "train loss:0.057052461792715024\n",
      "train loss:0.034965853552436074\n",
      "train loss:0.009261277338594937\n",
      "train loss:0.028980486653743486\n",
      "train loss:0.09322909437574949\n",
      "train loss:0.024966265148952917\n",
      "train loss:0.02122935345133944\n",
      "train loss:0.1309511962851262\n",
      "train loss:0.015704089798847264\n",
      "train loss:0.03333488135887118\n",
      "train loss:0.017647905489752033\n",
      "train loss:0.008406929236898175\n",
      "train loss:0.031388780480361804\n",
      "train loss:0.08374342863631662\n",
      "train loss:0.02461435374446732\n",
      "train loss:0.05257096259278189\n",
      "train loss:0.008879214935547365\n",
      "train loss:0.017494732672361395\n",
      "train loss:0.08144877493309859\n",
      "train loss:0.039839818388253935\n",
      "train loss:0.021062448271074407\n",
      "train loss:0.052396173285683884\n",
      "train loss:0.09335418529547015\n",
      "train loss:0.03787850942442684\n",
      "train loss:0.01827159487430245\n",
      "train loss:0.010595920770721034\n",
      "train loss:0.03236111348311696\n",
      "train loss:0.0028809186656970076\n",
      "train loss:0.061046942794597055\n",
      "train loss:0.03693389400180807\n",
      "train loss:0.04406084485709498\n",
      "train loss:0.03680850322365206\n",
      "train loss:0.053341573047621584\n",
      "train loss:0.017298317242291618\n",
      "train loss:0.04352960793453969\n",
      "train loss:0.02281443124320113\n",
      "train loss:0.020511943135812005\n",
      "train loss:0.046418845160535446\n",
      "train loss:0.0142904550543315\n",
      "train loss:0.021575918103918196\n",
      "train loss:0.016602713782795528\n",
      "train loss:0.06770828169186234\n",
      "train loss:0.03203298098397551\n",
      "train loss:0.035019221576870437\n",
      "train loss:0.08479137538025987\n",
      "train loss:0.027201661413479388\n",
      "train loss:0.01834525121392456\n",
      "train loss:0.059594320897782035\n",
      "train loss:0.004015203621178871\n",
      "train loss:0.011166243257989903\n",
      "train loss:0.022800535263125307\n",
      "train loss:0.048664720136396535\n",
      "train loss:0.021497349240076194\n",
      "train loss:0.028837708178902476\n",
      "train loss:0.029117963999270687\n",
      "train loss:0.056032347288076365\n",
      "train loss:0.021941871372577585\n",
      "train loss:0.014284005593771151\n",
      "train loss:0.07385679094328114\n",
      "train loss:0.022673564746786175\n",
      "train loss:0.041509738911559166\n",
      "train loss:0.048534876266608386\n",
      "train loss:0.048264176687550904\n",
      "train loss:0.057201545042896575\n",
      "train loss:0.06940444179160564\n",
      "train loss:0.07891004809824394\n",
      "train loss:0.014178802413633047\n",
      "train loss:0.08572550836154448\n",
      "train loss:0.03963872073910503\n",
      "train loss:0.027466555761177108\n",
      "train loss:0.010291059803727033\n",
      "train loss:0.03287678542134769\n",
      "train loss:0.06709720233227845\n",
      "train loss:0.0436147816817895\n",
      "train loss:0.03777245376127844\n",
      "train loss:0.04910147831049953\n",
      "train loss:0.11419183507323444\n",
      "train loss:0.09300691926854679\n",
      "train loss:0.0054505256491983334\n",
      "train loss:0.014297380826808391\n",
      "train loss:0.02705765449480364\n",
      "train loss:0.03232303963879844\n",
      "train loss:0.011089769220861308\n",
      "train loss:0.07308589896419138\n",
      "train loss:0.0649304086636746\n",
      "train loss:0.031228708438281853\n",
      "train loss:0.046466274710097615\n",
      "train loss:0.05058328322420474\n",
      "train loss:0.037086735869218276\n",
      "train loss:0.09106678028915577\n",
      "train loss:0.05041206502160707\n",
      "train loss:0.02598738713564177\n",
      "train loss:0.04367043772198728\n",
      "train loss:0.023706180978046904\n",
      "train loss:0.05830535390462034\n",
      "train loss:0.06702923717551582\n",
      "train loss:0.03428737200265391\n",
      "train loss:0.048981482445433565\n",
      "train loss:0.10965917653745126\n",
      "train loss:0.025634987653248398\n",
      "train loss:0.04158371724426001\n",
      "train loss:0.023662501697153886\n",
      "train loss:0.05360747103154164\n",
      "train loss:0.017658335744490754\n",
      "train loss:0.03271087352022342\n",
      "train loss:0.02735520820433233\n",
      "train loss:0.06530937278193027\n",
      "train loss:0.04016958461963816\n",
      "train loss:0.03974248203997668\n",
      "train loss:0.009294151027122789\n",
      "train loss:0.09858604223374269\n",
      "train loss:0.0111978844693123\n",
      "train loss:0.052198782702415715\n",
      "train loss:0.01803487634766606\n",
      "train loss:0.031626815130486936\n",
      "train loss:0.034550686740703274\n",
      "train loss:0.03759579867036707\n",
      "train loss:0.026795371660965704\n",
      "train loss:0.03341419068217792\n",
      "train loss:0.02458274830897825\n",
      "train loss:0.08282315815054773\n",
      "train loss:0.13533638299529843\n",
      "train loss:0.02382834219340404\n",
      "train loss:0.046668544466479056\n",
      "train loss:0.05178373655030888\n",
      "train loss:0.07573024857548798\n",
      "train loss:0.013734630562364188\n",
      "train loss:0.09998063589706142\n",
      "train loss:0.01570251841539009\n",
      "train loss:0.01896184899767054\n",
      "train loss:0.00874528390260374\n",
      "train loss:0.040398407544978816\n",
      "train loss:0.04449093680460581\n",
      "train loss:0.020263610702765302\n",
      "train loss:0.016019401474362115\n",
      "train loss:0.02857627432997967\n",
      "train loss:0.03357033588348266\n",
      "train loss:0.07440897241274559\n",
      "train loss:0.09704425074396986\n",
      "train loss:0.02574994453956787\n",
      "train loss:0.022254417007249425\n",
      "train loss:0.14532008709617125\n",
      "train loss:0.06506091901404809\n",
      "train loss:0.030265504717850635\n",
      "train loss:0.05970676364413639\n",
      "train loss:0.0287307979133679\n",
      "train loss:0.049308748436928666\n",
      "train loss:0.03041580685242246\n",
      "train loss:0.07097419052465366\n",
      "train loss:0.03302949117532504\n",
      "train loss:0.04780990743876561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.030363054227070134\n",
      "train loss:0.021646305688501396\n",
      "train loss:0.02522757276351321\n",
      "train loss:0.04056235048228739\n",
      "train loss:0.055670699338832876\n",
      "train loss:0.03636437220019276\n",
      "train loss:0.020747384138480998\n",
      "train loss:0.06036295208233902\n",
      "train loss:0.015528871907631362\n",
      "train loss:0.06515902739119996\n",
      "train loss:0.02223109723950645\n",
      "train loss:0.10497890381788573\n",
      "train loss:0.04282961753741613\n",
      "train loss:0.04080247156575164\n",
      "train loss:0.00896945225694935\n",
      "train loss:0.014371420092818459\n",
      "train loss:0.035773466618697784\n",
      "train loss:0.04158898461295061\n",
      "train loss:0.03624243231440249\n",
      "train loss:0.06852868441588017\n",
      "train loss:0.038327362356531176\n",
      "train loss:0.024719895171281384\n",
      "train loss:0.024521789970003285\n",
      "train loss:0.026972844810240086\n",
      "train loss:0.061211854846155093\n",
      "train loss:0.03034120834468361\n",
      "train loss:0.011634882775130806\n",
      "train loss:0.027706106034300106\n",
      "train loss:0.012517516237029873\n",
      "train loss:0.049071550544629486\n",
      "train loss:0.06856420254131151\n",
      "train loss:0.03457111150020544\n",
      "train loss:0.022241035251383982\n",
      "train loss:0.09554320836641421\n",
      "train loss:0.016206118196253186\n",
      "train loss:0.015800551140896044\n",
      "train loss:0.05945287648415345\n",
      "train loss:0.06919825730385047\n",
      "train loss:0.11742862096823119\n",
      "train loss:0.039595903482422375\n",
      "train loss:0.04397926836122758\n",
      "train loss:0.022066970815122672\n",
      "train loss:0.019578191821143603\n",
      "train loss:0.020528910482872774\n",
      "train loss:0.04406747437315807\n",
      "train loss:0.04192667347102108\n",
      "train loss:0.0964448561342381\n",
      "train loss:0.013886502934019277\n",
      "train loss:0.02541746130506487\n",
      "train loss:0.03682744568008785\n",
      "train loss:0.04916934854431571\n",
      "train loss:0.09841555262681613\n",
      "train loss:0.02412907935681297\n",
      "train loss:0.010259164033622458\n",
      "train loss:0.04290739334948223\n",
      "train loss:0.017850523466684736\n",
      "train loss:0.0349640258127135\n",
      "train loss:0.09140706699844131\n",
      "train loss:0.02747811297544893\n",
      "train loss:0.02178588427140298\n",
      "train loss:0.02934636073957746\n",
      "train loss:0.03588636390870416\n",
      "train loss:0.11864814379125965\n",
      "train loss:0.047275064241416836\n",
      "train loss:0.03523038757971134\n",
      "train loss:0.10736817936224403\n",
      "train loss:0.009472530601765476\n",
      "train loss:0.03418288564200816\n",
      "train loss:0.09142374875105688\n",
      "train loss:0.027690552235253362\n",
      "train loss:0.04573113436765163\n",
      "train loss:0.014184695521538082\n",
      "train loss:0.07770160027458714\n",
      "train loss:0.047562088309143095\n",
      "train loss:0.09632096290890452\n",
      "train loss:0.014201392160763637\n",
      "train loss:0.03707933265608967\n",
      "train loss:0.031774677201655104\n",
      "train loss:0.01975100708741708\n",
      "train loss:0.01530652407647996\n",
      "train loss:0.07148305552169167\n",
      "train loss:0.04785384982308342\n",
      "train loss:0.07459198478144106\n",
      "train loss:0.01154576202993199\n",
      "train loss:0.046470809110817024\n",
      "train loss:0.09294486516677741\n",
      "train loss:0.09441783864150316\n",
      "train loss:0.03790449345293936\n",
      "train loss:0.08613613842257369\n",
      "train loss:0.0878216200866491\n",
      "train loss:0.013346803379733452\n",
      "train loss:0.024747156392844305\n",
      "train loss:0.032487832224194574\n",
      "train loss:0.048947862327882\n",
      "train loss:0.034731915789752515\n",
      "train loss:0.029013242467631196\n",
      "train loss:0.03717807146010842\n",
      "train loss:0.08472240472278468\n",
      "train loss:0.050638599197391616\n",
      "train loss:0.027095202214694622\n",
      "train loss:0.044559436550884256\n",
      "train loss:0.09380066213028453\n",
      "train loss:0.035543475649956864\n",
      "train loss:0.0159831299296711\n",
      "train loss:0.023388284942954436\n",
      "train loss:0.04335825434383324\n",
      "train loss:0.038412611719261344\n",
      "train loss:0.017315621283281648\n",
      "train loss:0.02089584047389468\n",
      "train loss:0.006688268340787179\n",
      "train loss:0.07140682224336592\n",
      "train loss:0.06069341389618295\n",
      "train loss:0.06363661140029021\n",
      "train loss:0.027389571477831214\n",
      "train loss:0.03287325159877992\n",
      "train loss:0.03657111725652292\n",
      "train loss:0.06386815332852684\n",
      "train loss:0.06196150841241586\n",
      "train loss:0.03328452950900435\n",
      "train loss:0.05374288353675267\n",
      "train loss:0.021114733841782165\n",
      "train loss:0.01520489229516989\n",
      "train loss:0.02011207075478164\n",
      "train loss:0.042806571909255034\n",
      "train loss:0.04051713114538956\n",
      "train loss:0.015495378810403908\n",
      "train loss:0.0744540493834769\n",
      "train loss:0.009983919263028503\n",
      "train loss:0.04169089994304375\n",
      "train loss:0.0538426479223488\n",
      "train loss:0.08859835515782193\n",
      "train loss:0.01966166130700971\n",
      "train loss:0.017776702206261038\n",
      "train loss:0.0711001161024582\n",
      "train loss:0.009595041578594317\n",
      "train loss:0.01100849532958174\n",
      "train loss:0.046123193974902846\n",
      "train loss:0.03743635672219181\n",
      "train loss:0.03579314391465029\n",
      "train loss:0.01826518861496754\n",
      "train loss:0.044340827685209826\n",
      "train loss:0.03542819367386675\n",
      "train loss:0.023360185262076137\n",
      "train loss:0.022289231543255756\n",
      "train loss:0.03261912299689234\n",
      "train loss:0.04400389078715198\n",
      "train loss:0.035064982049336904\n",
      "train loss:0.060230906982844744\n",
      "=== epoch:5, train acc:0.9873414855072464, test acc:0.9790760869565217 ===\n",
      "train loss:0.024559181704761495\n",
      "train loss:0.021530818043250657\n",
      "train loss:0.05134747175185486\n",
      "train loss:0.013019654965678194\n",
      "train loss:0.055946952565495554\n",
      "train loss:0.1008613983503493\n",
      "train loss:0.06307642081421479\n",
      "train loss:0.029031867885832292\n",
      "train loss:0.02575283425719525\n",
      "train loss:0.04487825353584031\n",
      "train loss:0.014707967151576441\n",
      "train loss:0.02800116305166146\n",
      "train loss:0.0059727543127969926\n",
      "train loss:0.016466931539861562\n",
      "train loss:0.029552245391787867\n",
      "train loss:0.03633503821763067\n",
      "train loss:0.015595172270007392\n",
      "train loss:0.07051525828138704\n",
      "train loss:0.032444759885515796\n",
      "train loss:0.0339408998781728\n",
      "train loss:0.025190274448066704\n",
      "train loss:0.011366064938219468\n",
      "train loss:0.08691786438041231\n",
      "train loss:0.021191714811508725\n",
      "train loss:0.016005092568608773\n",
      "train loss:0.022203775757256235\n",
      "train loss:0.01712573422869493\n",
      "train loss:0.020961680120133906\n",
      "train loss:0.05602378457846009\n",
      "train loss:0.007431188636473463\n",
      "train loss:0.025147073824978036\n",
      "train loss:0.029898065809832862\n",
      "train loss:0.008590800928946445\n",
      "train loss:0.025119789861504226\n",
      "train loss:0.01585257957765969\n",
      "train loss:0.09156839435387971\n",
      "train loss:0.011325567926084825\n",
      "train loss:0.026220075805436456\n",
      "train loss:0.04153997152456297\n",
      "train loss:0.06786332007355286\n",
      "train loss:0.014836899445551685\n",
      "train loss:0.024517304852594363\n",
      "train loss:0.0362088876480394\n",
      "train loss:0.015971512870761154\n",
      "train loss:0.04946050014169332\n",
      "train loss:0.029264494568338337\n",
      "train loss:0.10451971318961183\n",
      "train loss:0.04018400983887352\n",
      "train loss:0.016537525683126834\n",
      "train loss:0.04891043778110255\n",
      "train loss:0.04306353397779576\n",
      "train loss:0.01055786072578068\n",
      "train loss:0.015071196366484728\n",
      "train loss:0.04160103254557091\n",
      "train loss:0.013770438455156072\n",
      "train loss:0.009042760954218191\n",
      "train loss:0.027634172729548837\n",
      "train loss:0.003989791649822375\n",
      "train loss:0.027939979937821935\n",
      "train loss:0.049894983821551796\n",
      "train loss:0.02798889892110327\n",
      "train loss:0.05786180546540279\n",
      "train loss:0.027528363734546923\n",
      "train loss:0.052588892881180106\n",
      "train loss:0.02086593140232536\n",
      "train loss:0.025853790692180155\n",
      "train loss:0.035640513934033016\n",
      "train loss:0.0885510933487983\n",
      "train loss:0.03955811991104893\n",
      "train loss:0.05439090484466469\n",
      "train loss:0.018099234840941242\n",
      "train loss:0.05642710899942057\n",
      "train loss:0.06010141697021195\n",
      "train loss:0.08265837830094995\n",
      "train loss:0.060460832705640205\n",
      "train loss:0.07641079442404718\n",
      "train loss:0.023795900300910115\n",
      "train loss:0.052453063238682866\n",
      "train loss:0.027557019100602036\n",
      "train loss:0.06957697607864975\n",
      "train loss:0.02989835589144788\n",
      "train loss:0.018558013889250997\n",
      "train loss:0.016631813510946274\n",
      "train loss:0.034875122213897346\n",
      "train loss:0.023516182405516266\n",
      "train loss:0.1180250891123169\n",
      "train loss:0.019261759778580998\n",
      "train loss:0.03787435722440853\n",
      "train loss:0.027500054099863173\n",
      "train loss:0.03808129633809387\n",
      "train loss:0.022163023933783114\n",
      "train loss:0.021186522375074273\n",
      "train loss:0.022104242638397123\n",
      "train loss:0.06625606303521664\n",
      "train loss:0.0638389206965172\n",
      "train loss:0.020216794967119957\n",
      "train loss:0.01712178466600369\n",
      "train loss:0.03441654917747077\n",
      "train loss:0.011995742437213556\n",
      "train loss:0.0730087788441483\n",
      "train loss:0.051137540869031266\n",
      "train loss:0.011851728876541305\n",
      "train loss:0.03521609878525791\n",
      "train loss:0.027374516405436106\n",
      "train loss:0.03312427816541483\n",
      "train loss:0.01449054933938613\n",
      "train loss:0.012065709941756412\n",
      "train loss:0.017719610244865885\n",
      "train loss:0.031148514145235243\n",
      "train loss:0.005564736919507529\n",
      "train loss:0.012121155626900764\n",
      "train loss:0.032829699637592295\n",
      "train loss:0.003006815031597671\n",
      "train loss:0.0022537060248774294\n",
      "train loss:0.05520959205935883\n",
      "train loss:0.008330805019825254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.039237902245849596\n",
      "train loss:0.01686762886585843\n",
      "train loss:0.003837709963212706\n",
      "train loss:0.0638912037720317\n",
      "train loss:0.05084512747765493\n",
      "train loss:0.03610167955151859\n",
      "train loss:0.016577752650185292\n",
      "train loss:0.029835999714729687\n",
      "train loss:0.07303454651800305\n",
      "train loss:0.05728733517467397\n",
      "train loss:0.018541277730252975\n",
      "train loss:0.018240245409953115\n",
      "train loss:0.02775454979277938\n",
      "train loss:0.033239149787346454\n",
      "train loss:0.0910489653613717\n",
      "train loss:0.020951932456393832\n",
      "train loss:0.03713769198583803\n",
      "train loss:0.01043837762791116\n",
      "train loss:0.024356111678290788\n",
      "train loss:0.0399779575651034\n",
      "train loss:0.03624465152036481\n",
      "train loss:0.027995599951595077\n",
      "train loss:0.024820815710180934\n",
      "train loss:0.10866202118375148\n",
      "train loss:0.022627368840482647\n",
      "train loss:0.01173389728299295\n",
      "train loss:0.023261139208329538\n",
      "train loss:0.025591911376065837\n",
      "train loss:0.013370187346447682\n",
      "train loss:0.03679058400218417\n",
      "train loss:0.01155059688424254\n",
      "train loss:0.01699681861816983\n",
      "train loss:0.04413830182801203\n",
      "train loss:0.0105516184263456\n",
      "train loss:0.07768462116219524\n",
      "train loss:0.014464112865249696\n",
      "train loss:0.10163302029075288\n",
      "train loss:0.005868904355512417\n",
      "train loss:0.08038497956219336\n",
      "train loss:0.07453194824178556\n",
      "train loss:0.07534772809364215\n",
      "train loss:0.030435339857443813\n",
      "train loss:0.031841445491842355\n",
      "train loss:0.007246254002472235\n",
      "train loss:0.00831271106141589\n",
      "train loss:0.033192920300365324\n",
      "train loss:0.014409826862877229\n",
      "train loss:0.025974430379689613\n",
      "train loss:0.02066902356490829\n",
      "train loss:0.049671131200581795\n",
      "train loss:0.06918949189737775\n",
      "train loss:0.04148936486268463\n",
      "train loss:0.0203166523476063\n",
      "train loss:0.022523847986020504\n",
      "train loss:0.016361195040947383\n",
      "train loss:0.02343492681907728\n",
      "train loss:0.03305535141501018\n",
      "train loss:0.04650308212343765\n",
      "train loss:0.13336706420900601\n",
      "train loss:0.06211046720130568\n",
      "train loss:0.027733583594002426\n",
      "train loss:0.0240489410514243\n",
      "train loss:0.03468531071693666\n",
      "train loss:0.02941088333502239\n",
      "train loss:0.029414950433544747\n",
      "train loss:0.03426113349520073\n",
      "train loss:0.01984653935059528\n",
      "train loss:0.048127827401042676\n",
      "train loss:0.05598172260509277\n",
      "train loss:0.009059269539026537\n",
      "train loss:0.01064971875902607\n",
      "train loss:0.05112948857976815\n",
      "train loss:0.0656724734613375\n",
      "train loss:0.04890234172211987\n",
      "train loss:0.02470656319716291\n",
      "train loss:0.024269823850678916\n",
      "train loss:0.15711766081385609\n",
      "train loss:0.035000882707798844\n",
      "train loss:0.025089801219578132\n",
      "train loss:0.03538691181922771\n",
      "train loss:0.04817162530486573\n",
      "train loss:0.020526509304750237\n",
      "train loss:0.02282554927574987\n",
      "train loss:0.0194802250152779\n",
      "train loss:0.013388969152326066\n",
      "train loss:0.0627167889190898\n",
      "train loss:0.03812582554363773\n",
      "train loss:0.04286683796350278\n",
      "train loss:0.02851197653637153\n",
      "train loss:0.08093212239084403\n",
      "train loss:0.01688069235541547\n",
      "train loss:0.013097921134408897\n",
      "train loss:0.018761951985834444\n",
      "train loss:0.038784207925598904\n",
      "train loss:0.06423590844856673\n",
      "train loss:0.02601426636231481\n",
      "train loss:0.010328974303280388\n",
      "train loss:0.007941182739804688\n",
      "train loss:0.013210419855197617\n",
      "train loss:0.007179256203530747\n",
      "train loss:0.03383846345975986\n",
      "train loss:0.02069633346438172\n",
      "train loss:0.012626552317924316\n",
      "train loss:0.0338741137956484\n",
      "train loss:0.02805025933433788\n",
      "train loss:0.008565759498443242\n",
      "train loss:0.018770600019556865\n",
      "train loss:0.006603962586953412\n",
      "train loss:0.0086131150901723\n",
      "train loss:0.032369559206595684\n",
      "train loss:0.00809136255075574\n",
      "train loss:0.025027731822578066\n",
      "train loss:0.11984407755376933\n",
      "train loss:0.08019262542543035\n",
      "train loss:0.051777232078787476\n",
      "train loss:0.04640508541388318\n",
      "train loss:0.06704550218277532\n",
      "train loss:0.04186714174145382\n",
      "train loss:0.047955201746346404\n",
      "train loss:0.07118199165511714\n",
      "train loss:0.07682106288449651\n",
      "train loss:0.015041065213066333\n",
      "train loss:0.022111096242003574\n",
      "train loss:0.057765472813160176\n",
      "train loss:0.051280709677846675\n",
      "train loss:0.04739724405818455\n",
      "train loss:0.01962872714621306\n",
      "train loss:0.018465730707151125\n",
      "train loss:0.01533729335078152\n",
      "train loss:0.025549867148896385\n",
      "train loss:0.025956826723005817\n",
      "train loss:0.02157925116435367\n",
      "train loss:0.01397472745296456\n",
      "train loss:0.010756054895602394\n",
      "train loss:0.08425780198012275\n",
      "train loss:0.04739037407123767\n",
      "train loss:0.07787863419411355\n",
      "train loss:0.017843684337960273\n",
      "train loss:0.01836746295914303\n",
      "train loss:0.05466144015620316\n",
      "train loss:0.025849653653286026\n",
      "train loss:0.09063113461725596\n",
      "train loss:0.008135257566581807\n",
      "train loss:0.013998183971015517\n",
      "train loss:0.009845224227984579\n",
      "train loss:0.02618922663164239\n",
      "train loss:0.07014611774298334\n",
      "train loss:0.02093555258508689\n",
      "train loss:0.027116028065579546\n",
      "train loss:0.03494215562301292\n",
      "train loss:0.009404303165825273\n",
      "train loss:0.045764356955264186\n",
      "train loss:0.008715827436632352\n",
      "train loss:0.02620731024588401\n",
      "train loss:0.017353980960065598\n",
      "train loss:0.0132082692194697\n",
      "train loss:0.020181079585109025\n",
      "train loss:0.0064293836471060834\n",
      "train loss:0.011245795856511077\n",
      "train loss:0.02217066605400443\n",
      "train loss:0.0174892219394177\n",
      "train loss:0.03795226588834897\n",
      "train loss:0.005970183821158401\n",
      "train loss:0.02285975463280424\n",
      "train loss:0.04327867017106838\n",
      "train loss:0.03326027356394243\n",
      "train loss:0.019586122186358447\n",
      "train loss:0.05017762359574464\n",
      "train loss:0.02474287143367372\n",
      "train loss:0.006160937979643057\n",
      "train loss:0.064978490450717\n",
      "train loss:0.04414804875739885\n",
      "train loss:0.01158000457372913\n",
      "train loss:0.033093099552089146\n",
      "train loss:0.024086865555126832\n",
      "train loss:0.04981321894763526\n",
      "train loss:0.022041216999825135\n",
      "train loss:0.026051278382416037\n",
      "train loss:0.026951009673277616\n",
      "train loss:0.014785271178851535\n",
      "train loss:0.005685730637577125\n",
      "train loss:0.03098594772491513\n",
      "train loss:0.06325223826940236\n",
      "train loss:0.026813641599763103\n",
      "train loss:0.03335687900945145\n",
      "train loss:0.05013715673558376\n",
      "train loss:0.076474598144332\n",
      "train loss:0.025282212939069345\n",
      "train loss:0.009273259095508695\n",
      "train loss:0.027058406378718414\n",
      "train loss:0.0207475040537445\n",
      "train loss:0.040367672274646486\n",
      "train loss:0.06634503944365995\n",
      "train loss:0.020172603608946602\n",
      "train loss:0.010931266812442997\n",
      "train loss:0.02194489754083356\n",
      "train loss:0.03881378565165429\n",
      "train loss:0.010151347659182545\n",
      "train loss:0.029005284621882222\n",
      "train loss:0.018295923144898554\n",
      "train loss:0.05754700070251427\n",
      "train loss:0.01986020804199949\n",
      "train loss:0.05266080632219725\n",
      "train loss:0.01037320664708048\n",
      "train loss:0.012594334993074037\n",
      "train loss:0.019190031485842196\n",
      "train loss:0.008746445564515629\n",
      "train loss:0.041312601450315296\n",
      "train loss:0.008577590938750579\n",
      "train loss:0.009542182803671773\n",
      "train loss:0.01991516578626618\n",
      "train loss:0.026262727341594974\n",
      "train loss:0.04362733330938753\n",
      "train loss:0.010666027427667571\n",
      "train loss:0.03351534105054566\n",
      "train loss:0.021020490174493884\n",
      "train loss:0.015067916646688101\n",
      "train loss:0.07222318531731038\n",
      "train loss:0.05488289088297549\n",
      "train loss:0.012894947218995024\n",
      "train loss:0.015857121026105165\n",
      "train loss:0.07204238679211829\n",
      "train loss:0.022220702850663902\n",
      "train loss:0.008843706147874284\n",
      "train loss:0.05370586425658666\n",
      "train loss:0.010749441028046314\n",
      "train loss:0.08872349427997644\n",
      "train loss:0.011551274011764536\n",
      "train loss:0.03778527627975984\n",
      "train loss:0.04703788651033983\n",
      "train loss:0.022813122172898038\n",
      "train loss:0.026057381323345775\n",
      "train loss:0.013883026889603389\n",
      "train loss:0.011911223141971568\n",
      "train loss:0.00858909429822466\n",
      "train loss:0.014643988676558892\n",
      "train loss:0.007802518988564598\n",
      "train loss:0.006413347867340556\n",
      "train loss:0.06525903931118303\n",
      "train loss:0.008521078854778834\n",
      "train loss:0.04794668772071157\n",
      "train loss:0.03432708032747484\n",
      "train loss:0.011603018059650683\n",
      "train loss:0.08259417315035215\n",
      "train loss:0.0059414882077822246\n",
      "train loss:0.019811915796519766\n",
      "train loss:0.0048588350430921745\n",
      "train loss:0.04055534643732049\n",
      "train loss:0.024183380789943552\n",
      "train loss:0.03511544754310197\n",
      "train loss:0.023197492074169105\n",
      "train loss:0.03014128325290884\n",
      "=== epoch:6, train acc:0.9904438405797101, test acc:0.9823369565217391 ===\n",
      "train loss:0.0062349380848736565\n",
      "train loss:0.007693329927646818\n",
      "train loss:0.013808051780934454\n",
      "train loss:0.011439055688216375\n",
      "train loss:0.014087245510331699\n",
      "train loss:0.04668721012315821\n",
      "train loss:0.06394211192824872\n",
      "train loss:0.007307414089638082\n",
      "train loss:0.03561116287862665\n",
      "train loss:0.02685642795987371\n",
      "train loss:0.028526369024254924\n",
      "train loss:0.004370225374126999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.011765931418884561\n",
      "train loss:0.07810294143333675\n",
      "train loss:0.023392348308850144\n",
      "train loss:0.05830708432778883\n",
      "train loss:0.05383391443801824\n",
      "train loss:0.018716012990179148\n",
      "train loss:0.028945270785666347\n",
      "train loss:0.017918971483075783\n",
      "train loss:0.04098598219894599\n",
      "train loss:0.04369602125207986\n",
      "train loss:0.03323138209934627\n",
      "train loss:0.03363914007689596\n",
      "train loss:0.028147928271755218\n",
      "train loss:0.012102469934979376\n",
      "train loss:0.028647586118204886\n",
      "train loss:0.01972060517580563\n",
      "train loss:0.0554171445801976\n",
      "train loss:0.012515098948080497\n",
      "train loss:0.03373697368079446\n",
      "train loss:0.035386303124818407\n",
      "train loss:0.005284099247595056\n",
      "train loss:0.05084575069245002\n",
      "train loss:0.025634473622240715\n",
      "train loss:0.006015704937234859\n",
      "train loss:0.008817083935862054\n",
      "train loss:0.02520398531337429\n",
      "train loss:0.05290054945099982\n",
      "train loss:0.01677289566822595\n",
      "train loss:0.0647168227855749\n",
      "train loss:0.04034492033032115\n",
      "train loss:0.009084285575708147\n",
      "train loss:0.03895320820657026\n",
      "train loss:0.034102900993286396\n",
      "train loss:0.031187683330102793\n",
      "train loss:0.011526277751054921\n",
      "train loss:0.02955683888964881\n",
      "train loss:0.015826508297195944\n",
      "train loss:0.005653874009363089\n",
      "train loss:0.05693553533108518\n",
      "train loss:0.05266585751900695\n",
      "train loss:0.009131591878925881\n",
      "train loss:0.03207600355137452\n",
      "train loss:0.03988120807794427\n",
      "train loss:0.007862400332047149\n",
      "train loss:0.05677608242053471\n",
      "train loss:0.04422105910213026\n",
      "train loss:0.07236402731617837\n",
      "train loss:0.08436772309791915\n",
      "train loss:0.010300670924505567\n",
      "train loss:0.0775557750313209\n",
      "train loss:0.03247162774275925\n",
      "train loss:0.012270463800186428\n",
      "train loss:0.020163953330022983\n",
      "train loss:0.030707801453674435\n",
      "train loss:0.052128548653608255\n",
      "train loss:0.011479258841581725\n",
      "train loss:0.00479757860795813\n",
      "train loss:0.02028005709984303\n",
      "train loss:0.015200356590543503\n",
      "train loss:0.01623571981367789\n",
      "train loss:0.023765040054625874\n",
      "train loss:0.00840599435609356\n",
      "train loss:0.012685393946037116\n",
      "train loss:0.008296484381049095\n",
      "train loss:0.11548073829373534\n",
      "train loss:0.03936532050889312\n",
      "train loss:0.01603928707246967\n",
      "train loss:0.039812039154707324\n",
      "train loss:0.011692380585827637\n",
      "train loss:0.0204189397295653\n",
      "train loss:0.034956874946967524\n",
      "train loss:0.005189908062250519\n",
      "train loss:0.023970986864472188\n",
      "train loss:0.0032803898248321564\n",
      "train loss:0.015519547281633236\n",
      "train loss:0.004949971341537515\n",
      "train loss:0.020603876062405768\n",
      "train loss:0.057549970861695506\n",
      "train loss:0.03470451248528678\n",
      "train loss:0.035636968692516285\n",
      "train loss:0.043422283145940224\n",
      "train loss:0.03755870440514902\n",
      "train loss:0.015852984463921645\n",
      "train loss:0.014161035662292588\n",
      "train loss:0.05254041518624001\n",
      "train loss:0.020750096747901292\n",
      "train loss:0.06117726132142118\n",
      "train loss:0.014017034261178478\n",
      "train loss:0.017526084530323317\n",
      "train loss:0.05943299039986957\n",
      "train loss:0.012797096727539573\n",
      "train loss:0.05422987994513197\n",
      "train loss:0.023734309393470633\n",
      "train loss:0.008016738346978943\n",
      "train loss:0.01642375658011218\n",
      "train loss:0.013600235584353365\n",
      "train loss:0.01937875331430327\n",
      "train loss:0.01779869688054686\n",
      "train loss:0.06399658545286699\n",
      "train loss:0.004883155626562441\n",
      "train loss:0.013259022433496078\n",
      "train loss:0.049995803602943714\n",
      "train loss:0.01502227389266644\n",
      "train loss:0.017488140889885286\n",
      "train loss:0.04997251882188199\n",
      "train loss:0.032895776510097924\n",
      "train loss:0.006108523225468835\n",
      "train loss:0.013923052295217174\n",
      "train loss:0.013085533646176943\n",
      "train loss:0.03466653806063591\n",
      "train loss:0.03425450688375838\n",
      "train loss:0.026780292368699037\n",
      "train loss:0.0029070555680579006\n",
      "train loss:0.03925545326901688\n",
      "train loss:0.009819834779020679\n",
      "train loss:0.01969373421510112\n",
      "train loss:0.006265344386504904\n",
      "train loss:0.025709811200835944\n",
      "train loss:0.015873284787123795\n",
      "train loss:0.03207119073484445\n",
      "train loss:0.042836548565399056\n",
      "train loss:0.004438848644661534\n",
      "train loss:0.01152293494606017\n",
      "train loss:0.028757470128060715\n",
      "train loss:0.013839010265599494\n",
      "train loss:0.01945318193901408\n",
      "train loss:0.0701271099993139\n",
      "train loss:0.04641822222025757\n",
      "train loss:0.02820592381336359\n",
      "train loss:0.02275534267217803\n",
      "train loss:0.033547812603212636\n",
      "train loss:0.01439075763066858\n",
      "train loss:0.01547030102231477\n",
      "train loss:0.02256505345099446\n",
      "train loss:0.0060295014483617935\n",
      "train loss:0.046039766074919754\n",
      "train loss:0.007033346983435924\n",
      "train loss:0.014378150170539948\n",
      "train loss:0.01864831705212414\n",
      "train loss:0.02366821135950053\n",
      "train loss:0.03875151251460426\n",
      "train loss:0.02081370355327706\n",
      "train loss:0.03296221348737789\n",
      "train loss:0.024774333337625694\n",
      "train loss:0.022399737206599307\n",
      "train loss:0.009696421682163968\n",
      "train loss:0.03469748875815819\n",
      "train loss:0.008451384432305914\n",
      "train loss:0.024804079216434405\n",
      "train loss:0.03658362861090073\n",
      "train loss:0.005411845505512354\n",
      "train loss:0.007327917167017804\n",
      "train loss:0.016508895911883738\n",
      "train loss:0.04266305999738028\n",
      "train loss:0.032568800853698385\n",
      "train loss:0.005330008343565471\n",
      "train loss:0.039415987749549984\n",
      "train loss:0.05185879421577989\n",
      "train loss:0.045936647210324116\n",
      "train loss:0.03812904764266205\n",
      "train loss:0.025999737241957102\n",
      "train loss:0.03643562971667139\n",
      "train loss:0.011401423597371467\n",
      "train loss:0.025892741325891925\n",
      "train loss:0.019702810919366313\n",
      "train loss:0.016887907359840606\n",
      "train loss:0.0031126062548084824\n",
      "train loss:0.07050421542793205\n",
      "train loss:0.017998317706213286\n",
      "train loss:0.031305910180429254\n",
      "train loss:0.02269922991029723\n",
      "train loss:0.03417379110369771\n",
      "train loss:0.010913312260714073\n",
      "train loss:0.014877146939481663\n",
      "train loss:0.02928255030412569\n",
      "train loss:0.006680405689964732\n",
      "train loss:0.015788767749376837\n",
      "train loss:0.022013343651223424\n",
      "train loss:0.010126562115610345\n",
      "train loss:0.02072858778710841\n",
      "train loss:0.014412850925427894\n",
      "train loss:0.006404912849715115\n",
      "train loss:0.00827675003249288\n",
      "train loss:0.045725293033171\n",
      "train loss:0.007844375558069246\n",
      "train loss:0.0075204347583922105\n",
      "train loss:0.03672724115560421\n",
      "train loss:0.014648692035802343\n",
      "train loss:0.030115971881499903\n",
      "train loss:0.00446695476908827\n",
      "train loss:0.011596310549908508\n",
      "train loss:0.03876115570935087\n",
      "train loss:0.012019939437360693\n",
      "train loss:0.032015215589661625\n",
      "train loss:0.047393794429403786\n",
      "train loss:0.013342474953033612\n",
      "train loss:0.01907697323804502\n",
      "train loss:0.017093157257247263\n",
      "train loss:0.036271385517965535\n",
      "train loss:0.01384398298804078\n",
      "train loss:0.011595694687336337\n",
      "train loss:0.008390620464000657\n",
      "train loss:0.021790559562981305\n",
      "train loss:0.03399478335196187\n",
      "train loss:0.011931017077922107\n",
      "train loss:0.013598758500695241\n",
      "train loss:0.05191597632415791\n",
      "train loss:0.0740740778737068\n",
      "train loss:0.018712789109327603\n",
      "train loss:0.03253058688846637\n",
      "train loss:0.012526471191644508\n",
      "train loss:0.018523442472346316\n",
      "train loss:0.031729343785920176\n",
      "train loss:0.06001221488714939\n",
      "train loss:0.07984550371391645\n",
      "train loss:0.02266239916646194\n",
      "train loss:0.01989289609209196\n",
      "train loss:0.005971749325177415\n",
      "train loss:0.008263785943420213\n",
      "train loss:0.01939607059924004\n",
      "train loss:0.03228711061818433\n",
      "train loss:0.006637509818330356\n",
      "train loss:0.010152889875565969\n",
      "train loss:0.024006586195507683\n",
      "train loss:0.06145758090475121\n",
      "train loss:0.010313920579109943\n",
      "train loss:0.03486812387821413\n",
      "train loss:0.025492487245202332\n",
      "train loss:0.01898263526183234\n",
      "train loss:0.05249861040967497\n",
      "train loss:0.03209282520641393\n",
      "train loss:0.04063335117044484\n",
      "train loss:0.005301290524203641\n",
      "train loss:0.014980498735451271\n",
      "train loss:0.16110658154732757\n",
      "train loss:0.015418478414705809\n",
      "train loss:0.04228319458680834\n",
      "train loss:0.003272345338084731\n",
      "train loss:0.04944348279221969\n",
      "train loss:0.07006837952307628\n",
      "train loss:0.030638449599446466\n",
      "train loss:0.07835051616350984\n",
      "train loss:0.01700176918517399\n",
      "train loss:0.012170218225995988\n",
      "train loss:0.00819823744347452\n",
      "train loss:0.013829500174545995\n",
      "train loss:0.015984137971087057\n",
      "train loss:0.00651322246822351\n",
      "train loss:0.009420985626952764\n",
      "train loss:0.03348550199935751\n",
      "train loss:0.029152878838553404\n",
      "train loss:0.007599211191671062\n",
      "train loss:0.011763323856554004\n",
      "train loss:0.031437925709290174\n",
      "train loss:0.01375419380626681\n",
      "train loss:0.01674162799961177\n",
      "train loss:0.040203836044808386\n",
      "train loss:0.01090753955830226\n",
      "train loss:0.020791697616294503\n",
      "train loss:0.02186897964993125\n",
      "train loss:0.022637366122031852\n",
      "train loss:0.0677229390802133\n",
      "train loss:0.044062631925720463\n",
      "train loss:0.01691898794337999\n",
      "train loss:0.06434889092534171\n",
      "train loss:0.009387854624196954\n",
      "train loss:0.025174821279053076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.03715429880186621\n",
      "train loss:0.023209531851367624\n",
      "train loss:0.030942568136293473\n",
      "train loss:0.0573887951219474\n",
      "train loss:0.010866032552624076\n",
      "train loss:0.006321849291075553\n",
      "train loss:0.008998158280929222\n",
      "train loss:0.013670637198498923\n",
      "train loss:0.007084746379714292\n",
      "train loss:0.009694885065022604\n",
      "train loss:0.003787536988165883\n",
      "train loss:0.02595141248647311\n",
      "train loss:0.012516160813967024\n",
      "train loss:0.007043955987705897\n",
      "train loss:0.011889619174355041\n",
      "train loss:0.011127146327370314\n",
      "train loss:0.08868615027429998\n",
      "train loss:0.018946810504551048\n",
      "train loss:0.007413578873963711\n",
      "train loss:0.0055424302746513\n",
      "train loss:0.01653476524542409\n",
      "train loss:0.008095601252951567\n",
      "train loss:0.02334810632293939\n",
      "train loss:0.0053205939866164875\n",
      "train loss:0.013563671298441986\n",
      "train loss:0.045022182327714905\n",
      "train loss:0.010294099423797583\n",
      "train loss:0.06779774357907509\n",
      "train loss:0.00452831243188865\n",
      "train loss:0.002961472783344015\n",
      "train loss:0.042076288106256715\n",
      "train loss:0.06183673834694628\n",
      "train loss:0.017446337131133433\n",
      "train loss:0.007354289252581959\n",
      "train loss:0.037500414230145676\n",
      "train loss:0.00876305795386526\n",
      "train loss:0.012742471794000393\n",
      "train loss:0.006886360445016949\n",
      "train loss:0.014124764735070537\n",
      "train loss:0.005593600188241482\n",
      "train loss:0.03003284467049069\n",
      "train loss:0.005257802425549157\n",
      "train loss:0.008116321807564577\n",
      "train loss:0.009293856379974775\n",
      "train loss:0.0036632831127860893\n",
      "train loss:0.04704063016809893\n",
      "train loss:0.038776202088461977\n",
      "train loss:0.008370595629361346\n",
      "train loss:0.003979206824128873\n",
      "train loss:0.020171274148851955\n",
      "train loss:0.009313220618048604\n",
      "train loss:0.026659155332746532\n",
      "train loss:0.03634500484288276\n",
      "train loss:0.025678390487352938\n",
      "train loss:0.04115819845199419\n",
      "train loss:0.037540965896648584\n",
      "train loss:0.09457947934847222\n",
      "train loss:0.024134618704515602\n",
      "train loss:0.0036791482115419273\n",
      "train loss:0.008560004982669353\n",
      "train loss:0.016858228935225924\n",
      "train loss:0.012075779587217127\n",
      "train loss:0.016502256437879257\n",
      "train loss:0.03905406555000864\n",
      "train loss:0.006482498127171508\n",
      "train loss:0.06796882685785294\n",
      "train loss:0.10215835933536539\n",
      "train loss:0.025936120179090135\n",
      "train loss:0.029935091945535047\n",
      "train loss:0.03261474192614996\n",
      "train loss:0.017076052404565844\n",
      "train loss:0.04260976975969029\n",
      "train loss:0.043735276702375994\n",
      "train loss:0.03266847918542118\n",
      "train loss:0.008115278599773627\n",
      "train loss:0.05785346677455503\n",
      "train loss:0.023619701418950534\n",
      "train loss:0.07519077459395745\n",
      "train loss:0.03965923649445978\n",
      "train loss:0.00882176789682118\n",
      "train loss:0.03624407825941189\n",
      "train loss:0.04655634251215643\n",
      "train loss:0.017905394277153652\n",
      "train loss:0.05049610839988545\n",
      "train loss:0.06483563707408796\n",
      "train loss:0.026929230866277572\n",
      "train loss:0.04734957079631819\n",
      "train loss:0.009999081765132676\n",
      "train loss:0.014283069090952285\n",
      "=== epoch:7, train acc:0.9914855072463769, test acc:0.9816123188405798 ===\n",
      "train loss:0.05780445281116673\n",
      "train loss:0.009291165328966126\n",
      "train loss:0.03264574465310085\n",
      "train loss:0.008165914550356033\n",
      "train loss:0.008780610201545051\n",
      "train loss:0.038625131166601816\n",
      "train loss:0.012590396301168877\n",
      "train loss:0.02896752319151697\n",
      "train loss:0.009281850836856365\n",
      "train loss:0.03906326839598716\n",
      "train loss:0.0498877606546358\n",
      "train loss:0.032827143130084356\n",
      "train loss:0.010391019684336195\n",
      "train loss:0.01409561151405819\n",
      "train loss:0.038721476768116644\n",
      "train loss:0.01865573255159053\n",
      "train loss:0.007622951198907575\n",
      "train loss:0.013085114781683022\n",
      "train loss:0.006227346943574058\n",
      "train loss:0.012389685090856096\n",
      "train loss:0.007736929200681511\n",
      "train loss:0.03075096483692171\n",
      "train loss:0.007568308832132659\n",
      "train loss:0.008282737379419869\n",
      "train loss:0.013759305463730146\n",
      "train loss:0.010648414862724517\n",
      "train loss:0.01734548264294494\n",
      "train loss:0.020220901115542804\n",
      "train loss:0.0057791931028308985\n",
      "train loss:0.059819281219572966\n",
      "train loss:0.013637915247625396\n",
      "train loss:0.03573278004134461\n",
      "train loss:0.059730890023219935\n",
      "train loss:0.07770093380760497\n",
      "train loss:0.01005127442202085\n",
      "train loss:0.010893287694968138\n",
      "train loss:0.029102260398098115\n",
      "train loss:0.02228829528495879\n",
      "train loss:0.03041521285363623\n",
      "train loss:0.013461201280949468\n",
      "train loss:0.026559475166238848\n",
      "train loss:0.011560729692275144\n",
      "train loss:0.042757091568266854\n",
      "train loss:0.029478700716433295\n",
      "train loss:0.030906143224661543\n",
      "train loss:0.007301394227046591\n",
      "train loss:0.024564938655994597\n",
      "train loss:0.016473380820631336\n",
      "train loss:0.019260673294832022\n",
      "train loss:0.05732893137289979\n",
      "train loss:0.016869330844645106\n",
      "train loss:0.008303627402077839\n",
      "train loss:0.026459394062173687\n",
      "train loss:0.007742481109662944\n",
      "train loss:0.011228969021534157\n",
      "train loss:0.034909953307341826\n",
      "train loss:0.010625604159395419\n",
      "train loss:0.04843350662495392\n",
      "train loss:0.006495426174960887\n",
      "train loss:0.023351714622889208\n",
      "train loss:0.010770368479091559\n",
      "train loss:0.015132037764992714\n",
      "train loss:0.033870790429063956\n",
      "train loss:0.035918940801070894\n",
      "train loss:0.0174092140967025\n",
      "train loss:0.0443498035195031\n",
      "train loss:0.05368211015397846\n",
      "train loss:0.02384049806201648\n",
      "train loss:0.011889637082034514\n",
      "train loss:0.05071182023157817\n",
      "train loss:0.016364742620895874\n",
      "train loss:0.014712734795474644\n",
      "train loss:0.06730713475043326\n",
      "train loss:0.01884938046389795\n",
      "train loss:0.038087734445701994\n",
      "train loss:0.027815933204884704\n",
      "train loss:0.00999565749206312\n",
      "train loss:0.011547771046821946\n",
      "train loss:0.012403315251214807\n",
      "train loss:0.006868182030242868\n",
      "train loss:0.010136138542265739\n",
      "train loss:0.010006667487666042\n",
      "train loss:0.04744727542358184\n",
      "train loss:0.07434775595147193\n",
      "train loss:0.026604305354164037\n",
      "train loss:0.018377420720598434\n",
      "train loss:0.06712420356815395\n",
      "train loss:0.010223933034897818\n",
      "train loss:0.008372875357393702\n",
      "train loss:0.016576311437149494\n",
      "train loss:0.04967988482210579\n",
      "train loss:0.013578227885454852\n",
      "train loss:0.012604844570250413\n",
      "train loss:0.03309145673156438\n",
      "train loss:0.04838827453699804\n",
      "train loss:0.016869904124483085\n",
      "train loss:0.010579509419063418\n",
      "train loss:0.06170168247192594\n",
      "train loss:0.0044867649553637385\n",
      "train loss:0.0068008772642027164\n",
      "train loss:0.03268540358792324\n",
      "train loss:0.006350686618320723\n",
      "train loss:0.009289145513057527\n",
      "train loss:0.005875831955184763\n",
      "train loss:0.01559717336906358\n",
      "train loss:0.020355396687949463\n",
      "train loss:0.011817849892686453\n",
      "train loss:0.009201990521188693\n",
      "train loss:0.0059027316741338265\n",
      "train loss:0.025602507754832658\n",
      "train loss:0.014968989121894571\n",
      "train loss:0.003574102463420348\n",
      "train loss:0.01200332584371767\n",
      "train loss:0.02551971598456079\n",
      "train loss:0.007631730319135357\n",
      "train loss:0.010669749971435485\n",
      "train loss:0.008368891738327146\n",
      "train loss:0.00835504041754284\n",
      "train loss:0.0030519932052158806\n",
      "train loss:0.003920740891770648\n",
      "train loss:0.03850288542212151\n",
      "train loss:0.017593738082640923\n",
      "train loss:0.004394623204464079\n",
      "train loss:0.024004920981866653\n",
      "train loss:0.021547612497240944\n",
      "train loss:0.06171427257108935\n",
      "train loss:0.014242612523189995\n",
      "train loss:0.006186895652830528\n",
      "train loss:0.007007062893132924\n",
      "train loss:0.0033213267424910954\n",
      "train loss:0.0029319902548412017\n",
      "train loss:0.013395039873898807\n",
      "train loss:0.01018004101982996\n",
      "train loss:0.05999749858818772\n",
      "train loss:0.02583678618451254\n",
      "train loss:0.050619662803725075\n",
      "train loss:0.004534169908753146\n",
      "train loss:0.0624598718077242\n",
      "train loss:0.04189912817255385\n",
      "train loss:0.02617537162637318\n",
      "train loss:0.04322813648699501\n",
      "train loss:0.06408688001771594\n",
      "train loss:0.05358298878355925\n",
      "train loss:0.025255532701560366\n",
      "train loss:0.048577455129039014\n",
      "train loss:0.016055847212287214\n",
      "train loss:0.010867923211067719\n",
      "train loss:0.015539505830845391\n",
      "train loss:0.012359729947575076\n",
      "train loss:0.042202106044630414\n",
      "train loss:0.054954706023401356\n",
      "train loss:0.020768929952484663\n",
      "train loss:0.04892830621305437\n",
      "train loss:0.025966204085657953\n",
      "train loss:0.009262279505870446\n",
      "train loss:0.004797024591397184\n",
      "train loss:0.04384683598930085\n",
      "train loss:0.016134031008978065\n",
      "train loss:0.022636486969356162\n",
      "train loss:0.05143047775899654\n",
      "train loss:0.013842624616142274\n",
      "train loss:0.009577545741482806\n",
      "train loss:0.01657068826306787\n",
      "train loss:0.005526840717634801\n",
      "train loss:0.004665272348878328\n",
      "train loss:0.03138599682341057\n",
      "train loss:0.005148641506993907\n",
      "train loss:0.02253445013811047\n",
      "train loss:0.011570298338051599\n",
      "train loss:0.013529071151153881\n",
      "train loss:0.004376299727652033\n",
      "train loss:0.025013861148455174\n",
      "train loss:0.029206756165336592\n",
      "train loss:0.013052110624424844\n",
      "train loss:0.0039022999725848015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.006730184388928347\n",
      "train loss:0.0059365318997123264\n",
      "train loss:0.025502700377681986\n",
      "train loss:0.005402765715259894\n",
      "train loss:0.009448438097566502\n",
      "train loss:0.012324214144657316\n",
      "train loss:0.0028978728664618525\n",
      "train loss:0.00719130411703421\n",
      "train loss:0.031889773250972676\n",
      "train loss:0.01846376012909725\n",
      "train loss:0.013745393179322283\n",
      "train loss:0.027021692982783133\n",
      "train loss:0.025088764296506316\n",
      "train loss:0.0032149532895965976\n",
      "train loss:0.016231692642110912\n",
      "train loss:0.03754554931477569\n",
      "train loss:0.02413701911794226\n",
      "train loss:0.05100635925457653\n",
      "train loss:0.014347725398343957\n",
      "train loss:0.0037789516368748667\n",
      "train loss:0.01709831946577663\n",
      "train loss:0.030829807018596486\n",
      "train loss:0.05972225491633384\n",
      "train loss:0.003657848986156445\n",
      "train loss:0.023256579681833364\n",
      "train loss:0.011746442497233608\n",
      "train loss:0.019629131719709533\n",
      "train loss:0.034439943545362214\n",
      "train loss:0.011100936360414756\n",
      "train loss:0.012401027236488987\n",
      "train loss:0.004579747072031247\n",
      "train loss:0.031986060546984114\n",
      "train loss:0.009192389354140691\n",
      "train loss:0.0032450806010756003\n",
      "train loss:0.02029659851845814\n",
      "train loss:0.021950384381735837\n",
      "train loss:0.002425285428327442\n",
      "train loss:0.008772302120878019\n",
      "train loss:0.0014163823049001929\n",
      "train loss:0.012942997511200899\n",
      "train loss:0.02078910515434881\n",
      "train loss:0.02208844516184368\n",
      "train loss:0.021120188769691815\n",
      "train loss:0.016029741546763857\n",
      "train loss:0.008613204363210733\n",
      "train loss:0.02319440357241709\n",
      "train loss:0.03309608540846517\n",
      "train loss:0.033716870852175326\n",
      "train loss:0.009916618078835825\n",
      "train loss:0.020782541898497972\n",
      "train loss:0.0031590019733243806\n",
      "train loss:0.02106000641348664\n",
      "train loss:0.02998399271139077\n",
      "train loss:0.041056504350765397\n",
      "train loss:0.050158753238349015\n",
      "train loss:0.015674317209541055\n",
      "train loss:0.014359417313995319\n",
      "train loss:0.00374081245923701\n",
      "train loss:0.01284747395922009\n",
      "train loss:0.011686439718667596\n",
      "train loss:0.0075306037215288855\n",
      "train loss:0.01853705805649906\n",
      "train loss:0.009540002734750823\n",
      "train loss:0.03596481791562651\n",
      "train loss:0.03175204662902723\n",
      "train loss:0.004928254898856044\n",
      "train loss:0.008613978689137502\n",
      "train loss:0.04804433853963888\n",
      "train loss:0.020068003689815445\n",
      "train loss:0.007234340546918678\n",
      "train loss:0.03359986029026503\n",
      "train loss:0.014387577662385269\n",
      "train loss:0.008912495701110301\n",
      "train loss:0.005534833555002278\n",
      "train loss:0.011327330538564407\n",
      "train loss:0.00824522841487393\n",
      "train loss:0.0019360712232986419\n",
      "train loss:0.012583736718853813\n",
      "train loss:0.07969732891817373\n",
      "train loss:0.013609494234564114\n",
      "train loss:0.008655811190924227\n",
      "train loss:0.013241467728808557\n",
      "train loss:0.007350637281883197\n",
      "train loss:0.04480610786142051\n",
      "train loss:0.009540535087795314\n",
      "train loss:0.025254683803826793\n",
      "train loss:0.026249183156124237\n",
      "train loss:0.03152016991925487\n",
      "train loss:0.014444131722151076\n",
      "train loss:0.039660107245758296\n",
      "train loss:0.007198968602521965\n",
      "train loss:0.012442310771382631\n",
      "train loss:0.02359136955210174\n",
      "train loss:0.015172152389598384\n",
      "train loss:0.021759817959294308\n",
      "train loss:0.003803788779839091\n",
      "train loss:0.014077735265622235\n",
      "train loss:0.01161222997022685\n",
      "train loss:0.00784702479373344\n",
      "train loss:0.002487440613102566\n",
      "train loss:0.0053438714183307425\n",
      "train loss:0.013504960226308437\n",
      "train loss:0.003097470388322332\n",
      "train loss:0.0023079185485143983\n",
      "train loss:0.008175267792361844\n",
      "train loss:0.035191554229779\n",
      "train loss:0.03172696548537186\n",
      "train loss:0.003758247147698367\n",
      "train loss:0.0056791793341941805\n",
      "train loss:0.012782550259843947\n",
      "train loss:0.012558362026401641\n",
      "train loss:0.015201281579089712\n",
      "train loss:0.002057926620810275\n",
      "train loss:0.011853750571458743\n",
      "train loss:0.022445344699358785\n",
      "train loss:0.004573707418425511\n",
      "train loss:0.03538483299058604\n",
      "train loss:0.020909259267592815\n",
      "train loss:0.039554458381749126\n",
      "train loss:0.02716006482749752\n",
      "train loss:0.012584920120139408\n",
      "train loss:0.011146123180664994\n",
      "train loss:0.041439525210742664\n",
      "train loss:0.004229035688299224\n",
      "train loss:0.008998432988048043\n",
      "train loss:0.014188581510227226\n",
      "train loss:0.01342735659499596\n",
      "train loss:0.014112020794220142\n",
      "train loss:0.05631917613105539\n",
      "train loss:0.02767650046881783\n",
      "train loss:0.002843421484506517\n",
      "train loss:0.0162672669474666\n",
      "train loss:0.009009093479686753\n",
      "train loss:0.03224137037915271\n",
      "train loss:0.037700928213016495\n",
      "train loss:0.029493658031068147\n",
      "train loss:0.01409201076568377\n",
      "train loss:0.006539117462187014\n",
      "train loss:0.013445052690533612\n",
      "train loss:0.035640098864788965\n",
      "train loss:0.027659205852940474\n",
      "train loss:0.004842014632487704\n",
      "train loss:0.04512533430374876\n",
      "train loss:0.018353290226876557\n",
      "train loss:0.02988861560874297\n",
      "train loss:0.023161658492400795\n",
      "train loss:0.018946337816903216\n",
      "train loss:0.04397101401700052\n",
      "train loss:0.01232018935778032\n",
      "train loss:0.012238340855212993\n",
      "train loss:0.0357527837660609\n",
      "train loss:0.006171794933600111\n",
      "train loss:0.010378408805396887\n",
      "train loss:0.006232739944811391\n",
      "train loss:0.04167004914294538\n",
      "train loss:0.006702536380194775\n",
      "train loss:0.011670816174230235\n",
      "train loss:0.017027155154237694\n",
      "train loss:0.004005600472642359\n",
      "train loss:0.007812750027757569\n",
      "train loss:0.009257420753224443\n",
      "train loss:0.024128290447882984\n",
      "train loss:0.00979558040140995\n",
      "train loss:0.019015079204112217\n",
      "train loss:0.043777692507002745\n",
      "train loss:0.006842383462702292\n",
      "train loss:0.004863488934555044\n",
      "train loss:0.023121178654931903\n",
      "train loss:0.027335389104680537\n",
      "train loss:0.007743487338227299\n",
      "train loss:0.005815266678027165\n",
      "train loss:0.0077440167096984\n",
      "train loss:0.04037489310881104\n",
      "train loss:0.07301357366041712\n",
      "train loss:0.0057703026921381104\n",
      "train loss:0.001448521694172658\n",
      "train loss:0.004129591719611368\n",
      "train loss:0.039420704265089715\n",
      "train loss:0.010374300659556699\n",
      "train loss:0.011856683675783465\n",
      "train loss:0.006945535158032815\n",
      "train loss:0.004913323514380117\n",
      "train loss:0.049545355112046975\n",
      "train loss:0.047494356422618206\n",
      "train loss:0.007912936473242234\n",
      "train loss:0.016883022566328423\n",
      "train loss:0.00812894772371911\n",
      "train loss:0.01669541673740503\n",
      "train loss:0.008496873302836208\n",
      "train loss:0.03214414683487563\n",
      "train loss:0.007577601294770862\n",
      "train loss:0.012325738717224958\n",
      "train loss:0.03449351828334314\n",
      "=== epoch:8, train acc:0.9939764492753623, test acc:0.9852355072463768 ===\n",
      "train loss:0.017793060088772957\n",
      "train loss:0.01989380118155115\n",
      "train loss:0.00616442943830241\n",
      "train loss:0.015861984263852994\n",
      "train loss:0.006881575022358298\n",
      "train loss:0.04697338852924267\n",
      "train loss:0.025474073588558344\n",
      "train loss:0.016861736604880027\n",
      "train loss:0.00396680174364488\n",
      "train loss:0.006179822120246914\n",
      "train loss:0.008583971312686527\n",
      "train loss:0.0073163879250010555\n",
      "train loss:0.01026894942368836\n",
      "train loss:0.003892622869329495\n",
      "train loss:0.018579128180753815\n",
      "train loss:0.033783480284908206\n",
      "train loss:0.022958407077445283\n",
      "train loss:0.007138110829094854\n",
      "train loss:0.014621996079721209\n",
      "train loss:0.008980672402307599\n",
      "train loss:0.012995667595806806\n",
      "train loss:0.006638258747045069\n",
      "train loss:0.05123555638016533\n",
      "train loss:0.016053166204663138\n",
      "train loss:0.002052484278036308\n",
      "train loss:0.022937798545433163\n",
      "train loss:0.0061276314607235744\n",
      "train loss:0.008020312717436614\n",
      "train loss:0.011178642895981408\n",
      "train loss:0.01136335617919049\n",
      "train loss:0.029995548738696455\n",
      "train loss:0.00572711186249651\n",
      "train loss:0.03222845761912159\n",
      "train loss:0.016431144815905775\n",
      "train loss:0.020106141458951034\n",
      "train loss:0.04709206593275759\n",
      "train loss:0.015879950009173527\n",
      "train loss:0.01090177697743907\n",
      "train loss:0.026277192311873903\n",
      "train loss:0.01648999100908494\n",
      "train loss:0.03073744830522042\n",
      "train loss:0.019559909648996524\n",
      "train loss:0.0706738436824623\n",
      "train loss:0.03389832032900886\n",
      "train loss:0.0334453108017424\n",
      "train loss:0.010129421389437808\n",
      "train loss:0.011787521631602354\n",
      "train loss:0.0524484733540074\n",
      "train loss:0.008159075642641789\n",
      "train loss:0.0019042511935915473\n",
      "train loss:0.04779880442517213\n",
      "train loss:0.017671734895373933\n",
      "train loss:0.007316230809005048\n",
      "train loss:0.01708464167250637\n",
      "train loss:0.002559994534668721\n",
      "train loss:0.02382220939370554\n",
      "train loss:0.0121279446470481\n",
      "train loss:0.012308198416607357\n",
      "train loss:0.010506083273336636\n",
      "train loss:0.005922858917153283\n",
      "train loss:0.01817382635791981\n",
      "train loss:0.0167178418213751\n",
      "train loss:0.013570378724933113\n",
      "train loss:0.03584075163338487\n",
      "train loss:0.049621291999289115\n",
      "train loss:0.043963720931187805\n",
      "train loss:0.013860884626960486\n",
      "train loss:0.008837208464735392\n",
      "train loss:0.01827536505668487\n",
      "train loss:0.04729991108874545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.026305349294652353\n",
      "train loss:0.0182391485640631\n",
      "train loss:0.009413945214662912\n",
      "train loss:0.02530510774950077\n",
      "train loss:0.004726193997584362\n",
      "train loss:0.02917721050514683\n",
      "train loss:0.008315189764917715\n",
      "train loss:0.006734046631429023\n",
      "train loss:0.03854227927314912\n",
      "train loss:0.015654312598969742\n",
      "train loss:0.01023089579561594\n",
      "train loss:0.011542058282996674\n",
      "train loss:0.006865672285781886\n",
      "train loss:0.06753035423763035\n",
      "train loss:0.0064326292889748445\n",
      "train loss:0.012091822597334689\n",
      "train loss:0.019928490747968954\n",
      "train loss:0.015670315242924678\n",
      "train loss:0.017004153009468743\n",
      "train loss:0.00468198904584014\n",
      "train loss:0.004249054521791561\n",
      "train loss:0.008617935206453993\n",
      "train loss:0.01412955093194426\n",
      "train loss:0.027432423496340144\n",
      "train loss:0.02991421023587424\n",
      "train loss:0.035578433185075344\n",
      "train loss:0.041186718508371974\n",
      "train loss:0.009069335269841765\n",
      "train loss:0.03191102549271382\n",
      "train loss:0.011771504691153003\n",
      "train loss:0.01278958663519878\n",
      "train loss:0.06743638776507567\n",
      "train loss:0.008734530900422973\n",
      "train loss:0.024569461355653358\n",
      "train loss:0.021633740669426128\n",
      "train loss:0.00387827209407895\n",
      "train loss:0.02289508239034945\n",
      "train loss:0.01139964808556413\n",
      "train loss:0.005710953933594043\n",
      "train loss:0.004331166034452312\n",
      "train loss:0.008375022033152785\n",
      "train loss:0.03545990730533726\n",
      "train loss:0.031910224626031956\n",
      "train loss:0.0096455920777287\n",
      "train loss:0.004366808490608228\n",
      "train loss:0.019834672416217006\n",
      "train loss:0.009382466352396037\n",
      "train loss:0.00571479979392439\n",
      "train loss:0.006771351909037034\n",
      "train loss:0.007365289217964152\n",
      "train loss:0.04925902137335087\n",
      "train loss:0.030308799142057818\n",
      "train loss:0.006468493818397582\n",
      "train loss:0.00723992277105609\n",
      "train loss:0.05128971139821313\n",
      "train loss:0.011518888113969337\n",
      "train loss:0.008785925485393612\n",
      "train loss:0.012511983735631282\n",
      "train loss:0.04397405472511033\n",
      "train loss:0.02568132879438122\n",
      "train loss:0.002807643540551294\n",
      "train loss:0.03003820608828339\n",
      "train loss:0.012129228722979231\n",
      "train loss:0.009413069199312693\n",
      "train loss:0.030711021336097384\n",
      "train loss:0.04177260406018236\n",
      "train loss:0.022049486386491186\n",
      "train loss:0.006532709246338869\n",
      "train loss:0.05031839317256467\n",
      "train loss:0.05125031251471865\n",
      "train loss:0.0030898036028170327\n",
      "train loss:0.011551038396483545\n",
      "train loss:0.011519731470290144\n",
      "train loss:0.01858769548316591\n",
      "train loss:0.026940233692989472\n",
      "train loss:0.004037651062462881\n",
      "train loss:0.0026498403606109544\n",
      "train loss:0.007189835914214782\n",
      "train loss:0.009285730209502241\n",
      "train loss:0.006977773507994489\n",
      "train loss:0.01661973841607259\n",
      "train loss:0.026352033966279758\n",
      "train loss:0.028949879811790154\n",
      "train loss:0.020236758071423853\n",
      "train loss:0.011993523404099586\n",
      "train loss:0.008403844930468416\n",
      "train loss:0.002959188393303074\n",
      "train loss:0.013406279936011593\n",
      "train loss:0.0056210057293357525\n",
      "train loss:0.007367146033548451\n",
      "train loss:0.01852734923541372\n",
      "train loss:0.004927662062938226\n",
      "train loss:0.007630688177346907\n",
      "train loss:0.03451470702317707\n",
      "train loss:0.013418525338261862\n",
      "train loss:0.03395308347564731\n",
      "train loss:0.006572761379778637\n",
      "train loss:0.005765313977553497\n",
      "train loss:0.008756770457024355\n",
      "train loss:0.006435025536999007\n",
      "train loss:0.015638481341748105\n",
      "train loss:0.00956695755278564\n",
      "train loss:0.01580947749333498\n",
      "train loss:0.018369364215326704\n",
      "train loss:0.006200381592803047\n",
      "train loss:0.05949976115666707\n",
      "train loss:0.0044537801707907184\n",
      "train loss:0.016898640795209398\n",
      "train loss:0.02820101255218224\n",
      "train loss:0.012094881836729687\n",
      "train loss:0.009506170758606883\n",
      "train loss:0.008597880067813342\n",
      "train loss:0.02657140527679313\n",
      "train loss:0.004066161965786935\n",
      "train loss:0.03967874396372505\n",
      "train loss:0.005796933999636977\n",
      "train loss:0.0031213576671477385\n",
      "train loss:0.026649987124514808\n",
      "train loss:0.007088364193010482\n",
      "train loss:0.06840494273814915\n",
      "train loss:0.01423106033717178\n",
      "train loss:0.03255592670518044\n",
      "train loss:0.021372179498799528\n",
      "train loss:0.007854432119993811\n",
      "train loss:0.01207560428197743\n",
      "train loss:0.006982943303460063\n",
      "train loss:0.007335686215547682\n",
      "train loss:0.00404622653710918\n",
      "train loss:0.0075643238125723935\n",
      "train loss:0.03728351002203194\n",
      "train loss:0.018510490161658885\n",
      "train loss:0.004826146660030466\n",
      "train loss:0.011295420041310547\n",
      "train loss:0.008327327334711775\n",
      "train loss:0.06406524113397526\n",
      "train loss:0.02023850827343338\n",
      "train loss:0.04351906557406459\n",
      "train loss:0.01109114366505098\n",
      "train loss:0.0218004891212159\n",
      "train loss:0.018635391811764344\n",
      "train loss:0.015489074639493533\n",
      "train loss:0.011982859646363697\n",
      "train loss:0.0030736337405625116\n",
      "train loss:0.013453630062201705\n",
      "train loss:0.011807179862309\n",
      "train loss:0.016415382896738582\n",
      "train loss:0.012180444415540203\n",
      "train loss:0.005203962010542411\n",
      "train loss:0.005858164295123809\n",
      "train loss:0.0024904216586415996\n",
      "train loss:0.011465341829346935\n",
      "train loss:0.0032290850710020697\n",
      "train loss:0.0029693267840092516\n",
      "train loss:0.002748274657178211\n",
      "train loss:0.006520223198394794\n",
      "train loss:0.019270780681625608\n",
      "train loss:0.004401647036606606\n",
      "train loss:0.008701920737571776\n",
      "train loss:0.010625742288027009\n",
      "train loss:0.007109437532484231\n",
      "train loss:0.0063572737790079745\n",
      "train loss:0.0031571707554423753\n",
      "train loss:0.004871956904159142\n",
      "train loss:0.013170241623500062\n",
      "train loss:0.017676902187335863\n",
      "train loss:0.02043381571004414\n",
      "train loss:0.0158618404214987\n",
      "train loss:0.0034861816840917764\n",
      "train loss:0.0007523707631750867\n",
      "train loss:0.002751695948733149\n",
      "train loss:0.007595946222871365\n",
      "train loss:0.05409101792708371\n",
      "train loss:0.006601060923205155\n",
      "train loss:0.00864224360374417\n",
      "train loss:0.008195304331539272\n",
      "train loss:0.03642236945925829\n",
      "train loss:0.031395697263458075\n",
      "train loss:0.035630954450714854\n",
      "train loss:0.00808290937327725\n",
      "train loss:0.004589186337246342\n",
      "train loss:0.02530148579893822\n",
      "train loss:0.034639324950054125\n",
      "train loss:0.017294541022996858\n",
      "train loss:0.016423838097324114\n",
      "train loss:0.04830227328003311\n",
      "train loss:0.005800761303848252\n",
      "train loss:0.051831805400693445\n",
      "train loss:0.00819902799804393\n",
      "train loss:0.016401912577054274\n",
      "train loss:0.02763822809097128\n",
      "train loss:0.00697772178770627\n",
      "train loss:0.03382043900667617\n",
      "train loss:0.02400418589422557\n",
      "train loss:0.006323451756757496\n",
      "train loss:0.061990232018414027\n",
      "train loss:0.0036138230881579093\n",
      "train loss:0.004893130703307334\n",
      "train loss:0.007992747959492806\n",
      "train loss:0.0016994084015411193\n",
      "train loss:0.009186348088285385\n",
      "train loss:0.047461974786853926\n",
      "train loss:0.015059223903772068\n",
      "train loss:0.016117647248859755\n",
      "train loss:0.003121128902567238\n",
      "train loss:0.02872472098967461\n",
      "train loss:0.009421408130422061\n",
      "train loss:0.0037104918043952195\n",
      "train loss:0.016809311469962406\n",
      "train loss:0.025164538319765773\n",
      "train loss:0.011680296494034589\n",
      "train loss:0.009579652500910663\n",
      "train loss:0.005191813599368596\n",
      "train loss:0.01638900894786041\n",
      "train loss:0.004706539959609119\n",
      "train loss:0.01737703214217166\n",
      "train loss:0.04552263169254834\n",
      "train loss:0.01074924276172847\n",
      "train loss:0.01748754378305688\n",
      "train loss:0.008388957577103062\n",
      "train loss:0.006695841838610247\n",
      "train loss:0.015022728325832206\n",
      "train loss:0.015978020201971466\n",
      "train loss:0.005776703806855937\n",
      "train loss:0.011856851452559942\n",
      "train loss:0.0083944233134153\n",
      "train loss:0.026563889550610367\n",
      "train loss:0.0021411608784446044\n",
      "train loss:0.009218339993014154\n",
      "train loss:0.0435377809193788\n",
      "train loss:0.02329861194282015\n",
      "train loss:0.0024350815735445968\n",
      "train loss:0.021910263117996806\n",
      "train loss:0.007636996121530533\n",
      "train loss:0.011881377262442655\n",
      "train loss:0.006568788744801596\n",
      "train loss:0.027709084389702716\n",
      "train loss:0.017880425421655775\n",
      "train loss:0.02595845305671818\n",
      "train loss:0.013217274134645162\n",
      "train loss:0.0018086835409416935\n",
      "train loss:0.09376997889494837\n",
      "train loss:0.006126794120986147\n",
      "train loss:0.003499925659996553\n",
      "train loss:0.006825230001789828\n",
      "train loss:0.015123637528884449\n",
      "train loss:0.021169419717058168\n",
      "train loss:0.024276278158249842\n",
      "train loss:0.005879077064448018\n",
      "train loss:0.003426026117866977\n",
      "train loss:0.0033058826509283186\n",
      "train loss:0.001824566238646599\n",
      "train loss:0.011830958682492971\n",
      "train loss:0.008218859052633828\n",
      "train loss:0.027986492249093897\n",
      "train loss:0.029565919930059394\n",
      "train loss:0.027423639228271523\n",
      "train loss:0.0557047093318226\n",
      "train loss:0.014320845724350401\n",
      "train loss:0.006974502641259162\n",
      "train loss:0.005359927015766898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.02625828201997159\n",
      "train loss:0.0036039469982610305\n",
      "train loss:0.030123319060607532\n",
      "train loss:0.0077327199832353775\n",
      "train loss:0.00410215208857929\n",
      "train loss:0.01752404193989141\n",
      "train loss:0.008295661697762767\n",
      "train loss:0.011247521586717124\n",
      "train loss:0.014687943918753455\n",
      "train loss:0.002985522277273493\n",
      "train loss:0.001777175546134276\n",
      "train loss:0.011387049129421305\n",
      "train loss:0.023208257520727426\n",
      "train loss:0.029845738954792568\n",
      "train loss:0.004636484654481118\n",
      "train loss:0.025068428797629782\n",
      "train loss:0.0177556438563947\n",
      "train loss:0.0023616834514875213\n",
      "train loss:0.0022465669432134693\n",
      "train loss:0.01621440041249563\n",
      "train loss:0.02006196845045761\n",
      "train loss:0.008786154172382562\n",
      "train loss:0.00369495657073957\n",
      "train loss:0.01433661271335914\n",
      "train loss:0.02932455612641841\n",
      "train loss:0.0024340432837835686\n",
      "train loss:0.0017424176531780071\n",
      "train loss:0.003546611066150551\n",
      "train loss:0.02203962496914606\n",
      "train loss:0.004686511861679058\n",
      "train loss:0.014850098902643345\n",
      "train loss:0.027720700003526343\n",
      "train loss:0.006914507726464458\n",
      "train loss:0.006864259627575467\n",
      "train loss:0.03717768504007845\n",
      "train loss:0.013374182909954197\n",
      "train loss:0.0076578448190490166\n",
      "train loss:0.01905902821017208\n",
      "=== epoch:9, train acc:0.994338768115942, test acc:0.9856884057971015 ===\n",
      "train loss:0.0016561918436828603\n",
      "train loss:0.004452640198876613\n",
      "train loss:0.001864971195453762\n",
      "train loss:0.023473657475901445\n",
      "train loss:0.009282719743116194\n",
      "train loss:0.005589732850553614\n",
      "train loss:0.01142049697623528\n",
      "train loss:0.002469799479377943\n",
      "train loss:0.0031916831600049176\n",
      "train loss:0.0019909864572302104\n",
      "train loss:0.014977969081251692\n",
      "train loss:0.03439001590055614\n",
      "train loss:0.004986229230819511\n",
      "train loss:0.013721955219060562\n",
      "train loss:0.0039223339242982665\n",
      "train loss:0.006869537474458372\n",
      "train loss:0.024910008840563356\n",
      "train loss:0.030772789526480524\n",
      "train loss:0.013953957897574613\n",
      "train loss:0.03351891201255887\n",
      "train loss:0.020700256191780245\n",
      "train loss:0.008489461199669973\n",
      "train loss:0.023751189600000618\n",
      "train loss:0.003575773692573002\n",
      "train loss:0.007134481339920501\n",
      "train loss:0.012457825042230863\n",
      "train loss:0.005410957897669127\n",
      "train loss:0.022266211585655248\n",
      "train loss:0.014768556217850119\n",
      "train loss:0.025270711272773857\n",
      "train loss:0.01204514308119851\n",
      "train loss:0.02107839400802946\n",
      "train loss:0.01577409980574908\n",
      "train loss:0.010970152819494504\n",
      "train loss:0.02061342940138187\n",
      "train loss:0.009927600100496141\n",
      "train loss:0.015038274073182959\n",
      "train loss:0.009095286842703476\n",
      "train loss:0.030597591624632034\n",
      "train loss:0.006512921442660384\n",
      "train loss:0.024903025703221403\n",
      "train loss:0.023510955407680837\n",
      "train loss:0.037278936788403876\n",
      "train loss:0.00445686096321228\n",
      "train loss:0.0638745702794192\n",
      "train loss:0.003278298614190359\n",
      "train loss:0.012449918434043496\n",
      "train loss:0.0402149308468579\n",
      "train loss:0.005539895909304856\n",
      "train loss:0.005323517987113415\n",
      "train loss:0.003080845483774463\n",
      "train loss:0.02445075913259797\n",
      "train loss:0.018920118848169583\n",
      "train loss:0.008081038300966802\n",
      "train loss:0.02390391086539612\n",
      "train loss:0.05154609163411989\n",
      "train loss:0.002913153287692674\n",
      "train loss:0.04795513346905643\n",
      "train loss:0.035901381371392246\n",
      "train loss:0.003761984496100588\n",
      "train loss:0.0033807989252608696\n",
      "train loss:0.012534049961644278\n",
      "train loss:0.0026785768848741343\n",
      "train loss:0.008633883709394117\n",
      "train loss:0.016371871843931744\n",
      "train loss:0.036119113451997954\n",
      "train loss:0.00821250274740783\n",
      "train loss:0.018761876206778116\n",
      "train loss:0.01940306657618657\n",
      "train loss:0.0077172125190821525\n",
      "train loss:0.04156098157148488\n",
      "train loss:0.014782459053134179\n",
      "train loss:0.004193568664080998\n",
      "train loss:0.009904950398341963\n",
      "train loss:0.006350770574368816\n",
      "train loss:0.008509649104253721\n",
      "train loss:0.035630802609958365\n",
      "train loss:0.00727848578716588\n",
      "train loss:0.0307789694939964\n",
      "train loss:0.001973970923119886\n",
      "train loss:0.0035812733098663754\n",
      "train loss:0.00449989773815246\n",
      "train loss:0.03192385984457662\n",
      "train loss:0.004502630590537289\n",
      "train loss:0.055799937249883885\n",
      "train loss:0.0038715714745510637\n",
      "train loss:0.023079449858018682\n",
      "train loss:0.00918619914396995\n",
      "train loss:0.0059540469694175785\n",
      "train loss:0.0018227089996110978\n",
      "train loss:0.006892817417363404\n",
      "train loss:0.014876491922844303\n",
      "train loss:0.009342537081268419\n",
      "train loss:0.0045190931455727345\n",
      "train loss:0.0015409712363178279\n",
      "train loss:0.03180285755557454\n",
      "train loss:0.01432571574184434\n",
      "train loss:0.0016633368042163123\n",
      "train loss:0.006233069894262818\n",
      "train loss:0.004716018309749876\n",
      "train loss:0.01213312561034394\n",
      "train loss:0.0015227209817601029\n",
      "train loss:0.018876168883936048\n",
      "train loss:0.002808679956405553\n",
      "train loss:0.028337674529021774\n",
      "train loss:0.0026705107955921556\n",
      "train loss:0.003738451687766085\n",
      "train loss:0.009088533716769504\n",
      "train loss:0.009684256486571787\n",
      "train loss:0.03100384327307694\n",
      "train loss:0.00328253917400409\n",
      "train loss:0.027445153447333004\n",
      "train loss:0.014284837479835728\n",
      "train loss:0.004802951113154669\n",
      "train loss:0.0040908800494846905\n",
      "train loss:0.014282238429722871\n",
      "train loss:0.03489914485531427\n",
      "train loss:0.003964535757271287\n",
      "train loss:0.04168062358644875\n",
      "train loss:0.005283146136339995\n",
      "train loss:0.0032173173050665297\n",
      "train loss:0.02917145159434608\n",
      "train loss:0.011829377067485118\n",
      "train loss:0.006002055352194979\n",
      "train loss:0.006340206272972841\n",
      "train loss:0.001549368719803019\n",
      "train loss:0.005046886645785004\n",
      "train loss:0.00287858481425157\n",
      "train loss:0.0022135995363919787\n",
      "train loss:0.011507344398166829\n",
      "train loss:0.0024955344676189438\n",
      "train loss:0.017614653029690956\n",
      "train loss:0.016147147129340008\n",
      "train loss:0.009623995080627829\n",
      "train loss:0.019947301555900472\n",
      "train loss:0.007973392650317431\n",
      "train loss:0.008687383986778584\n",
      "train loss:0.04418284853685942\n",
      "train loss:0.012858079299951744\n",
      "train loss:0.02480689007436884\n",
      "train loss:0.012871445585131118\n",
      "train loss:0.01486871280726799\n",
      "train loss:0.021152285606318624\n",
      "train loss:0.03959022428640469\n",
      "train loss:0.025367995997728463\n",
      "train loss:0.018254492290010946\n",
      "train loss:0.0029209591722418405\n",
      "train loss:0.0038245303460651904\n",
      "train loss:0.004549983147269978\n",
      "train loss:0.006087218225903122\n",
      "train loss:0.009185207620174156\n",
      "train loss:0.002617468284561703\n",
      "train loss:0.017648140167740238\n",
      "train loss:0.007480796039171872\n",
      "train loss:0.018452466097742717\n",
      "train loss:0.006338848762569131\n",
      "train loss:0.005603545158845511\n",
      "train loss:0.0065044927524108166\n",
      "train loss:0.011772734869224\n",
      "train loss:0.005172775037200335\n",
      "train loss:0.024171120112655327\n",
      "train loss:0.018816922203128264\n",
      "train loss:0.006091497965814233\n",
      "train loss:0.00895993448414445\n",
      "train loss:0.011057688679520627\n",
      "train loss:0.05500306585543217\n",
      "train loss:0.009080472953737663\n",
      "train loss:0.003747793870649041\n",
      "train loss:0.005159238139004994\n",
      "train loss:0.009855636879079577\n",
      "train loss:0.0020428621324796266\n",
      "train loss:0.005866316548786035\n",
      "train loss:0.05810249739795352\n",
      "train loss:0.03191353925291162\n",
      "train loss:0.050786492630318654\n",
      "train loss:0.0070620240156499545\n",
      "train loss:0.012705019101683061\n",
      "train loss:0.002745800672266761\n",
      "train loss:0.018995344825477777\n",
      "train loss:0.00319755127002114\n",
      "train loss:0.0010727140221561487\n",
      "train loss:0.03505730859407968\n",
      "train loss:0.044954097078593595\n",
      "train loss:0.014271723788383846\n",
      "train loss:0.00868331259735078\n",
      "train loss:0.002715496848233128\n",
      "train loss:0.00625161584205533\n",
      "train loss:0.032974749313124295\n",
      "train loss:0.045333572318740606\n",
      "train loss:0.01206058724641283\n",
      "train loss:0.002370730669467967\n",
      "train loss:0.002756049967099811\n",
      "train loss:0.011308016278630931\n",
      "train loss:0.009557379330636058\n",
      "train loss:0.057645078447020937\n",
      "train loss:0.01244886282572468\n",
      "train loss:0.022603145765268392\n",
      "train loss:0.003306676514241791\n",
      "train loss:0.0025558136658594415\n",
      "train loss:0.00258704062369896\n",
      "train loss:0.01687931488562119\n",
      "train loss:0.003438164959581751\n",
      "train loss:0.04310612685655189\n",
      "train loss:0.01432297992372427\n",
      "train loss:0.006365230481025132\n",
      "train loss:0.005020187893746462\n",
      "train loss:0.01901577817270937\n",
      "train loss:0.010554194164097478\n",
      "train loss:0.018917615092266988\n",
      "train loss:0.03450588578257431\n",
      "train loss:0.0104062952119779\n",
      "train loss:0.0012794718834750578\n",
      "train loss:0.03413410464163\n",
      "train loss:0.014504907417797268\n",
      "train loss:0.00733493400280247\n",
      "train loss:0.02457729622501809\n",
      "train loss:0.00567227999175898\n",
      "train loss:0.007167434116749313\n",
      "train loss:0.01542838897233439\n",
      "train loss:0.0222285289118741\n",
      "train loss:0.01568527682269535\n",
      "train loss:0.01792683659368451\n",
      "train loss:0.012918551295701939\n",
      "train loss:0.00862465665749266\n",
      "train loss:0.002366421555869931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.007254898115519915\n",
      "train loss:0.04791545604133064\n",
      "train loss:0.005259747547170641\n",
      "train loss:0.010069031230449232\n",
      "train loss:0.03243007559196816\n",
      "train loss:0.021972392925211914\n",
      "train loss:0.0024357082189034726\n",
      "train loss:0.0020990098445776524\n",
      "train loss:0.002916746149808208\n",
      "train loss:0.0024789691522250096\n",
      "train loss:0.002264527930568257\n",
      "train loss:0.006797809082049433\n",
      "train loss:0.008113607655413793\n",
      "train loss:0.02652683709957428\n",
      "train loss:0.030632114700750546\n",
      "train loss:0.00467390144513567\n",
      "train loss:0.019286922615804947\n",
      "train loss:0.01784317038055648\n",
      "train loss:0.038506124916641335\n",
      "train loss:0.019887813972285607\n",
      "train loss:0.08452485317347162\n",
      "train loss:0.003940655791218503\n",
      "train loss:0.014130967062700395\n",
      "train loss:0.022039248915359317\n",
      "train loss:0.017694743751973066\n",
      "train loss:0.015293692821471508\n",
      "train loss:0.06057111391272262\n",
      "train loss:0.0419905479905563\n",
      "train loss:0.01672064426691248\n",
      "train loss:0.011864279423935585\n",
      "train loss:0.003877617076127137\n",
      "train loss:0.010416785990690814\n",
      "train loss:0.021710142835943736\n",
      "train loss:0.028962690286520022\n",
      "train loss:0.009411542096055622\n",
      "train loss:0.02555803657806325\n",
      "train loss:0.006405772360892204\n",
      "train loss:0.010240121042452654\n",
      "train loss:0.009584732788895767\n",
      "train loss:0.02515109741195772\n",
      "train loss:0.0034504700999455776\n",
      "train loss:0.020340079182345403\n",
      "train loss:0.01857376516407093\n",
      "train loss:0.03292050776194896\n",
      "train loss:0.014501543059202277\n",
      "train loss:0.008656162653135617\n",
      "train loss:0.01039210209426395\n",
      "train loss:0.004086727033366619\n",
      "train loss:0.019263544803614332\n",
      "train loss:0.004238058278314594\n",
      "train loss:0.023627994748666076\n",
      "train loss:0.04973110741867442\n",
      "train loss:0.002488669634773012\n",
      "train loss:0.006064240276769964\n",
      "train loss:0.004063134314147236\n",
      "train loss:0.0056352716041876754\n",
      "train loss:0.00384448192431157\n",
      "train loss:0.010495439173988992\n",
      "train loss:0.004875265806057416\n",
      "train loss:0.003168190142585773\n",
      "train loss:0.004254951467344618\n",
      "train loss:0.0019273798570470636\n",
      "train loss:0.014385490322458535\n",
      "train loss:0.008608349950938938\n",
      "train loss:0.002737650618668769\n",
      "train loss:0.0023366722090453\n",
      "train loss:0.009450572761586175\n",
      "train loss:0.011022667432311005\n",
      "train loss:0.010372071454818048\n",
      "train loss:0.00578744506224654\n",
      "train loss:0.009659793512940617\n",
      "train loss:0.04904587957504218\n",
      "train loss:0.002359355251233139\n",
      "train loss:0.017974535688049353\n",
      "train loss:0.013602090468281116\n",
      "train loss:0.008574293431184649\n",
      "train loss:0.014195105316955788\n",
      "train loss:0.019509039663409777\n",
      "train loss:0.00502418285470185\n",
      "train loss:0.005639559489787072\n",
      "train loss:0.0038076657496274166\n",
      "train loss:0.005374836588589626\n",
      "train loss:0.0036343399644616276\n",
      "train loss:0.04238912382021416\n",
      "train loss:0.006149081969585638\n",
      "train loss:0.003963926098322619\n",
      "train loss:0.0036758009828877823\n",
      "train loss:0.04253131649844018\n",
      "train loss:0.0018821700828898938\n",
      "train loss:0.0015290267748186937\n",
      "train loss:0.014859260036806808\n",
      "train loss:0.004353263798071934\n",
      "train loss:0.03938853908319727\n",
      "train loss:0.017661725229475827\n",
      "train loss:0.008223650485461618\n",
      "train loss:0.009748366965882342\n",
      "train loss:0.02678272611875364\n",
      "train loss:0.014488605659265432\n",
      "train loss:0.005818754795895637\n",
      "train loss:0.019234920360520415\n",
      "train loss:0.018137397207071075\n",
      "train loss:0.018993856401395132\n",
      "train loss:0.0036256438910924746\n",
      "train loss:0.006493752366171498\n",
      "train loss:0.02657416318414664\n",
      "train loss:0.009017923622911369\n",
      "train loss:0.010899331327720592\n",
      "train loss:0.07451735521594878\n",
      "train loss:0.016397572033807272\n",
      "train loss:0.016239315147541362\n",
      "train loss:0.004233960411953523\n",
      "train loss:0.009383963086495606\n",
      "train loss:0.030449564156151358\n",
      "train loss:0.007024376116556616\n",
      "train loss:0.009669729444414315\n",
      "train loss:0.0049600482344633046\n",
      "train loss:0.0030741421362052647\n",
      "train loss:0.013173794199020102\n",
      "train loss:0.011421801999880582\n",
      "train loss:0.0035165160305768734\n",
      "train loss:0.012828598610296706\n",
      "train loss:0.003818733945522429\n",
      "train loss:0.06006334705689302\n",
      "train loss:0.013717497451374506\n",
      "train loss:0.014748581517621947\n",
      "train loss:0.02992122632272237\n",
      "train loss:0.002691126775599807\n",
      "train loss:0.022408976340789193\n",
      "train loss:0.04408431540784004\n",
      "train loss:0.00481858107505158\n",
      "train loss:0.006474528939633478\n",
      "train loss:0.009970683150801124\n",
      "train loss:0.002513301803835578\n",
      "train loss:0.007897345537382423\n",
      "train loss:0.023639033726747402\n",
      "train loss:0.010202151703628155\n",
      "train loss:0.017542407500587047\n",
      "train loss:0.002899993354038233\n",
      "train loss:0.002261801633614281\n",
      "train loss:0.0022894135114593073\n",
      "train loss:0.012237059269196274\n",
      "train loss:0.015705183171746486\n",
      "train loss:0.005628137077547803\n",
      "=== epoch:10, train acc:0.9941802536231884, test acc:0.9846920289855072 ===\n",
      "train loss:0.01807364752210895\n",
      "train loss:0.030683193439482427\n",
      "train loss:0.043205682155255255\n",
      "train loss:0.004251497848669509\n",
      "train loss:0.021208148015244413\n",
      "train loss:0.03852433007024008\n",
      "train loss:0.005146363211240659\n",
      "train loss:0.06104722108009681\n",
      "train loss:0.004911538648799707\n",
      "train loss:0.012683524637073807\n",
      "train loss:0.0049762258309403356\n",
      "train loss:0.0033251398696560654\n",
      "train loss:0.0035895286934435095\n",
      "train loss:0.018687073045566268\n",
      "train loss:0.03768863176820252\n",
      "train loss:0.02297397559953993\n",
      "train loss:0.023795206603381344\n",
      "train loss:0.056988951046119646\n",
      "train loss:0.03930087037206113\n",
      "train loss:0.006213744034495758\n",
      "train loss:0.04833538906773378\n",
      "train loss:0.00959124300245088\n",
      "train loss:0.008333696295710364\n",
      "train loss:0.016413493892163594\n",
      "train loss:0.017179594102952733\n",
      "train loss:0.013404333247057022\n",
      "train loss:0.02365176831339706\n",
      "train loss:0.010078922380118201\n",
      "train loss:0.007080271910997528\n",
      "train loss:0.006663504699806702\n",
      "train loss:0.0052213198201399875\n",
      "train loss:0.011258462519756426\n",
      "train loss:0.016803240432453\n",
      "train loss:0.012842268101783542\n",
      "train loss:0.0063376994986146335\n",
      "train loss:0.007633794261483323\n",
      "train loss:0.006983188503265001\n",
      "train loss:0.0025027280635324606\n",
      "train loss:0.0049478394514549185\n",
      "train loss:0.01737314263494075\n",
      "train loss:0.003518471038258001\n",
      "train loss:0.011832059098572618\n",
      "train loss:0.011974297250794469\n",
      "train loss:0.002638518162503521\n",
      "train loss:0.005013963097196581\n",
      "train loss:0.0095353713196031\n",
      "train loss:0.009021367087918711\n",
      "train loss:0.021634837053954484\n",
      "train loss:0.0037221218144150793\n",
      "train loss:0.0267312788629979\n",
      "train loss:0.006896436120448577\n",
      "train loss:0.052721572909888666\n",
      "train loss:0.0023959563947571136\n",
      "train loss:0.01158286078880768\n",
      "train loss:0.045613697680774674\n",
      "train loss:0.05348620493853837\n",
      "train loss:0.03215660708766551\n",
      "train loss:0.003840598325014432\n",
      "train loss:0.007956938972924328\n",
      "train loss:0.004253816051812211\n",
      "train loss:0.003499966330614809\n",
      "train loss:0.029831532409445607\n",
      "train loss:0.021161817392146134\n",
      "train loss:0.0028081629465449304\n",
      "train loss:0.00528379888081224\n",
      "train loss:0.003455093528154547\n",
      "train loss:0.022614003994039858\n",
      "train loss:0.0025616211531689368\n",
      "train loss:0.020384707686004575\n",
      "train loss:0.03261975063014241\n",
      "train loss:0.013250634248839873\n",
      "train loss:0.02238490790149703\n",
      "train loss:0.014952073923639533\n",
      "train loss:0.03185294858973086\n",
      "train loss:0.02058469922161969\n",
      "train loss:0.005319180152482192\n",
      "train loss:0.016434696711041863\n",
      "train loss:0.03342547424794847\n",
      "train loss:0.004755769958324948\n",
      "train loss:0.0049896621115238975\n",
      "train loss:0.03271625461043726\n",
      "train loss:0.021863296993404117\n",
      "train loss:0.007275127744053284\n",
      "train loss:0.01951951632541059\n",
      "train loss:0.015829221697148677\n",
      "train loss:0.004301180284069508\n",
      "train loss:0.005065262430510991\n",
      "train loss:0.022873896560540573\n",
      "train loss:0.03033813540229611\n",
      "train loss:0.006844693093950744\n",
      "train loss:0.017154026296718524\n",
      "train loss:0.004721287373227495\n",
      "train loss:0.011940922521111919\n",
      "train loss:0.00661691798867333\n",
      "train loss:0.07325193253002539\n",
      "train loss:0.01524390229007242\n",
      "train loss:0.03791240343327833\n",
      "train loss:0.02153465717070322\n",
      "train loss:0.015033065356628465\n",
      "train loss:0.011073852932085638\n",
      "train loss:0.0023687237663469907\n",
      "train loss:0.03263712645517044\n",
      "train loss:0.04444277369645442\n",
      "train loss:0.003329881082400965\n",
      "train loss:0.01872413091718968\n",
      "train loss:0.00845551641438208\n",
      "train loss:0.06415698956227002\n",
      "train loss:0.007450844831585447\n",
      "train loss:0.015756787740521784\n",
      "train loss:0.012528237502508295\n",
      "train loss:0.007769796221219276\n",
      "train loss:0.0069844487806188515\n",
      "train loss:0.040058076449894855\n",
      "train loss:0.010361154540554677\n",
      "train loss:0.025636762394105485\n",
      "train loss:0.02592179631601783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.022199658940171015\n",
      "train loss:0.0344859271581497\n",
      "train loss:0.00876062779634473\n",
      "train loss:0.006515896276567381\n",
      "train loss:0.01847834002768616\n",
      "train loss:0.008047599516693133\n",
      "train loss:0.008433927353781972\n",
      "train loss:0.0022677170719969807\n",
      "train loss:0.013580318650365412\n",
      "train loss:0.041036460247971134\n",
      "train loss:0.03675714275560303\n",
      "train loss:0.023402072741583613\n",
      "train loss:0.012170149759161383\n",
      "train loss:0.013215238224571522\n",
      "train loss:0.029723826208864854\n",
      "train loss:0.008179306235719773\n",
      "train loss:0.019048508404689087\n",
      "train loss:0.021399982440398875\n",
      "train loss:0.016673367274242012\n",
      "train loss:0.06055293304260768\n",
      "train loss:0.008340445125187522\n",
      "train loss:0.00826883144783835\n",
      "train loss:0.007846809405601592\n",
      "train loss:0.02908946864733281\n",
      "train loss:0.015630072254508402\n",
      "train loss:0.005506480585843529\n",
      "train loss:0.01513819844072087\n",
      "train loss:0.06539157884128778\n",
      "train loss:0.02259896489516062\n",
      "train loss:0.008688803995501532\n",
      "train loss:0.02944697550619203\n",
      "train loss:0.012896528457723124\n",
      "train loss:0.0057345195225418375\n",
      "train loss:0.0061828197265890745\n",
      "train loss:0.0035770398524088403\n",
      "train loss:0.004993671585223672\n",
      "train loss:0.0075040595438966694\n",
      "train loss:0.005992333073215385\n",
      "train loss:0.016225622950840048\n",
      "train loss:0.00960409589180907\n",
      "train loss:0.0026897994487827723\n",
      "train loss:0.04536302127973957\n",
      "train loss:0.019286531716167763\n",
      "train loss:0.011262984518064284\n",
      "train loss:0.0032041089285334296\n",
      "train loss:0.0021701875347088524\n",
      "train loss:0.004347992814927073\n",
      "train loss:0.02729874987359166\n",
      "train loss:0.0029079300305111825\n",
      "train loss:0.019020921266837687\n",
      "train loss:0.003636252155123734\n",
      "train loss:0.03476510023462227\n",
      "train loss:0.011918705175002663\n",
      "train loss:0.0022614393202966764\n",
      "train loss:0.02333355898182696\n",
      "train loss:0.021029545223625514\n",
      "train loss:0.028127156154637102\n",
      "train loss:0.008700354785554964\n",
      "train loss:0.05942249418870674\n",
      "train loss:0.008258810586132962\n",
      "train loss:0.03008094364517525\n",
      "train loss:0.005640806575306176\n",
      "train loss:0.056867621652843856\n",
      "train loss:0.001972415295820479\n",
      "train loss:0.010623157148122437\n",
      "train loss:0.03008649432498731\n",
      "train loss:0.014060870931998936\n",
      "train loss:0.010940499273508427\n",
      "train loss:0.0068229770557648286\n",
      "train loss:0.0035460400502039164\n",
      "train loss:0.012731652268844148\n",
      "train loss:0.011951461774809404\n",
      "train loss:0.004935273455743604\n",
      "train loss:0.02702097085324048\n",
      "train loss:0.006915324539857417\n",
      "train loss:0.0030797682274436187\n",
      "train loss:0.007849013118201633\n",
      "train loss:0.003428203706745342\n",
      "train loss:0.006059036734543063\n",
      "train loss:0.004222982231954751\n",
      "train loss:0.001973441670323159\n",
      "train loss:0.0030720917037829424\n",
      "train loss:0.01753582409258322\n",
      "train loss:0.013254176145870345\n",
      "train loss:0.007836288726593914\n",
      "train loss:0.009683152695520413\n",
      "train loss:0.020205715531826356\n",
      "train loss:0.05240724815234919\n",
      "train loss:0.02780737233831477\n",
      "train loss:0.0021562722265442884\n",
      "train loss:0.0038182288298017914\n",
      "train loss:0.005550385338538876\n",
      "train loss:0.0016515154766903357\n",
      "train loss:0.009084931332594881\n",
      "train loss:0.009895994192969307\n",
      "train loss:0.019501809665434803\n",
      "train loss:0.0022180342128400197\n",
      "train loss:0.002227521756631853\n",
      "train loss:0.010419212442410187\n",
      "train loss:0.0216084287012101\n",
      "train loss:0.0066768818063350655\n",
      "train loss:0.0041875928829785535\n",
      "train loss:0.006806647084267659\n",
      "train loss:0.01848364354590456\n",
      "train loss:0.005434120748872311\n",
      "train loss:0.004305132955019317\n",
      "train loss:0.017183014443816518\n",
      "train loss:0.007988930059707933\n",
      "train loss:0.005005313908372988\n",
      "train loss:0.0037408327526912625\n",
      "train loss:0.014539977203110684\n",
      "train loss:0.002116590380222007\n",
      "train loss:0.004703578411398617\n",
      "train loss:0.00457300655663569\n",
      "train loss:0.006591694584100517\n",
      "train loss:0.005424131204782103\n",
      "train loss:0.004797793089795246\n",
      "train loss:0.012810365436520539\n",
      "train loss:0.007321426984836867\n",
      "train loss:0.0044860365562630795\n",
      "train loss:0.004585015686650449\n",
      "train loss:0.03703699379535481\n",
      "train loss:0.014416825659090289\n",
      "train loss:0.004366371341505137\n",
      "train loss:0.014464383856658939\n",
      "train loss:0.005128312852407407\n",
      "train loss:0.018839351123977773\n",
      "train loss:0.002642242020812862\n",
      "train loss:0.013335744400713868\n",
      "train loss:0.018063966917824525\n",
      "train loss:0.007739896938542238\n",
      "train loss:0.0010781168985653099\n",
      "train loss:0.0027320421159936284\n",
      "train loss:0.0053357555318118395\n",
      "train loss:0.0029270532563002763\n",
      "train loss:0.0022002467174090227\n",
      "train loss:0.0023955333214698235\n",
      "train loss:0.0035372106392691244\n",
      "train loss:0.006597546392360387\n",
      "train loss:0.013040386951247429\n",
      "train loss:0.019057337455146248\n",
      "train loss:0.0045841940872264125\n",
      "train loss:0.01423193254906809\n",
      "train loss:0.005323138884064745\n",
      "train loss:0.00643051557574501\n",
      "train loss:0.0019563006812728934\n",
      "train loss:0.0038180798901928706\n",
      "train loss:0.008163008547420742\n",
      "train loss:0.0027141267746370306\n",
      "train loss:0.002051374174753232\n",
      "train loss:0.027472485645737534\n",
      "train loss:0.005145921801555823\n",
      "train loss:0.010749807293875011\n",
      "train loss:0.015579894553752598\n",
      "train loss:0.03628593600492521\n",
      "train loss:0.0042837029833789324\n",
      "train loss:0.010619272212616077\n",
      "train loss:0.007362353392504293\n",
      "train loss:0.0067609559912309635\n",
      "train loss:0.038264745405353386\n",
      "train loss:0.018479764003536184\n",
      "train loss:0.006995762026180926\n",
      "train loss:0.037003488341131\n",
      "train loss:0.011442020719072403\n",
      "train loss:0.009811458331977675\n",
      "train loss:0.010957706841704513\n",
      "train loss:0.0052137687289544234\n",
      "train loss:0.05598123276169793\n",
      "train loss:0.060709767410445266\n",
      "train loss:0.005831017064300033\n",
      "train loss:0.03379952733350754\n",
      "train loss:0.013340212357547874\n",
      "train loss:0.005106903397980794\n",
      "train loss:0.004583008383268567\n",
      "train loss:0.009354466829269251\n",
      "train loss:0.005636531666570368\n",
      "train loss:0.03934209277511069\n",
      "train loss:0.011661495700511322\n",
      "train loss:0.011378587754826792\n",
      "train loss:0.008432903572483736\n",
      "train loss:0.03384788504293609\n",
      "train loss:0.030007750420278662\n",
      "train loss:0.013618367425303264\n",
      "train loss:0.009911884693245504\n",
      "train loss:0.031259299956387834\n",
      "train loss:0.0058088002615661015\n",
      "train loss:0.00809779650608686\n",
      "train loss:0.001904850098803712\n",
      "train loss:0.05113116982094006\n",
      "train loss:0.0025664507378726073\n",
      "train loss:0.009942157872441884\n",
      "train loss:0.01970294361947533\n",
      "train loss:0.019327450514370837\n",
      "train loss:0.010634977168067791\n",
      "train loss:0.04351456019997569\n",
      "train loss:0.003448473347745161\n",
      "train loss:0.008640409239781066\n",
      "train loss:0.010318221965055663\n",
      "train loss:0.01077563739358353\n",
      "train loss:0.029080886400292265\n",
      "train loss:0.0032837682421184345\n",
      "train loss:0.0441321688253178\n",
      "train loss:0.000910934562784143\n",
      "train loss:0.005474087706949422\n",
      "train loss:0.00529572183147781\n",
      "train loss:0.10142286775556576\n",
      "train loss:0.019296018498390872\n",
      "train loss:0.04803536389403342\n",
      "train loss:0.03536498273933672\n",
      "train loss:0.019548935076906256\n",
      "train loss:0.001438802666722886\n",
      "train loss:0.00723092753245998\n",
      "train loss:0.006384561708791794\n",
      "train loss:0.003715585453644189\n",
      "train loss:0.01820390421680823\n",
      "train loss:0.0025851313707634456\n",
      "train loss:0.003119215607844499\n",
      "train loss:0.011206918118821001\n",
      "train loss:0.021698910776460623\n",
      "train loss:0.0023311748199633248\n",
      "train loss:0.01393784289127617\n",
      "train loss:0.018102916051877357\n",
      "train loss:0.04361867024966321\n",
      "train loss:0.019174391850843183\n",
      "train loss:0.023368983246787718\n",
      "train loss:0.001649775251711786\n",
      "train loss:0.00815659619008347\n",
      "train loss:0.00522955790127603\n",
      "train loss:0.0033305821572162326\n",
      "train loss:0.049349749174136655\n",
      "train loss:0.008485939772639414\n",
      "train loss:0.0023509216630570105\n",
      "train loss:0.0025277779545669495\n",
      "train loss:0.006282360843134705\n",
      "train loss:0.0044153211054677455\n",
      "train loss:0.003932891943150858\n",
      "train loss:0.005307540303015571\n",
      "train loss:0.022620872089549805\n",
      "train loss:0.0014977320121766357\n",
      "train loss:0.004293656485055138\n",
      "train loss:0.04871391819736359\n",
      "train loss:0.01303037963548865\n",
      "train loss:0.03553922573935508\n",
      "train loss:0.008184704553252674\n",
      "train loss:0.0009528060335074856\n",
      "train loss:0.029509470344491237\n",
      "train loss:0.007743831364236655\n",
      "train loss:0.023059445080837106\n",
      "train loss:0.005751448781338017\n",
      "train loss:0.0044951961658296034\n",
      "train loss:0.014598097068223792\n",
      "train loss:0.0033804902335608806\n",
      "=== epoch:11, train acc:0.9958106884057971, test acc:0.9856884057971015 ===\n",
      "train loss:0.0033922352017490146\n",
      "train loss:0.00901434578501321\n",
      "train loss:0.023053598914071365\n",
      "train loss:0.007926564572424546\n",
      "train loss:0.007826422156291465\n",
      "train loss:0.02065105545717277\n",
      "train loss:0.003424134706223972\n",
      "train loss:0.011424372645874752\n",
      "train loss:0.008554421324029792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00259836504998302\n",
      "train loss:0.0137006564027761\n",
      "train loss:0.004670213429640482\n",
      "train loss:0.010453756096135254\n",
      "train loss:0.01695882875892951\n",
      "train loss:0.019715757718043393\n",
      "train loss:0.013630878181880244\n",
      "train loss:0.004638069462270437\n",
      "train loss:0.0024944268965665798\n",
      "train loss:0.009743182187497482\n",
      "train loss:0.0032950747394913846\n",
      "train loss:0.015755630941043187\n",
      "train loss:0.004976623780313059\n",
      "train loss:0.019450991193319986\n",
      "train loss:0.012290712435587981\n",
      "train loss:0.020276873874706745\n",
      "train loss:0.0021359038511564983\n",
      "train loss:0.02193488899227566\n",
      "train loss:0.018166883978263694\n",
      "train loss:0.020202973223124584\n",
      "train loss:0.008074945024924297\n",
      "train loss:0.006082741381335041\n",
      "train loss:0.013412058303917422\n",
      "train loss:0.0017793468028803599\n",
      "train loss:0.006155601737805303\n",
      "train loss:0.04102128721748834\n",
      "train loss:0.01117772635514525\n",
      "train loss:0.005402340670201704\n",
      "train loss:0.006463926438321704\n",
      "train loss:0.03231093992946008\n",
      "train loss:0.015014787452425375\n",
      "train loss:0.008771052425116927\n",
      "train loss:0.01445733905534134\n",
      "train loss:0.048197877948798866\n",
      "train loss:0.0031039683749908434\n",
      "train loss:0.011290921713392334\n",
      "train loss:0.018423057814375966\n",
      "train loss:0.008722764842998894\n",
      "train loss:0.009069334410098266\n",
      "train loss:0.00303157381084983\n",
      "train loss:0.015467962275540804\n",
      "train loss:0.010775181656304086\n",
      "train loss:0.005304675953122124\n",
      "train loss:0.02131665311420572\n",
      "train loss:0.006080925755827052\n",
      "train loss:0.006594059914259439\n",
      "train loss:0.0036686902881721503\n",
      "train loss:0.005740754997780349\n",
      "train loss:0.019083884475312302\n",
      "train loss:0.01151463573367109\n",
      "train loss:0.04614133391073856\n",
      "train loss:0.003931207241659119\n",
      "train loss:0.009827127468307561\n",
      "train loss:0.0017011161606515308\n",
      "train loss:0.002440618231311037\n",
      "train loss:0.01090077562370311\n",
      "train loss:0.0038626487940268055\n",
      "train loss:0.01554290051529899\n",
      "train loss:0.006677253449444121\n",
      "train loss:0.015364594027138577\n",
      "train loss:0.007494528094250716\n",
      "train loss:0.0036162161197886874\n",
      "train loss:0.0024353531537708055\n",
      "train loss:0.007602000565973355\n",
      "train loss:0.01936935184864889\n",
      "train loss:0.0029162095954568634\n",
      "train loss:0.008477299142640924\n",
      "train loss:0.02613731741233353\n",
      "train loss:0.001204133474839549\n",
      "train loss:0.014375939472376476\n",
      "train loss:0.0027665526529777286\n",
      "train loss:0.007087976365436746\n",
      "train loss:0.017158509603644394\n",
      "train loss:0.0021570314719901906\n",
      "train loss:0.015553442988049998\n",
      "train loss:0.005395204244671766\n",
      "train loss:0.017857284358131807\n",
      "train loss:0.02030562145579266\n",
      "train loss:0.0035442553236857627\n",
      "train loss:0.01670259242138352\n",
      "train loss:0.001566797767382652\n",
      "train loss:0.00537921872754061\n",
      "train loss:0.004614636375409635\n",
      "train loss:0.008625524233797933\n",
      "train loss:0.011093697953363558\n",
      "train loss:0.006239374481365556\n",
      "train loss:0.024189492831133908\n",
      "train loss:0.025121511348777156\n",
      "train loss:0.025249768518857508\n",
      "train loss:0.0058232546260118204\n",
      "train loss:0.012180335163190649\n",
      "train loss:0.038065131012995385\n",
      "train loss:0.039152429218723306\n",
      "train loss:0.007599095915746005\n",
      "train loss:0.009573259588812099\n",
      "train loss:0.004706632716046862\n",
      "train loss:0.014982846630057578\n",
      "train loss:0.0088472743174585\n",
      "train loss:0.003172241529071416\n",
      "train loss:0.009117259262906617\n",
      "train loss:0.01181859785768422\n",
      "train loss:0.016142730032574517\n",
      "train loss:0.0051066460858197275\n",
      "train loss:0.004284832747313076\n",
      "train loss:0.02020555581186933\n",
      "train loss:0.0055782008073570815\n",
      "train loss:0.018458834492199035\n",
      "train loss:0.004599858534999746\n",
      "train loss:0.008835580698734212\n",
      "train loss:0.021247101620705134\n",
      "train loss:0.01665850634783963\n",
      "train loss:0.007598507592993399\n",
      "train loss:0.019030878679361797\n",
      "train loss:0.005467930202432011\n",
      "train loss:0.01658942146102143\n",
      "train loss:0.013334610772866299\n",
      "train loss:0.013390770970852084\n",
      "train loss:0.012320257302672248\n",
      "train loss:0.020617513278209296\n",
      "train loss:0.007078174233755297\n",
      "train loss:0.037637419764069306\n",
      "train loss:0.0066702768888685145\n",
      "train loss:0.007347951948965674\n",
      "train loss:0.007476010917215546\n",
      "train loss:0.010092833897543434\n",
      "train loss:0.009583917969343693\n",
      "train loss:0.014376799487509775\n",
      "train loss:0.006813015541702257\n",
      "train loss:0.003119770849359384\n",
      "train loss:0.004975901291252011\n",
      "train loss:0.0167750368645005\n",
      "train loss:0.0018556907566388092\n",
      "train loss:0.0055813025776663815\n",
      "train loss:0.004731638113899896\n",
      "train loss:0.0033419240541624705\n",
      "train loss:0.04439782043277018\n",
      "train loss:0.003156907819338745\n",
      "train loss:0.027158126591010093\n",
      "train loss:0.003176911506865306\n",
      "train loss:0.007298269510453212\n",
      "train loss:0.021740305882392546\n",
      "train loss:0.008066405429047054\n",
      "train loss:0.0032216108676743557\n",
      "train loss:0.007982589850809792\n",
      "train loss:0.01467222628658201\n",
      "train loss:0.0039448852266655504\n",
      "train loss:0.0030803547600406775\n",
      "train loss:0.005476869355196221\n",
      "train loss:0.044033102567622724\n",
      "train loss:0.020280420484395274\n",
      "train loss:0.001940685321496287\n",
      "train loss:0.03767207700361138\n",
      "train loss:0.010227436339055759\n",
      "train loss:0.011089547658071026\n",
      "train loss:0.0067544466237501165\n",
      "train loss:0.006742106742105811\n",
      "train loss:0.033085078083672365\n",
      "train loss:0.002904985546830487\n",
      "train loss:0.009801545226562753\n",
      "train loss:0.004276703490552366\n",
      "train loss:0.006936360838677728\n",
      "train loss:0.0030922024183901897\n",
      "train loss:0.022153309924386154\n",
      "train loss:0.01246318121661982\n",
      "train loss:0.011600061906889324\n",
      "train loss:0.005507137846542326\n",
      "train loss:0.004314647073908067\n",
      "train loss:0.030256355748892953\n",
      "train loss:0.012166046986528475\n",
      "train loss:0.006803778872209012\n",
      "train loss:0.005168042177696949\n",
      "train loss:0.014460177199551569\n",
      "train loss:0.011856514478551784\n",
      "train loss:0.003524030067474613\n",
      "train loss:0.005950877971830879\n",
      "train loss:0.006389858664150259\n",
      "train loss:0.013995129647398998\n",
      "train loss:0.0033836918361648638\n",
      "train loss:0.01325502786464665\n",
      "train loss:0.0026242373415695823\n",
      "train loss:0.027307598420060256\n",
      "train loss:0.0033161779380151235\n",
      "train loss:0.011342209045442834\n",
      "train loss:0.00393693038603514\n",
      "train loss:0.006901293748104311\n",
      "train loss:0.001892572092591568\n",
      "train loss:0.003279092009513905\n",
      "train loss:0.002966832244513264\n",
      "train loss:0.011380311238828565\n",
      "train loss:0.016095543890937647\n",
      "train loss:0.003206743607758472\n",
      "train loss:0.01910720891222609\n",
      "train loss:0.006574150609386032\n",
      "train loss:0.02350276665673976\n",
      "train loss:0.002611753284542969\n",
      "train loss:0.006123450946891378\n",
      "train loss:0.012810405386978665\n",
      "train loss:0.0014699655649713498\n",
      "train loss:0.009965813376968283\n",
      "train loss:0.00826663992338422\n",
      "train loss:0.06275164787951246\n",
      "train loss:0.000823445183427169\n",
      "train loss:0.00294265998020023\n",
      "train loss:0.007512483020771069\n",
      "train loss:0.011138514663102074\n",
      "train loss:0.012454838587209817\n",
      "train loss:0.018003585344578153\n",
      "train loss:0.01274826821336852\n",
      "train loss:0.010095029561539858\n",
      "train loss:0.002029835688098237\n",
      "train loss:0.003990498563117254\n",
      "train loss:0.017770795993281963\n",
      "train loss:0.004619054495613522\n",
      "train loss:0.0018498033246484905\n",
      "train loss:0.001631285545173245\n",
      "train loss:0.00215793562939432\n",
      "train loss:0.020356558996772165\n",
      "train loss:0.03106007543934847\n",
      "train loss:0.005587539901664824\n",
      "train loss:0.0020043316260647793\n",
      "train loss:0.03555633586076274\n",
      "train loss:0.006475634758186451\n",
      "train loss:0.00734536245456059\n",
      "train loss:0.0211845297169992\n",
      "train loss:0.002151372673580366\n",
      "train loss:0.009413584288576554\n",
      "train loss:0.005958659733236158\n",
      "train loss:0.005892391677656015\n",
      "train loss:0.034873615110607804\n",
      "train loss:0.0051122587846323675\n",
      "train loss:0.0019453488724890638\n",
      "train loss:0.013877596961130337\n",
      "train loss:0.008987729146249951\n",
      "train loss:0.01163261558477449\n",
      "train loss:0.02700939144004685\n",
      "train loss:0.005822953142237995\n",
      "train loss:0.00250514627653955\n",
      "train loss:0.0025624390673992374\n",
      "train loss:0.015431763185241558\n",
      "train loss:0.007391052394927146\n",
      "train loss:0.0014340733731158265\n",
      "train loss:0.007231309033759842\n",
      "train loss:0.008557083782370635\n",
      "train loss:0.007404380837446832\n",
      "train loss:0.0060115393941660026\n",
      "train loss:0.019653019846466318\n",
      "train loss:0.001877745024590552\n",
      "train loss:0.0022488446089188358\n",
      "train loss:0.03902844196458874\n",
      "train loss:0.005121012207056624\n",
      "train loss:0.009983449274609606\n",
      "train loss:0.03937012018101314\n",
      "train loss:0.008092518065096707\n",
      "train loss:0.0024140516540254674\n",
      "train loss:0.03952176266864937\n",
      "train loss:0.0033943972779675725\n",
      "train loss:0.0018694343223850081\n",
      "train loss:0.013014533352186938\n",
      "train loss:0.0067420056099264615\n",
      "train loss:0.003289043867426041\n",
      "train loss:0.0034062085632651047\n",
      "train loss:0.004213615330753135\n",
      "train loss:0.008540002257825068\n",
      "train loss:0.0006124260256294896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.008391592812787051\n",
      "train loss:0.0036533507645679763\n",
      "train loss:0.0014508696162461494\n",
      "train loss:0.009045706037790683\n",
      "train loss:0.0077430067198538685\n",
      "train loss:0.018876823639274117\n",
      "train loss:0.00037428948558657115\n",
      "train loss:0.0038670265472742766\n",
      "train loss:0.004035914933722109\n",
      "train loss:0.0012083268720794252\n",
      "train loss:0.01232363737889929\n",
      "train loss:0.005918038608644016\n",
      "train loss:0.004360817387280313\n",
      "train loss:0.00786242798030657\n",
      "train loss:0.005293821024359905\n",
      "train loss:0.023083949843986692\n",
      "train loss:0.00631803374641259\n",
      "train loss:0.006292766332649563\n",
      "train loss:0.007660476077085701\n",
      "train loss:0.009480674500273704\n",
      "train loss:0.001768486083028334\n",
      "train loss:0.0025201499151643273\n",
      "train loss:0.03219553166950749\n",
      "train loss:0.0026829011063052664\n",
      "train loss:0.003725012361768407\n",
      "train loss:0.007977048139965555\n",
      "train loss:0.0017610653557596121\n",
      "train loss:0.03602644286088842\n",
      "train loss:0.010840989803375563\n",
      "train loss:0.006073897890724785\n",
      "train loss:0.006089095168469563\n",
      "train loss:0.01376067164040062\n",
      "train loss:0.011853527444795928\n",
      "train loss:0.06103388677659785\n",
      "train loss:0.006886792356358205\n",
      "train loss:0.001230040888461128\n",
      "train loss:0.016121024725418306\n",
      "train loss:0.012045853523409731\n",
      "train loss:0.012707319632470926\n",
      "train loss:0.0036400261448536466\n",
      "train loss:0.0014209647454615166\n",
      "train loss:0.009631269613690962\n",
      "train loss:0.013032499285569744\n",
      "train loss:0.04225987183680228\n",
      "train loss:0.004371240713691748\n",
      "train loss:0.007112106892819191\n",
      "train loss:0.004826439571886039\n",
      "train loss:0.001949583951377613\n",
      "train loss:0.059641809514241854\n",
      "train loss:0.0037348995885138907\n",
      "train loss:0.0014743394640452142\n",
      "train loss:0.004113497876378659\n",
      "train loss:0.004139220896802654\n",
      "train loss:0.003510363229842151\n",
      "train loss:0.025654804941864805\n",
      "train loss:0.004542856038873351\n",
      "train loss:0.006443335695517547\n",
      "train loss:0.014625118325155097\n",
      "train loss:0.006305584209143077\n",
      "train loss:0.007387956982498086\n",
      "train loss:0.0030683275202085985\n",
      "train loss:0.006831555568577753\n",
      "train loss:0.002283379407586952\n",
      "train loss:0.0033205968064216034\n",
      "train loss:0.027531071889799766\n",
      "train loss:0.003391393045714042\n",
      "train loss:0.02507102435525432\n",
      "train loss:0.013680943218649646\n",
      "train loss:0.009875906384156289\n",
      "train loss:0.0028926729979270945\n",
      "train loss:0.004222040245470805\n",
      "train loss:0.008469276811859237\n",
      "train loss:0.008537473111936087\n",
      "train loss:0.004565354025011232\n",
      "train loss:0.003200386327032097\n",
      "train loss:0.009697668719053303\n",
      "train loss:0.011449036945443723\n",
      "train loss:0.004334967223886767\n",
      "train loss:0.002959521110609022\n",
      "train loss:0.03942268385848869\n",
      "train loss:0.006516245659111391\n",
      "train loss:0.005313349593803178\n",
      "train loss:0.016902645841320254\n",
      "train loss:0.015669774450392304\n",
      "train loss:0.002202573554431314\n",
      "train loss:0.038448526947917484\n",
      "train loss:0.0051545458242253745\n",
      "train loss:0.002373783783136816\n",
      "train loss:0.007289499508627606\n",
      "train loss:0.020887233250546863\n",
      "train loss:0.017709951013109078\n",
      "train loss:0.0033056377049683985\n",
      "train loss:0.005608618199352146\n",
      "train loss:0.0028366436526093735\n",
      "train loss:0.016447469433329507\n",
      "=== epoch:12, train acc:0.9966938405797101, test acc:0.9868659420289855 ===\n",
      "train loss:0.02083052592267962\n",
      "train loss:0.015790834723661648\n",
      "train loss:0.0073682097583905055\n",
      "train loss:0.018281896652577012\n",
      "train loss:0.017900813526936985\n",
      "train loss:0.007205038346924741\n",
      "train loss:0.005908376590608694\n",
      "train loss:0.006177219444520543\n",
      "train loss:0.013813530272464132\n",
      "train loss:0.0064980350341048\n",
      "train loss:0.01019304125155907\n",
      "train loss:0.004461515510125276\n",
      "train loss:0.009043543218729987\n",
      "train loss:0.01978868777639387\n",
      "train loss:0.017019072484682055\n",
      "train loss:0.003884913408411467\n",
      "train loss:0.047615844869959956\n",
      "train loss:0.031455629695901016\n",
      "train loss:0.005655319715111052\n",
      "train loss:0.00931217503299197\n",
      "train loss:0.007202736020832117\n",
      "train loss:0.009383879043132877\n",
      "train loss:0.020298241481469616\n",
      "train loss:0.0176607360809297\n",
      "train loss:0.0027879242434192754\n",
      "train loss:0.007332080078907404\n",
      "train loss:0.0013821323928506948\n",
      "train loss:0.010703433458947042\n",
      "train loss:0.002986239834630079\n",
      "train loss:0.004644110738601214\n",
      "train loss:0.033793192070131005\n",
      "train loss:0.009320925580620695\n",
      "train loss:0.002497487893900964\n",
      "train loss:0.00562569793829281\n",
      "train loss:0.002681800105523157\n",
      "train loss:0.01586642836505914\n",
      "train loss:0.0076001340173957295\n",
      "train loss:0.006873226804283369\n",
      "train loss:0.007558432090729146\n",
      "train loss:0.0017044469390185995\n",
      "train loss:0.021615664380378614\n",
      "train loss:0.004907805907580041\n",
      "train loss:0.011700328055906177\n",
      "train loss:0.0022360021912001676\n",
      "train loss:0.008698830555766125\n",
      "train loss:0.014533565999747323\n",
      "train loss:0.0033880693385628666\n",
      "train loss:0.0026812168219637407\n",
      "train loss:0.013421463677766057\n",
      "train loss:0.005693771362372683\n",
      "train loss:0.0017535870627654823\n",
      "train loss:0.009615183502079044\n",
      "train loss:0.005152102288929969\n",
      "train loss:0.003273770897096584\n",
      "train loss:0.0009997020484576462\n",
      "train loss:0.010054889211913077\n",
      "train loss:0.019350302408914097\n",
      "train loss:0.0028450434275316765\n",
      "train loss:0.012895320241086439\n",
      "train loss:0.0019631268918516185\n",
      "train loss:0.005542659499165831\n",
      "train loss:0.0009770939359187976\n",
      "train loss:0.002396854534390593\n",
      "train loss:0.0009799469264139755\n",
      "train loss:0.015426460081152383\n",
      "train loss:0.01239466154417768\n",
      "train loss:0.018195543757905844\n",
      "train loss:0.0015481874434551302\n",
      "train loss:0.018977613886534996\n",
      "train loss:0.0038780594399999155\n",
      "train loss:0.038730343937814195\n",
      "train loss:0.0021892723405892898\n",
      "train loss:0.013929090787563072\n",
      "train loss:0.07535231178350292\n",
      "train loss:0.0020454699840854198\n",
      "train loss:0.009688646797533621\n",
      "train loss:0.00448370398227287\n",
      "train loss:0.00634994532200703\n",
      "train loss:0.006995177849080551\n",
      "train loss:0.0014853779768647457\n",
      "train loss:0.005073845120916943\n",
      "train loss:0.005700561576755826\n",
      "train loss:0.030595076384938857\n",
      "train loss:0.002337868115625763\n",
      "train loss:0.007346452925666947\n",
      "train loss:0.0023204628587374086\n",
      "train loss:0.0016101488348357715\n",
      "train loss:0.002268440548417349\n",
      "train loss:0.03111802248820172\n",
      "train loss:0.0018050017320092948\n",
      "train loss:0.006964936242585671\n",
      "train loss:0.01959856476029139\n",
      "train loss:0.009824404576911904\n",
      "train loss:0.027091150395204676\n",
      "train loss:0.012030696031205634\n",
      "train loss:0.004041782296494915\n",
      "train loss:0.002439221090104776\n",
      "train loss:0.018993438209804602\n",
      "train loss:0.007666390794270087\n",
      "train loss:0.012187941600441387\n",
      "train loss:0.02876505801773423\n",
      "train loss:0.003667178642360666\n",
      "train loss:0.003739694299274321\n",
      "train loss:0.005742597330839493\n",
      "train loss:0.007160448050088367\n",
      "train loss:0.010565954152293333\n",
      "train loss:0.0064153593938494295\n",
      "train loss:0.0027612296987253866\n",
      "train loss:0.0013083267199749766\n",
      "train loss:0.008068797734538856\n",
      "train loss:0.0017159707874869069\n",
      "train loss:0.002860416704171158\n",
      "train loss:0.0044318763644107416\n",
      "train loss:0.010555754195160085\n",
      "train loss:0.01391737263187831\n",
      "train loss:0.03880133668363918\n",
      "train loss:0.006011786407627485\n",
      "train loss:0.002242918241473549\n",
      "train loss:0.03157688100307474\n",
      "train loss:0.01633098742184712\n",
      "train loss:0.0036655839570171617\n",
      "train loss:0.005917531921376314\n",
      "train loss:0.04099602698390954\n",
      "train loss:0.017895455481560112\n",
      "train loss:0.0029560623496915238\n",
      "train loss:0.003093743007458341\n",
      "train loss:0.007622101811911681\n",
      "train loss:0.005790070848990912\n",
      "train loss:0.009377180341968333\n",
      "train loss:0.004824591167380957\n",
      "train loss:0.0019223530239548918\n",
      "train loss:0.012545188801354424\n",
      "train loss:0.01068871355152445\n",
      "train loss:0.011361775585403215\n",
      "train loss:0.006977808929625143\n",
      "train loss:0.008165634682529807\n",
      "train loss:0.0034683377656607706\n",
      "train loss:0.028320995643454003\n",
      "train loss:0.0012934840738581463\n",
      "train loss:0.004388167832723596\n",
      "train loss:0.008623364520236737\n",
      "train loss:0.0034250318936230226\n",
      "train loss:0.00785031830188648\n",
      "train loss:0.002402073216031005\n",
      "train loss:0.002010569903926573\n",
      "train loss:0.003405286751533826\n",
      "train loss:0.011036208349548547\n",
      "train loss:0.017431478975623027\n",
      "train loss:0.012136143960449071\n",
      "train loss:0.008015124172669212\n",
      "train loss:0.0029745148004087507\n",
      "train loss:0.013856456105862749\n",
      "train loss:0.005908409570950796\n",
      "train loss:0.005270078855609191\n",
      "train loss:0.00719648316920841\n",
      "train loss:0.005426698610497525\n",
      "train loss:0.0013031125842657661\n",
      "train loss:0.004763856467305443\n",
      "train loss:0.02382131022027693\n",
      "train loss:0.001971620637246138\n",
      "train loss:0.018760969537473703\n",
      "train loss:0.001312680247897483\n",
      "train loss:0.02242179417816619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0022803944890409924\n",
      "train loss:0.004338179973546135\n",
      "train loss:0.007939792284642469\n",
      "train loss:0.02388214493654494\n",
      "train loss:0.004286757345783829\n",
      "train loss:0.01537026418881431\n",
      "train loss:0.001305327168800268\n",
      "train loss:0.015251353597443786\n",
      "train loss:0.015962021087111058\n",
      "train loss:0.013541781115530793\n",
      "train loss:0.003634396787970205\n",
      "train loss:0.008787253098843496\n",
      "train loss:0.001899507751105938\n",
      "train loss:0.007254205227056139\n",
      "train loss:0.015627479264897458\n",
      "train loss:0.06259567638115736\n",
      "train loss:0.011881001728902392\n",
      "train loss:0.017134705780191606\n",
      "train loss:0.0031083728774619823\n",
      "train loss:0.006855081977106403\n",
      "train loss:0.004403400820770945\n",
      "train loss:0.0065947207355103905\n",
      "train loss:0.011772770207278578\n",
      "train loss:0.021722806976047625\n",
      "train loss:0.005793697265567283\n",
      "train loss:0.004511516715896534\n",
      "train loss:0.00869872922030225\n",
      "train loss:0.00569729682048114\n",
      "train loss:0.01185149678534441\n",
      "train loss:0.01109820993362826\n",
      "train loss:0.005328218433492244\n",
      "train loss:0.03688991462890785\n",
      "train loss:0.003739850907926759\n",
      "train loss:0.0046843734014638\n",
      "train loss:0.0022544414427853794\n",
      "train loss:0.005394867717429007\n",
      "train loss:0.04448098085203407\n",
      "train loss:0.004212033492732135\n",
      "train loss:0.0022050075900185593\n",
      "train loss:0.007989837315496797\n",
      "train loss:0.006482013795932597\n",
      "train loss:0.030647873353866376\n",
      "train loss:0.005615114973622901\n",
      "train loss:0.006324019764720745\n",
      "train loss:0.02736898026011307\n",
      "train loss:0.018003497095397957\n",
      "train loss:0.0035751374810174498\n",
      "train loss:0.006143399182537369\n",
      "train loss:0.02468322405342361\n",
      "train loss:0.006350133368851245\n",
      "train loss:0.01703211055636944\n",
      "train loss:0.019263152538502335\n",
      "train loss:0.01561871822770478\n",
      "train loss:0.01661253179085207\n",
      "train loss:0.006102674638647998\n",
      "train loss:0.01861922867150469\n",
      "train loss:0.015988687026268884\n",
      "train loss:0.011380865520482288\n",
      "train loss:0.0023927465664125495\n",
      "train loss:0.007362215573085738\n",
      "train loss:0.0065581606476745906\n",
      "train loss:0.001906266471092799\n",
      "train loss:0.026125133177136606\n",
      "train loss:0.004392911963266145\n",
      "train loss:0.018666125527215197\n",
      "train loss:0.020696900489255966\n",
      "train loss:0.004961156997803844\n",
      "train loss:0.002640699284980703\n",
      "train loss:0.011097731802843189\n",
      "train loss:0.0016854928403184448\n",
      "train loss:0.015760354599427494\n",
      "train loss:0.03069543185807745\n",
      "train loss:0.010795985948250875\n",
      "train loss:0.008871887616986818\n",
      "train loss:0.007677553823011085\n",
      "train loss:0.00662684058340288\n",
      "train loss:0.023075879522192977\n",
      "train loss:0.0016080034492703774\n",
      "train loss:0.005606102627471902\n",
      "train loss:0.01882450405983553\n",
      "train loss:0.003966952396927766\n",
      "train loss:0.005591815913714457\n",
      "train loss:0.009294644399262132\n",
      "train loss:0.025280088513831823\n",
      "train loss:0.03726236106545842\n",
      "train loss:0.015518822137232189\n",
      "train loss:0.018400192438167748\n",
      "train loss:0.004509302523322669\n",
      "train loss:0.006603314762619473\n",
      "train loss:0.0053836343981757215\n",
      "train loss:0.007508951868602254\n",
      "train loss:0.006825893483482236\n",
      "train loss:0.03117148975737549\n",
      "train loss:0.013805062722845643\n",
      "train loss:0.011474943606414682\n",
      "train loss:0.0058094658544590545\n",
      "train loss:0.00444529633838048\n",
      "train loss:0.0036411749261046496\n",
      "train loss:0.013609507643718589\n",
      "train loss:0.022799473099396658\n",
      "train loss:0.031248253596273264\n",
      "train loss:0.007284760109687447\n",
      "train loss:0.010228430998746334\n",
      "train loss:0.004821924711299097\n",
      "train loss:0.00431731169538503\n",
      "train loss:0.026515582438388096\n",
      "train loss:0.012052927355236172\n",
      "train loss:0.008752439820714908\n",
      "train loss:0.022962914128755566\n",
      "train loss:0.012710382403968507\n",
      "train loss:0.006846797608776767\n",
      "train loss:0.005512287789440585\n",
      "train loss:0.01359154712524316\n",
      "train loss:0.010966627322609624\n",
      "train loss:0.008233519858396808\n",
      "train loss:0.008828370463743136\n",
      "train loss:0.005042970985120893\n",
      "train loss:0.008896991060614813\n",
      "train loss:0.0028884130208580703\n",
      "train loss:0.014691511801195888\n",
      "train loss:0.021876692686891578\n",
      "train loss:0.011638699587167975\n",
      "train loss:0.002753220433358025\n",
      "train loss:0.014326777123617369\n",
      "train loss:0.014653174372339301\n",
      "train loss:0.005288851904101116\n",
      "train loss:0.008647025786146552\n",
      "train loss:0.016392034347349548\n",
      "train loss:0.0038285233550111935\n",
      "train loss:0.010487444517888392\n",
      "train loss:0.005881169606910176\n",
      "train loss:0.008781792290214465\n",
      "train loss:0.011085177829650551\n",
      "train loss:0.030528390056329385\n",
      "train loss:0.010164651440602685\n",
      "train loss:0.011769229718489963\n",
      "train loss:0.004320482492939279\n",
      "train loss:0.03249621895520306\n",
      "train loss:0.01383127064375859\n",
      "train loss:0.013668151258568661\n",
      "train loss:0.004528189160374839\n",
      "train loss:0.03136704756883827\n",
      "train loss:0.003825446050100442\n",
      "train loss:0.01553140049098358\n",
      "train loss:0.004034260254670717\n",
      "train loss:0.0038249464129527784\n",
      "train loss:0.02944137232593451\n",
      "train loss:0.00909971874974232\n",
      "train loss:0.008373310979331534\n",
      "train loss:0.002604553583979904\n",
      "train loss:0.0032684326351501555\n",
      "train loss:0.011664298244134957\n",
      "train loss:0.008512381673558778\n",
      "train loss:0.0066583771700619195\n",
      "train loss:0.028683506586483838\n",
      "train loss:0.008784962224063211\n",
      "train loss:0.003973402641409275\n",
      "train loss:0.0006522044208885481\n",
      "train loss:0.0030973530988868477\n",
      "train loss:0.0008800942919181331\n",
      "train loss:0.010579195405130309\n",
      "train loss:0.0035336398325059555\n",
      "train loss:0.00219341004263452\n",
      "train loss:0.003743250006098919\n",
      "train loss:0.005768439829076723\n",
      "train loss:0.0032806049409143936\n",
      "train loss:0.0012992055188374228\n",
      "train loss:0.0368952990994847\n",
      "train loss:0.01217122953715963\n",
      "train loss:0.008665312390440473\n",
      "train loss:0.006158936683439289\n",
      "train loss:0.0019025172386921078\n",
      "train loss:0.0008819633300115435\n",
      "train loss:0.0017226504632578145\n",
      "train loss:0.004485480234254767\n",
      "train loss:0.0014049285457397105\n",
      "train loss:0.010004801088973692\n",
      "train loss:0.00854394190115638\n",
      "train loss:0.0024405098089753064\n",
      "train loss:0.002837637292039389\n",
      "train loss:0.009926890977652918\n",
      "train loss:0.028329200425595495\n",
      "train loss:0.003219914710530619\n",
      "train loss:0.007358435818268329\n",
      "train loss:0.0008231665591863809\n",
      "train loss:0.0021180329346958742\n",
      "train loss:0.0029778467525319874\n",
      "train loss:0.008679641878552186\n",
      "train loss:0.012129612456215319\n",
      "train loss:0.04434215311557165\n",
      "train loss:0.006349462886975606\n",
      "train loss:0.0110082705335249\n",
      "train loss:0.0029969368227425837\n",
      "train loss:0.002784443980924948\n",
      "train loss:0.0010611578004463714\n",
      "train loss:0.002349488409982765\n",
      "train loss:0.0016984494707151453\n",
      "train loss:0.006942427553730845\n",
      "train loss:0.0018703186896652888\n",
      "train loss:0.0027625133096848805\n",
      "train loss:0.005446699010845534\n",
      "train loss:0.006865737059738976\n",
      "train loss:0.04909079795900749\n",
      "train loss:0.03871243576853624\n",
      "train loss:0.000488362879840241\n",
      "=== epoch:13, train acc:0.9963088768115942, test acc:0.9858695652173913 ===\n",
      "train loss:0.0013418740362036257\n",
      "train loss:0.001104851408841581\n",
      "train loss:0.02248177991861305\n",
      "train loss:0.0012635986355440981\n",
      "train loss:0.04724052958162061\n",
      "train loss:0.003621250961646949\n",
      "train loss:0.008663844220021564\n",
      "train loss:0.020840061972112304\n",
      "train loss:0.01826418921998694\n",
      "train loss:0.0026147159458027773\n",
      "train loss:0.019209344399475504\n",
      "train loss:0.0046957849877832265\n",
      "train loss:0.003946132504532225\n",
      "train loss:0.0038034149109091348\n",
      "train loss:0.007378331250129139\n",
      "train loss:0.0038135020253169303\n",
      "train loss:0.002693863315538549\n",
      "train loss:0.0024062723871565166\n",
      "train loss:0.006582391944458883\n",
      "train loss:0.0325311049386999\n",
      "train loss:0.018940014246680285\n",
      "train loss:0.004056635974187177\n",
      "train loss:0.006167005215710531\n",
      "train loss:0.006887146240217392\n",
      "train loss:0.01653518129715569\n",
      "train loss:0.019055235157816937\n",
      "train loss:0.003984137878682403\n",
      "train loss:0.031021918478694812\n",
      "train loss:0.0013105441055873042\n",
      "train loss:0.007288630652836662\n",
      "train loss:0.0093425425639016\n",
      "train loss:0.03146134592024491\n",
      "train loss:0.0036716755595057634\n",
      "train loss:0.0028527362635455264\n",
      "train loss:0.007640482546797089\n",
      "train loss:0.0038596454674381248\n",
      "train loss:0.016045113445111163\n",
      "train loss:0.031022208152435504\n",
      "train loss:0.024997716184012735\n",
      "train loss:0.014228141884935786\n",
      "train loss:0.020542500369043638\n",
      "train loss:0.007074857391480077\n",
      "train loss:0.0058546037538708484\n",
      "train loss:0.007642331822625382\n",
      "train loss:0.00875752992810632\n",
      "train loss:0.005053127902003157\n",
      "train loss:0.006587742119799062\n",
      "train loss:0.012018597724286428\n",
      "train loss:0.004569494421759411\n",
      "train loss:0.00281278063845255\n",
      "train loss:0.005724978686874251\n",
      "train loss:0.018910388144101252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.01552815344917802\n",
      "train loss:0.0027303084841608727\n",
      "train loss:0.021329762851784395\n",
      "train loss:0.01630123082722316\n",
      "train loss:0.0059477847023164535\n",
      "train loss:0.0057645187703212215\n",
      "train loss:0.002937508528156264\n",
      "train loss:0.005674717713880257\n",
      "train loss:0.005781335667947043\n",
      "train loss:0.0009478885089768289\n",
      "train loss:0.00781540605240162\n",
      "train loss:0.0024636327981124974\n",
      "train loss:0.005980848285454499\n",
      "train loss:0.003212852165817223\n",
      "train loss:0.0037031277752271117\n",
      "train loss:0.002024550344645332\n",
      "train loss:0.012045578377618337\n",
      "train loss:0.016719017893230777\n",
      "train loss:0.003016700577476486\n",
      "train loss:0.002513821528345819\n",
      "train loss:0.004504451585613281\n",
      "train loss:0.002655278374949742\n",
      "train loss:0.002891288867836552\n",
      "train loss:0.004381813658332045\n",
      "train loss:0.004631422185777958\n",
      "train loss:0.01267215456494732\n",
      "train loss:0.06041773400907462\n",
      "train loss:0.0021756763196711187\n",
      "train loss:0.01580100285884119\n",
      "train loss:0.0025180581087791607\n",
      "train loss:0.005711611174212278\n",
      "train loss:0.015942271809090202\n",
      "train loss:0.024233124777409226\n",
      "train loss:0.054749514717053686\n",
      "train loss:0.0027434532055595883\n",
      "train loss:0.004316799388845855\n",
      "train loss:0.0037596417588808234\n",
      "train loss:0.0027949188569444464\n",
      "train loss:0.002642243205277636\n",
      "train loss:0.014491910610663575\n",
      "train loss:0.004982554845942743\n",
      "train loss:0.00394279649280982\n",
      "train loss:0.0022791220472087093\n",
      "train loss:0.0011421017868477878\n",
      "train loss:0.0011918275694579742\n",
      "train loss:0.0014667337770845406\n",
      "train loss:0.002929187901713074\n",
      "train loss:0.00339231272829089\n",
      "train loss:0.0027590825364822305\n",
      "train loss:0.004411265773011729\n",
      "train loss:0.015572655003178781\n",
      "train loss:0.00788295882586176\n",
      "train loss:0.0214543531163943\n",
      "train loss:0.009096562266616348\n",
      "train loss:0.008810530291247953\n",
      "train loss:0.017894149515547943\n",
      "train loss:0.0037418260183336286\n",
      "train loss:0.013864619061071145\n",
      "train loss:0.0026014163396039635\n",
      "train loss:0.009736850760275032\n",
      "train loss:0.027487380294093502\n",
      "train loss:0.006076814869191487\n",
      "train loss:0.003850374087264164\n",
      "train loss:0.010644095329923244\n",
      "train loss:0.018758747182062534\n",
      "train loss:0.00436114461699495\n",
      "train loss:0.0036750441322475026\n",
      "train loss:0.032656506342087555\n",
      "train loss:0.00242724916220655\n",
      "train loss:0.004007380156822389\n",
      "train loss:0.022359243475944195\n",
      "train loss:0.0058821385230614755\n",
      "train loss:0.00439066832210097\n",
      "train loss:0.056516210697183475\n",
      "train loss:0.0049719138299161685\n",
      "train loss:0.020839539992150366\n",
      "train loss:0.021184663337997445\n",
      "train loss:0.0031998941254458373\n",
      "train loss:0.004985713816609982\n",
      "train loss:0.0031178545807425276\n",
      "train loss:0.0054616742642016654\n",
      "train loss:0.014429289695877902\n",
      "train loss:0.02580435470949953\n",
      "train loss:0.07614321090487214\n",
      "train loss:0.01289442539310131\n",
      "train loss:0.004394538132418968\n",
      "train loss:0.009898681276268936\n",
      "train loss:0.008615441699789042\n",
      "train loss:0.007312819821432897\n",
      "train loss:0.05055213440178097\n",
      "train loss:0.02719477082110102\n",
      "train loss:0.003874916533698122\n",
      "train loss:0.0022720594819003194\n",
      "train loss:0.0028423429282809242\n",
      "train loss:0.044039829066370786\n",
      "train loss:0.01690789393973048\n",
      "train loss:0.02948136614296699\n",
      "train loss:0.0623339854489598\n",
      "train loss:0.00797992524563204\n",
      "train loss:0.006789276598726394\n",
      "train loss:0.015863351453923598\n",
      "train loss:0.050076755930422934\n",
      "train loss:0.003088359464922545\n",
      "train loss:0.004438559553599466\n",
      "train loss:0.0063573836908087135\n",
      "train loss:0.005828588603167598\n",
      "train loss:0.001703050757377043\n",
      "train loss:0.0028955960929309067\n",
      "train loss:0.002232138207037521\n",
      "train loss:0.00333471721733976\n",
      "train loss:0.0028417534739101076\n",
      "train loss:0.006941694402127487\n",
      "train loss:0.0061189484299589\n",
      "train loss:0.0032270190672861734\n",
      "train loss:0.008652960162456117\n",
      "train loss:0.003664202007114095\n",
      "train loss:0.015014322779138872\n",
      "train loss:0.015125917842546006\n",
      "train loss:0.0060294744548872155\n",
      "train loss:0.004739444074807041\n",
      "train loss:0.03817649031545041\n",
      "train loss:0.012919184780928895\n",
      "train loss:0.0037188348990355455\n",
      "train loss:0.004723992356080245\n",
      "train loss:0.0008231998880528312\n",
      "train loss:0.0038597115502384807\n",
      "train loss:0.00936470699094564\n",
      "train loss:0.002762811590528297\n",
      "train loss:0.004871962400945716\n",
      "train loss:0.011527356989684283\n",
      "train loss:0.002328737700805228\n",
      "train loss:0.024766944761827992\n",
      "train loss:0.0049159631134568825\n",
      "train loss:0.0014668357095518024\n",
      "train loss:0.0028825273153776703\n",
      "train loss:0.0050929896941787085\n",
      "train loss:0.029258609560479352\n",
      "train loss:0.0038058311063330964\n",
      "train loss:0.004378297834803208\n",
      "train loss:0.0013346140844513465\n",
      "train loss:0.018165077665367117\n",
      "train loss:0.010661402774601073\n",
      "train loss:0.0008630382657472296\n",
      "train loss:0.011332075728201104\n",
      "train loss:0.03636171704457229\n",
      "train loss:0.005523820934061444\n",
      "train loss:0.00258417283215194\n",
      "train loss:0.012230134621707085\n",
      "train loss:0.0037727393621463825\n",
      "train loss:0.036828214901389655\n",
      "train loss:0.0004674059237223586\n",
      "train loss:0.02036437733661205\n",
      "train loss:0.011043081885101395\n",
      "train loss:0.0010938850776562041\n",
      "train loss:0.05769634028144698\n",
      "train loss:0.005870598486862442\n",
      "train loss:0.007599181753677278\n",
      "train loss:0.00998844206879699\n",
      "train loss:0.003077695589424667\n",
      "train loss:0.01936796985613641\n",
      "train loss:0.010989682143265145\n",
      "train loss:0.0025253595622423694\n",
      "train loss:0.00225921259706272\n",
      "train loss:0.0019892110639889884\n",
      "train loss:0.020936899694684062\n",
      "train loss:0.055536506204712205\n",
      "train loss:0.007994282045485636\n",
      "train loss:0.0025876477420469085\n",
      "train loss:0.004360032532988623\n",
      "train loss:0.009582333930750066\n",
      "train loss:0.0008147614489552119\n",
      "train loss:0.009860868656280008\n",
      "train loss:0.001613111604872526\n",
      "train loss:0.0015296278591614336\n",
      "train loss:0.0011817304375408753\n",
      "train loss:0.02118390897235656\n",
      "train loss:0.01443067676581025\n",
      "train loss:0.004795082560264646\n",
      "train loss:0.016350733637154294\n",
      "train loss:0.0012442041897460902\n",
      "train loss:0.003320010557359123\n",
      "train loss:0.0015042710725677736\n",
      "train loss:0.00234320288918576\n",
      "train loss:0.002554704180231325\n",
      "train loss:0.0006097250501037592\n",
      "train loss:0.004366126018679987\n",
      "train loss:0.000589181117675752\n",
      "train loss:0.0033507543557173935\n",
      "train loss:0.013725387437311045\n",
      "train loss:0.01443585422217922\n",
      "train loss:0.003256713006854317\n",
      "train loss:0.0047540056710592355\n",
      "train loss:0.0007545781282899863\n",
      "train loss:0.002344959607318298\n",
      "train loss:0.048644328003541994\n",
      "train loss:0.0027408294153287824\n",
      "train loss:0.0029295777718654392\n",
      "train loss:0.004385975867850933\n",
      "train loss:0.005412233396155708\n",
      "train loss:0.003029739372354834\n",
      "train loss:0.002243240059524114\n",
      "train loss:0.0015849399818610864\n",
      "train loss:0.003220104562844206\n",
      "train loss:0.015420515567554314\n",
      "train loss:0.0022351255768771968\n",
      "train loss:0.004781699001113843\n",
      "train loss:0.0021191043786523357\n",
      "train loss:0.0007786908563201921\n",
      "train loss:0.011252697925528495\n",
      "train loss:0.0021926609692260496\n",
      "train loss:0.004389778794172377\n",
      "train loss:0.032253243128742656\n",
      "train loss:0.007711393181281435\n",
      "train loss:0.020659131970563575\n",
      "train loss:0.0061539332130314895\n",
      "train loss:0.004661678666337799\n",
      "train loss:0.009474103149674654\n",
      "train loss:0.0018409156334198635\n",
      "train loss:0.005069278513273758\n",
      "train loss:0.003670155996545139\n",
      "train loss:0.01314185218225953\n",
      "train loss:0.006494054101302549\n",
      "train loss:0.011434338680258402\n",
      "train loss:0.001953045565329678\n",
      "train loss:0.07090182113161347\n",
      "train loss:0.0006259278357138048\n",
      "train loss:0.003860667238942811\n",
      "train loss:0.0007892116535369459\n",
      "train loss:0.02336616720875255\n",
      "train loss:0.013357610901670248\n",
      "train loss:0.00480599384703699\n",
      "train loss:0.013132382061639665\n",
      "train loss:0.012511333683444313\n",
      "train loss:0.0023347159174025704\n",
      "train loss:0.04220526489534759\n",
      "train loss:0.0021944404624848275\n",
      "train loss:0.018902450034359638\n",
      "train loss:0.0043368184388189015\n",
      "train loss:0.014187613945893145\n",
      "train loss:0.004011120538886196\n",
      "train loss:0.004728375527756306\n",
      "train loss:0.026614227685004946\n",
      "train loss:0.008004818439186786\n",
      "train loss:0.011827499266614487\n",
      "train loss:0.01437274066548407\n",
      "train loss:0.003834688442913227\n",
      "train loss:0.046373190290679905\n",
      "train loss:0.0023360024347982788\n",
      "train loss:0.01138879430128895\n",
      "train loss:0.003747615030649077\n",
      "train loss:0.012808049055176024\n",
      "train loss:0.013210134396674384\n",
      "train loss:0.03022579427241386\n",
      "train loss:0.0025789516460132766\n",
      "train loss:0.005746200554512303\n",
      "train loss:0.021923440191961404\n",
      "train loss:0.014466520000166272\n",
      "train loss:0.019853742347814226\n",
      "train loss:0.003551777168868896\n",
      "train loss:0.008345379856626004\n",
      "train loss:0.0035923531873207496\n",
      "train loss:0.0056496022705765075\n",
      "train loss:0.0035631956051372653\n",
      "train loss:0.009632342251342406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00914599038923927\n",
      "train loss:0.014669920051340806\n",
      "train loss:0.006638980056834284\n",
      "train loss:0.004243753643994954\n",
      "train loss:0.00626854230576247\n",
      "train loss:0.0032957551273626127\n",
      "train loss:0.004750584079890008\n",
      "train loss:0.0065292583468668985\n",
      "train loss:0.0027350927017235505\n",
      "train loss:0.0027149499926698776\n",
      "train loss:0.0019092316479038455\n",
      "train loss:0.005952570916635779\n",
      "train loss:0.00784792142800267\n",
      "train loss:0.00732204456821038\n",
      "train loss:0.002091716874437183\n",
      "train loss:0.0018114986819304036\n",
      "train loss:0.0011377676079450255\n",
      "train loss:0.019488471205624112\n",
      "train loss:0.010837155616495227\n",
      "train loss:0.0016693517053598\n",
      "train loss:0.0027270177599588996\n",
      "train loss:0.0058063759386785905\n",
      "train loss:0.0005409963817663393\n",
      "train loss:0.0014493431246225808\n",
      "train loss:0.03309893840484996\n",
      "train loss:0.0013065021435005862\n",
      "train loss:0.0022678961108114903\n",
      "train loss:0.0036692040937676344\n",
      "train loss:0.004850480951136977\n",
      "train loss:0.011347825123328348\n",
      "train loss:0.0020674445555979282\n",
      "train loss:0.001965278606890918\n",
      "train loss:0.04538750566668535\n",
      "train loss:0.004002519707385503\n",
      "train loss:0.0037463779222457324\n",
      "train loss:0.005982109520213465\n",
      "train loss:0.004296891711210902\n",
      "train loss:0.019902772523849067\n",
      "train loss:0.010010905038315254\n",
      "train loss:0.009989917610564066\n",
      "train loss:0.006282832297739502\n",
      "train loss:0.0031764337536649436\n",
      "train loss:0.005440672800701372\n",
      "train loss:0.002429643837177718\n",
      "train loss:0.005485552331714604\n",
      "train loss:0.003055240222941242\n",
      "train loss:0.01637881236107953\n",
      "train loss:0.011707393822490897\n",
      "train loss:0.002427725620250146\n",
      "train loss:0.016757471440054343\n",
      "train loss:0.0017738973590217485\n",
      "train loss:0.003975225437823668\n",
      "=== epoch:14, train acc:0.9971240942028986, test acc:0.9883152173913043 ===\n",
      "train loss:0.016989471578925653\n",
      "train loss:0.006242102321365106\n",
      "train loss:0.02120513108520627\n",
      "train loss:0.0037129302474081406\n",
      "train loss:0.00572283301299966\n",
      "train loss:0.0034529962882440812\n",
      "train loss:0.004717176083560973\n",
      "train loss:0.04369325352278153\n",
      "train loss:0.0033057104248853703\n",
      "train loss:0.00257554157096911\n",
      "train loss:0.00045050984644262593\n",
      "train loss:0.0011479064211514164\n",
      "train loss:0.008706904343770082\n",
      "train loss:0.0017026858319207213\n",
      "train loss:0.004318309236730037\n",
      "train loss:0.0007872624805382727\n",
      "train loss:0.006221657399810885\n",
      "train loss:0.0008832033810745638\n",
      "train loss:0.0013121668642837572\n",
      "train loss:0.0013772542526314876\n",
      "train loss:0.01898211171564054\n",
      "train loss:0.02183059566117804\n",
      "train loss:0.0059107115460217035\n",
      "train loss:0.0007770564896136342\n",
      "train loss:0.013285902385686783\n",
      "train loss:0.0019161923549556531\n",
      "train loss:0.014942424348095859\n",
      "train loss:0.00986116580376684\n",
      "train loss:0.0025777177844497562\n",
      "train loss:0.013848389402738285\n",
      "train loss:0.03209863588531957\n",
      "train loss:0.0046639299338230075\n",
      "train loss:0.00549361954561635\n",
      "train loss:0.019268439038929566\n",
      "train loss:0.004843259804168828\n",
      "train loss:0.0023841100688069047\n",
      "train loss:0.018616328724133234\n",
      "train loss:0.004438304193825375\n",
      "train loss:0.006724924651522533\n",
      "train loss:0.011131840469424562\n",
      "train loss:0.005721242630969746\n",
      "train loss:0.017052970235788143\n",
      "train loss:0.0044741314501791744\n",
      "train loss:0.016794660975306533\n",
      "train loss:0.004347570336542683\n",
      "train loss:0.00857465643392508\n",
      "train loss:0.003177413418328073\n",
      "train loss:0.01006836922626343\n",
      "train loss:0.004606912795893007\n",
      "train loss:0.014316366515996992\n",
      "train loss:0.003855750097260822\n",
      "train loss:0.02692488788998929\n",
      "train loss:0.003785256837418956\n",
      "train loss:0.0019977621308294004\n",
      "train loss:0.019037188370072546\n",
      "train loss:0.007221030993308495\n",
      "train loss:0.0018830500794105366\n",
      "train loss:0.006994321269282553\n",
      "train loss:0.004512637957232506\n",
      "train loss:0.0012788352599431322\n",
      "train loss:0.004782294279386964\n",
      "train loss:0.003200028977949321\n",
      "train loss:0.004371914874610764\n",
      "train loss:0.010578006209189245\n",
      "train loss:0.002021576269825219\n",
      "train loss:0.00491220250574777\n",
      "train loss:0.0032949616492042874\n",
      "train loss:0.006663996899974053\n",
      "train loss:0.0013015290316653347\n",
      "train loss:0.0039774826295395724\n",
      "train loss:0.003555768883576113\n",
      "train loss:0.005407312506152573\n",
      "train loss:0.007430093245099265\n",
      "train loss:0.0013956150829308297\n",
      "train loss:0.013316571671581337\n",
      "train loss:0.03910929705957051\n",
      "train loss:0.0010845147752650676\n",
      "train loss:0.00256454506914717\n",
      "train loss:0.003220453824318352\n",
      "train loss:0.01041680361736704\n",
      "train loss:0.0008164081522898115\n",
      "train loss:0.018553940060492897\n",
      "train loss:0.0008747103828380428\n",
      "train loss:0.00117898111000309\n",
      "train loss:0.001087163948612381\n",
      "train loss:0.025757152690901293\n",
      "train loss:0.02965063232195608\n",
      "train loss:0.01916893075786184\n",
      "train loss:0.011585574909657665\n",
      "train loss:0.03631602785974513\n",
      "train loss:0.029432394522538843\n",
      "train loss:0.004501625732326807\n",
      "train loss:0.0012836093197451965\n",
      "train loss:0.001355410500734168\n",
      "train loss:0.0009770191893673894\n",
      "train loss:0.004588558086078771\n",
      "train loss:0.0025263198395007657\n",
      "train loss:0.033678602333782615\n",
      "train loss:0.01624696028468439\n",
      "train loss:0.016662470256450505\n",
      "train loss:0.000642144065810054\n",
      "train loss:0.015031553386155378\n",
      "train loss:0.012741296031070156\n",
      "train loss:0.013739943707079361\n",
      "train loss:0.02972702053059939\n",
      "train loss:0.0015875590136790953\n",
      "train loss:0.007066556497325318\n",
      "train loss:0.0040030638415520774\n",
      "train loss:0.016607180270867678\n",
      "train loss:0.004834507093257031\n",
      "train loss:0.010573308918879696\n",
      "train loss:0.0012649029026971803\n",
      "train loss:0.0021094763042898957\n",
      "train loss:0.005871542294550503\n",
      "train loss:0.001998465041691833\n",
      "train loss:0.0028510902356286725\n",
      "train loss:0.0008398331131027534\n",
      "train loss:0.0029425478367041797\n",
      "train loss:0.004390833872776583\n",
      "train loss:0.0032862252105066354\n",
      "train loss:0.013308393530933977\n",
      "train loss:0.0037222819023269905\n",
      "train loss:0.004004477125789662\n",
      "train loss:0.00899402394845973\n",
      "train loss:0.009313035712336366\n",
      "train loss:0.00297710618070349\n",
      "train loss:0.003220532312170411\n",
      "train loss:0.001633889820479723\n",
      "train loss:0.0038969476352489624\n",
      "train loss:0.01939269875343775\n",
      "train loss:0.01067123264569178\n",
      "train loss:0.007659279537647775\n",
      "train loss:0.015278408713776873\n",
      "train loss:0.0175682770440685\n",
      "train loss:0.0034368837261035375\n",
      "train loss:0.004075589369208855\n",
      "train loss:0.0060987621524689496\n",
      "train loss:0.0033502187289411434\n",
      "train loss:0.002956738664649188\n",
      "train loss:0.00834187944709477\n",
      "train loss:0.001111093494906591\n",
      "train loss:0.0027472304125454408\n",
      "train loss:0.0030607362468693835\n",
      "train loss:0.012066400904152208\n",
      "train loss:0.017112330375781793\n",
      "train loss:0.008932530851472525\n",
      "train loss:0.010750231479607273\n",
      "train loss:0.002783797020662751\n",
      "train loss:0.009502319114967508\n",
      "train loss:0.007750347874282247\n",
      "train loss:0.009423407451276306\n",
      "train loss:0.0054762429985241124\n",
      "train loss:0.00430570287012842\n",
      "train loss:0.0014179515137988903\n",
      "train loss:0.016017861432861664\n",
      "train loss:0.0033882006735953744\n",
      "train loss:0.012214189712642128\n",
      "train loss:0.004073242324449226\n",
      "train loss:0.03883677832004593\n",
      "train loss:0.0024754242109330496\n",
      "train loss:0.0032349618757189827\n",
      "train loss:0.003085868823240949\n",
      "train loss:0.03867030288681594\n",
      "train loss:0.007741453588626472\n",
      "train loss:0.0029304547364222317\n",
      "train loss:0.0055259525844096455\n",
      "train loss:0.0033628662082132396\n",
      "train loss:0.005279586474677643\n",
      "train loss:0.005696708851900132\n",
      "train loss:0.014471132527430925\n",
      "train loss:0.004439772982715235\n",
      "train loss:0.08014873800772122\n",
      "train loss:0.01817842354637338\n",
      "train loss:0.008890494009309023\n",
      "train loss:0.007603779328043624\n",
      "train loss:0.010446240899761362\n",
      "train loss:0.010914249746619514\n",
      "train loss:0.006370654299033294\n",
      "train loss:0.001920885781985389\n",
      "train loss:0.005255501953446541\n",
      "train loss:0.006039478285970403\n",
      "train loss:0.006632482528794444\n",
      "train loss:0.0031836886718599544\n",
      "train loss:0.0029183710621688663\n",
      "train loss:0.0016296869789345825\n",
      "train loss:0.0014341532753679024\n",
      "train loss:0.025128635083376327\n",
      "train loss:0.004306811693072886\n",
      "train loss:0.0043510649086813775\n",
      "train loss:0.010109264286612133\n",
      "train loss:0.040411563864885564\n",
      "train loss:0.0007103594957047489\n",
      "train loss:0.000836511030235267\n",
      "train loss:0.0025200959691561065\n",
      "train loss:0.023900048911540062\n",
      "train loss:0.0018126402433923767\n",
      "train loss:0.0039529249536430985\n",
      "train loss:0.0029408296454837826\n",
      "train loss:0.007109003851626932\n",
      "train loss:0.010442634110816144\n",
      "train loss:0.008175320155919624\n",
      "train loss:0.003245546637709658\n",
      "train loss:0.02693611422528979\n",
      "train loss:0.0015790608136248164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.016994111021388352\n",
      "train loss:0.0014989618120807148\n",
      "train loss:0.0017446724125070184\n",
      "train loss:0.004282889257648626\n",
      "train loss:0.0014191495336868204\n",
      "train loss:0.013679619895413527\n",
      "train loss:0.002867260069210255\n",
      "train loss:0.015278814107945925\n",
      "train loss:0.0011389787940826175\n",
      "train loss:0.003213001754759851\n",
      "train loss:0.001571612728774882\n",
      "train loss:0.0020929842295569897\n",
      "train loss:0.03877660396424778\n",
      "train loss:0.002272892989369586\n",
      "train loss:0.0017065000608962576\n",
      "train loss:0.0007641373881487996\n",
      "train loss:0.0011969486987898092\n",
      "train loss:0.001566951044964395\n",
      "train loss:0.003335753326151539\n",
      "train loss:0.009490125930296632\n",
      "train loss:0.0037471476317057785\n",
      "train loss:0.0028993125555667344\n",
      "train loss:0.001471128574084376\n",
      "train loss:0.004291628372440198\n",
      "train loss:0.009784002455904337\n",
      "train loss:0.0038404235904254402\n",
      "train loss:0.0011801813931687937\n",
      "train loss:0.007584865281616106\n",
      "train loss:0.0017624392402655615\n",
      "train loss:0.005922426554836005\n",
      "train loss:0.0014136582507247037\n",
      "train loss:0.04445204352272435\n",
      "train loss:0.0007781160506889856\n",
      "train loss:0.01047577713249573\n",
      "train loss:0.01519428583501253\n",
      "train loss:0.0078064310126032405\n",
      "train loss:0.012289369271984818\n",
      "train loss:0.012325671929673375\n",
      "train loss:0.00923125771908715\n",
      "train loss:0.0008948912629463997\n",
      "train loss:0.005974456341557815\n",
      "train loss:0.006575316734505299\n",
      "train loss:0.0013427816878723198\n",
      "train loss:0.003199702614891228\n",
      "train loss:0.021790752730684607\n",
      "train loss:0.001613394468983895\n",
      "train loss:0.011181578131266004\n",
      "train loss:0.011022335223362824\n",
      "train loss:0.006507205708887784\n",
      "train loss:0.0021683072715113234\n",
      "train loss:0.021101063383109145\n",
      "train loss:0.00546873783128034\n",
      "train loss:0.00306742854388687\n",
      "train loss:0.009232136212315768\n",
      "train loss:0.005744777757484264\n",
      "train loss:0.004961805578889674\n",
      "train loss:0.006290374336700968\n",
      "train loss:0.004660740229535697\n",
      "train loss:0.006559321473331128\n",
      "train loss:0.0020798569281995096\n",
      "train loss:0.005757125973173147\n",
      "train loss:0.005131331889786587\n",
      "train loss:0.007100759689007931\n",
      "train loss:0.0010157351250935605\n",
      "train loss:0.0012313376630264372\n",
      "train loss:0.0031887993690501827\n",
      "train loss:0.003329956842213864\n",
      "train loss:0.0031542045918967598\n",
      "train loss:0.017696314657578642\n",
      "train loss:0.008263953150208133\n",
      "train loss:0.0011466696989904962\n",
      "train loss:0.0017486529140400007\n",
      "train loss:0.0093263072347254\n",
      "train loss:0.0060189132541651124\n",
      "train loss:0.002063392362409272\n",
      "train loss:0.004676318714172543\n",
      "train loss:0.0023845469587958136\n",
      "train loss:0.004971483777307348\n",
      "train loss:0.00947561699032313\n",
      "train loss:0.0021831550675032335\n",
      "train loss:0.007413617705469319\n",
      "train loss:0.0007925555929322492\n",
      "train loss:0.007140581588642762\n",
      "train loss:0.0003021540141601687\n",
      "train loss:0.02625847335598874\n",
      "train loss:0.013416786097524943\n",
      "train loss:0.01207368451743024\n",
      "train loss:0.003070118377716764\n",
      "train loss:0.0008193014008164699\n",
      "train loss:0.014285914222542193\n",
      "train loss:0.001007323330382128\n",
      "train loss:0.002052575256497039\n",
      "train loss:0.006357367946170128\n",
      "train loss:0.0022337409842277237\n",
      "train loss:0.0024183030792127736\n",
      "train loss:0.007165773496781671\n",
      "train loss:0.0018728631269539478\n",
      "train loss:0.003959029755733769\n",
      "train loss:0.0009390060468380974\n",
      "train loss:0.002045736889949504\n",
      "train loss:0.0007238459768810215\n",
      "train loss:0.006572104106125001\n",
      "train loss:0.02384446440528163\n",
      "train loss:0.0006544172030381272\n",
      "train loss:0.01771088275403184\n",
      "train loss:0.001977310784151099\n",
      "train loss:0.003064680940733411\n",
      "train loss:0.0009044514137422372\n",
      "train loss:0.0011008434643171\n",
      "train loss:0.0008305137167950737\n",
      "train loss:0.00047133393670386486\n",
      "train loss:0.013412838038006226\n",
      "train loss:0.004451493257110796\n",
      "train loss:0.01172854570120992\n",
      "train loss:0.011392119075397262\n",
      "train loss:0.019123042274946805\n",
      "train loss:0.008649999376287028\n",
      "train loss:0.001502687555517078\n",
      "train loss:0.0618597117010696\n",
      "train loss:0.023746193976367697\n",
      "train loss:0.01738409705880364\n",
      "train loss:0.00022154092798823568\n",
      "train loss:0.0018596740557329633\n",
      "train loss:0.0012084740681898682\n",
      "train loss:0.012121582914965541\n",
      "train loss:0.001078361601818136\n",
      "train loss:0.003206237763639249\n",
      "train loss:0.0007796779256036469\n",
      "train loss:0.002299871209836246\n",
      "train loss:0.009386492034771089\n",
      "train loss:0.0030229456119570063\n",
      "train loss:0.0020411987726579326\n",
      "train loss:0.003952642124625069\n",
      "train loss:0.01959894276538969\n",
      "train loss:0.0027090864172191033\n",
      "train loss:0.02610226621719848\n",
      "train loss:0.0033943895112039115\n",
      "train loss:0.006383697030870191\n",
      "train loss:0.0027609279933079943\n",
      "train loss:0.0036865368352978723\n",
      "train loss:0.00939950767127213\n",
      "train loss:0.02374038717269456\n",
      "train loss:0.01710875656505548\n",
      "train loss:0.0029347100239295624\n",
      "train loss:0.0008859782901959334\n",
      "train loss:0.003344003403835753\n",
      "train loss:0.002045226005458421\n",
      "train loss:0.021789705150571173\n",
      "train loss:0.003283175779417591\n",
      "train loss:0.006328093928205069\n",
      "train loss:0.0047296680655760535\n",
      "train loss:0.004640716299031774\n",
      "train loss:0.015184182069988065\n",
      "train loss:0.005840847926228717\n",
      "train loss:0.02508212328904213\n",
      "train loss:0.0035075660277784407\n",
      "train loss:0.008905941234097008\n",
      "train loss:0.0037909416574061967\n",
      "train loss:0.005478738157687095\n",
      "train loss:0.00538671819909599\n",
      "train loss:0.010438379169994913\n",
      "train loss:0.004242548500112153\n",
      "train loss:0.004444354633562598\n",
      "train loss:0.000738814717220762\n",
      "=== epoch:15, train acc:0.9966032608695652, test acc:0.9873188405797102 ===\n",
      "train loss:0.007508855580559807\n",
      "train loss:0.001128444304517885\n",
      "train loss:0.01256698722481519\n",
      "train loss:0.007091904181462895\n",
      "train loss:0.0005250288494819855\n",
      "train loss:0.0010948718014014828\n",
      "train loss:0.042417805252815986\n",
      "train loss:0.003759066687425688\n",
      "train loss:0.006548943972752962\n",
      "train loss:0.0009402620258856414\n",
      "train loss:0.008055066470746447\n",
      "train loss:0.0023700764918942944\n",
      "train loss:0.004608278680041318\n",
      "train loss:0.004620697199129971\n",
      "train loss:0.006929128623929589\n",
      "train loss:0.006305698569670564\n",
      "train loss:0.036453696470975024\n",
      "train loss:0.006365110727162532\n",
      "train loss:0.010595760220779642\n",
      "train loss:0.013423496603836894\n",
      "train loss:0.012499110017568331\n",
      "train loss:0.0008058994681316424\n",
      "train loss:0.004214956743553639\n",
      "train loss:0.005823636944679735\n",
      "train loss:0.0028055269574875944\n",
      "train loss:0.005254493288257293\n",
      "train loss:0.004115888362872419\n",
      "train loss:0.009736204745923483\n",
      "train loss:0.0008985023045865565\n",
      "train loss:0.05125602877603509\n",
      "train loss:0.014568333752901804\n",
      "train loss:0.003164252924744442\n",
      "train loss:0.004573972804787538\n",
      "train loss:0.005000876446485167\n",
      "train loss:0.002787793743521964\n",
      "train loss:0.012174152984569007\n",
      "train loss:0.004866140965923153\n",
      "train loss:0.003707318363492267\n",
      "train loss:0.002451289845701491\n",
      "train loss:0.005108821802597139\n",
      "train loss:0.0008640404445281258\n",
      "train loss:0.005334813994014695\n",
      "train loss:0.0005085290374561783\n",
      "train loss:0.008614878365038102\n",
      "train loss:0.004077682551874174\n",
      "train loss:0.006900285915000137\n",
      "train loss:0.004505376611721355\n",
      "train loss:0.003314684288825349\n",
      "train loss:0.0031169369012901803\n",
      "train loss:0.0022610916827783944\n",
      "train loss:0.0019225025548906048\n",
      "train loss:0.007815263917659831\n",
      "train loss:0.0031033559947559394\n",
      "train loss:0.002105814877088424\n",
      "train loss:0.005862102290764546\n",
      "train loss:0.002119968106564116\n",
      "train loss:0.0016875149539070443\n",
      "train loss:0.005299913630403503\n",
      "train loss:0.003252146999023896\n",
      "train loss:0.0011521900272081706\n",
      "train loss:0.002942525908245173\n",
      "train loss:0.0025648409329322636\n",
      "train loss:0.003717626786925402\n",
      "train loss:0.00463602231717361\n",
      "train loss:0.002448513544346829\n",
      "train loss:0.0015070661938916445\n",
      "train loss:0.0023083973890492483\n",
      "train loss:0.0019803178229434483\n",
      "train loss:0.0008219531926855866\n",
      "train loss:0.0022583883893657996\n",
      "train loss:0.005192389337820172\n",
      "train loss:0.008656199364556135\n",
      "train loss:0.006492850677088091\n",
      "train loss:0.025780939078911106\n",
      "train loss:0.0017245460482413354\n",
      "train loss:0.008308944330560558\n",
      "train loss:0.007644930948556285\n",
      "train loss:0.00415768768667628\n",
      "train loss:0.0043203799400597575\n",
      "train loss:0.03998050335600655\n",
      "train loss:0.0016111583232896872\n",
      "train loss:0.009032426785629896\n",
      "train loss:0.005794610099331135\n",
      "train loss:0.009212191040369352\n",
      "train loss:0.012988976900067069\n",
      "train loss:0.001614393133534485\n",
      "train loss:0.004328216157429068\n",
      "train loss:0.005765608609118867\n",
      "train loss:0.0009103420113016878\n",
      "train loss:0.006598148983533065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0015244008276039266\n",
      "train loss:0.0019261319425713326\n",
      "train loss:0.0041188489658359915\n",
      "train loss:0.002035082353219068\n",
      "train loss:0.0012172566282202186\n",
      "train loss:0.007388563329025709\n",
      "train loss:0.04098078594601035\n",
      "train loss:0.0030071900053583642\n",
      "train loss:0.008854866844189286\n",
      "train loss:0.0013989022630747838\n",
      "train loss:0.008635079058266997\n",
      "train loss:0.01156668465899758\n",
      "train loss:0.0028176679488180136\n",
      "train loss:0.0010127180583924232\n",
      "train loss:0.005985035584167525\n",
      "train loss:0.002198577164374401\n",
      "train loss:0.017926960863184863\n",
      "train loss:0.02773976169089909\n",
      "train loss:0.0037948860983123287\n",
      "train loss:0.0057758056172648505\n",
      "train loss:0.018319097142376728\n",
      "train loss:0.012587476866669367\n",
      "train loss:0.004116778839128549\n",
      "train loss:0.004978334762595109\n",
      "train loss:0.007436690734739684\n",
      "train loss:0.0027333723678189445\n",
      "train loss:0.005543437071488166\n",
      "train loss:0.014359164316167012\n",
      "train loss:0.0012833384787026881\n",
      "train loss:0.0005917300927626211\n",
      "train loss:0.0066704154025053215\n",
      "train loss:0.008902284707400466\n",
      "train loss:0.020804833363796196\n",
      "train loss:0.0023946220236944113\n",
      "train loss:0.006484381397306943\n",
      "train loss:0.000949151556809138\n",
      "train loss:0.0027644803848816605\n",
      "train loss:0.002274669410508831\n",
      "train loss:0.0019589580378020365\n",
      "train loss:0.003934697756323588\n",
      "train loss:0.00349094249821126\n",
      "train loss:0.01038930813129565\n",
      "train loss:0.008562800386801848\n",
      "train loss:0.0012853001668378012\n",
      "train loss:0.002671261433969522\n",
      "train loss:0.010521038715679177\n",
      "train loss:0.003053987810579576\n",
      "train loss:0.004642821131232539\n",
      "train loss:0.0007405243870255855\n",
      "train loss:0.006495822784731442\n",
      "train loss:0.0024921189468912978\n",
      "train loss:0.0014056849901970157\n",
      "train loss:0.00047268653227267276\n",
      "train loss:0.013492084427435276\n",
      "train loss:0.0008409402849648482\n",
      "train loss:0.008339014796535683\n",
      "train loss:0.024311633511601004\n",
      "train loss:0.001800285339502516\n",
      "train loss:0.010564962584898946\n",
      "train loss:0.0024999569454879425\n",
      "train loss:0.00477477382693703\n",
      "train loss:0.00018291191357708934\n",
      "train loss:0.0007089964368439532\n",
      "train loss:0.0008041287737307573\n",
      "train loss:0.0023768938932719644\n",
      "train loss:0.0018251439500688417\n",
      "train loss:0.0005516450985247268\n",
      "train loss:0.002807209014396013\n",
      "train loss:0.006658828579187776\n",
      "train loss:0.0005112729963461881\n",
      "train loss:0.0016280287810282567\n",
      "train loss:0.0021426464293839388\n",
      "train loss:0.015186044352327248\n",
      "train loss:0.025586145869110028\n",
      "train loss:0.010705907794709101\n",
      "train loss:0.002246313318291515\n",
      "train loss:0.001773066899976305\n",
      "train loss:0.00161866332016405\n",
      "train loss:0.007905553115942245\n",
      "train loss:0.0037314638698479844\n",
      "train loss:0.0012143116061278584\n",
      "train loss:0.0007855343088705825\n",
      "train loss:0.001721463622247224\n",
      "train loss:0.004904106845329144\n",
      "train loss:0.008132287297744319\n",
      "train loss:0.0009020953914650375\n",
      "train loss:0.027814963328293904\n",
      "train loss:0.0016462761824066418\n",
      "train loss:0.002565761193105216\n",
      "train loss:0.006533035798410717\n",
      "train loss:0.0020272490267267324\n",
      "train loss:0.009596091934339386\n",
      "train loss:0.00048509657797812626\n",
      "train loss:0.011610326101058837\n",
      "train loss:0.0026651550863693446\n",
      "train loss:0.0008339932949652441\n",
      "train loss:0.0007863038740512122\n",
      "train loss:0.0020655737141871086\n",
      "train loss:0.007880925794263752\n",
      "train loss:0.0014175786970097605\n",
      "train loss:0.002823704382765891\n",
      "train loss:0.007588117643583746\n",
      "train loss:0.012781135978975594\n",
      "train loss:0.0090065790920097\n",
      "train loss:0.01285507491704168\n",
      "train loss:0.0032870759052268284\n",
      "train loss:0.01662874582381302\n",
      "train loss:0.007109656258133224\n",
      "train loss:0.005324487978945891\n",
      "train loss:0.0044101823847772755\n",
      "train loss:0.0062638905363482066\n",
      "train loss:0.0018525542702905096\n",
      "train loss:0.03345313020677491\n",
      "train loss:0.0010967387487978433\n",
      "train loss:0.0018839619431782655\n",
      "train loss:0.0022178264909160942\n",
      "train loss:0.006104823758702555\n",
      "train loss:0.010488869425929935\n",
      "train loss:0.004806699857320263\n",
      "train loss:0.022238734084918853\n",
      "train loss:0.00953007570122676\n",
      "train loss:0.015577048524595365\n",
      "train loss:0.024511250089703052\n",
      "train loss:0.006252004838861457\n",
      "train loss:0.0032271014877542146\n",
      "train loss:0.011365912758643083\n",
      "train loss:0.015390010386776922\n",
      "train loss:0.01605939644996031\n",
      "train loss:0.004126937252419058\n",
      "train loss:0.00331142965013149\n",
      "train loss:0.01317078058019063\n",
      "train loss:0.0043162496943369945\n",
      "train loss:0.00685327732711434\n",
      "train loss:0.009716518315927514\n",
      "train loss:0.0040020433771944955\n",
      "train loss:0.02862483025969028\n",
      "train loss:0.0028269824535723767\n",
      "train loss:0.004037293946598142\n",
      "train loss:0.0015698379714598777\n",
      "train loss:0.002416805018577743\n",
      "train loss:0.001183125691446242\n",
      "train loss:0.018312863554619533\n",
      "train loss:0.0022512740724223647\n",
      "train loss:0.004728514617988181\n",
      "train loss:0.010972368456831106\n",
      "train loss:0.019437113484468842\n",
      "train loss:0.014790192255406617\n",
      "train loss:0.0037787072631442872\n",
      "train loss:0.005407790664710874\n",
      "train loss:0.002128989077494987\n",
      "train loss:0.01219735348031575\n",
      "train loss:0.01306676686346112\n",
      "train loss:0.009654896100175558\n",
      "train loss:0.0014316335190248733\n",
      "train loss:0.016432120032661885\n",
      "train loss:0.004077910992230683\n",
      "train loss:0.000885478273459851\n",
      "train loss:0.002507338873570142\n",
      "train loss:0.00042993726740432094\n",
      "train loss:0.006701918231909089\n",
      "train loss:0.0028544718799204177\n",
      "train loss:0.006499520621796471\n",
      "train loss:0.0010800024704946357\n",
      "train loss:0.013352285179877573\n",
      "train loss:0.006224889853999085\n",
      "train loss:0.007632917137738042\n",
      "train loss:0.0008813946645949333\n",
      "train loss:0.00376039626797995\n",
      "train loss:0.001223809186614144\n",
      "train loss:0.053803746613494144\n",
      "train loss:0.01369219260069194\n",
      "train loss:0.0007017261380678637\n",
      "train loss:0.006434236111555802\n",
      "train loss:0.0011537327084409602\n",
      "train loss:0.019907194248427542\n",
      "train loss:0.0033560573950401954\n",
      "train loss:0.006244283897023193\n",
      "train loss:0.002967695730661358\n",
      "train loss:0.004600010103057083\n",
      "train loss:0.003639827062656001\n",
      "train loss:0.003179956997316133\n",
      "train loss:0.0047475758997358114\n",
      "train loss:0.0009571631234196718\n",
      "train loss:0.010797868913220708\n",
      "train loss:0.008173961545901146\n",
      "train loss:0.015166594669265152\n",
      "train loss:0.006160669197826394\n",
      "train loss:0.01779445405055023\n",
      "train loss:0.003935807862964587\n",
      "train loss:0.0018475932375526323\n",
      "train loss:0.0007484730855355979\n",
      "train loss:0.0036591216369031843\n",
      "train loss:0.023337017569952344\n",
      "train loss:0.001573965791426876\n",
      "train loss:0.004157899502067497\n",
      "train loss:0.0005068400776958228\n",
      "train loss:0.005129087211483639\n",
      "train loss:0.0040824588924259575\n",
      "train loss:0.00673089926955515\n",
      "train loss:0.0005169900302804881\n",
      "train loss:0.0024791507737379646\n",
      "train loss:0.0006836715630460807\n",
      "train loss:0.025046861357452804\n",
      "train loss:0.0028656485366136937\n",
      "train loss:0.0020598013962041897\n",
      "train loss:0.004616172346633014\n",
      "train loss:0.0026402073778457095\n",
      "train loss:0.0018753650638613582\n",
      "train loss:0.026951149324195765\n",
      "train loss:0.009615005991998271\n",
      "train loss:0.007242224385673078\n",
      "train loss:0.012942069391961526\n",
      "train loss:0.018154015641701098\n",
      "train loss:0.00230560968632513\n",
      "train loss:0.01059494349065259\n",
      "train loss:0.006231315886142563\n",
      "train loss:0.012731410648432615\n",
      "train loss:0.002086464388584656\n",
      "train loss:0.003281220967789304\n",
      "train loss:0.006590607538601203\n",
      "train loss:0.003488660748796322\n",
      "train loss:0.0006361301125171691\n",
      "train loss:0.00303127735622378\n",
      "train loss:0.00746957146346205\n",
      "train loss:0.004063885669303682\n",
      "train loss:0.006608569445478007\n",
      "train loss:0.01850019047416813\n",
      "train loss:0.006244233692053989\n",
      "train loss:0.003086815295483253\n",
      "train loss:0.0009539032167527102\n",
      "train loss:0.0061126590223074175\n",
      "train loss:0.006375283060831281\n",
      "train loss:0.004767020250105031\n",
      "train loss:0.015072373924631063\n",
      "train loss:0.004113127606909236\n",
      "train loss:0.008553538994415097\n",
      "train loss:0.004443451432576841\n",
      "train loss:0.002001407171070478\n",
      "train loss:0.004382826577610535\n",
      "train loss:0.01095001070689859\n",
      "train loss:0.005240298865114454\n",
      "train loss:0.005318438636995731\n",
      "train loss:0.007879844066474798\n",
      "train loss:0.0029651114514378366\n",
      "train loss:0.005675303165401802\n",
      "train loss:0.007854796611197212\n",
      "train loss:0.005770732423124433\n",
      "train loss:0.0025197032711091537\n",
      "train loss:0.0006789359314217038\n",
      "train loss:0.009691177627924718\n",
      "train loss:0.012255702235503667\n",
      "train loss:0.0014936001571450193\n",
      "train loss:0.00758404234608584\n",
      "train loss:0.006492488646369139\n",
      "train loss:0.002021913757722233\n",
      "train loss:0.0013261281708694334\n",
      "train loss:0.003857134163934178\n",
      "train loss:0.0013082544415363958\n",
      "train loss:0.0597441657251915\n",
      "train loss:0.018200425080334814\n",
      "train loss:0.004227750407006475\n",
      "train loss:0.012507902896796754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0031397399044630994\n",
      "train loss:0.0014750912005172501\n",
      "train loss:0.006365488505052104\n",
      "train loss:0.002369221802051584\n",
      "train loss:0.011943655380875632\n",
      "train loss:0.005875815160565834\n",
      "train loss:0.0015850710635690907\n",
      "train loss:0.0027450379943491027\n",
      "train loss:0.0021526574789500225\n",
      "train loss:0.0015135059575982835\n",
      "train loss:0.0006314435812814266\n",
      "train loss:0.0013653751456301267\n",
      "train loss:0.005029486124882013\n",
      "train loss:0.002522793022598493\n",
      "train loss:0.006228244270034554\n",
      "train loss:0.0017689542562202717\n",
      "=== epoch:16, train acc:0.9977807971014493, test acc:0.9876811594202899 ===\n",
      "train loss:0.0108717722800357\n",
      "train loss:0.007748296701168213\n",
      "train loss:0.002430341616942371\n",
      "train loss:0.0039885858309043655\n",
      "train loss:0.0007475885555144416\n",
      "train loss:0.004799201974630604\n",
      "train loss:0.002319576341717643\n",
      "train loss:0.001263690131883518\n",
      "train loss:0.0028543027283038655\n",
      "train loss:0.01411245191317932\n",
      "train loss:0.0016982010431056927\n",
      "train loss:0.003980197110396179\n",
      "train loss:0.002677984688484258\n",
      "train loss:0.010175596740183097\n",
      "train loss:0.006329422611116331\n",
      "train loss:0.004885130980461564\n",
      "train loss:0.004383173627183867\n",
      "train loss:0.0021603498804467856\n",
      "train loss:0.012434065676430734\n",
      "train loss:0.0011574271563750205\n",
      "train loss:0.0013068396980436372\n",
      "train loss:0.0018959958128237535\n",
      "train loss:0.005170019760740052\n",
      "train loss:0.0020271741447897935\n",
      "train loss:0.024078874084715644\n",
      "train loss:0.036235364515782145\n",
      "train loss:0.002329942432889008\n",
      "train loss:0.000834208909243827\n",
      "train loss:0.01728211069447576\n",
      "train loss:0.007576086192777425\n",
      "train loss:0.004347245771351533\n",
      "train loss:0.011079841277642532\n",
      "train loss:0.001805080615946495\n",
      "train loss:0.00547820650016879\n",
      "train loss:0.0026622567195618103\n",
      "train loss:0.0027973246910559016\n",
      "train loss:0.019455514902446767\n",
      "train loss:0.0058151957475672846\n",
      "train loss:0.0034070829545520135\n",
      "train loss:0.026972104765038536\n",
      "train loss:0.04189503785552566\n",
      "train loss:0.0066614847159571396\n",
      "train loss:0.004127372460326414\n",
      "train loss:0.002638440592773088\n",
      "train loss:0.0011248708324867076\n",
      "train loss:0.003467242426372763\n",
      "train loss:0.001018006153326777\n",
      "train loss:0.0029173861217794096\n",
      "train loss:0.0025423024424029664\n",
      "train loss:0.012658130770530239\n",
      "train loss:0.004934628439513094\n",
      "train loss:0.001355565575088448\n",
      "train loss:0.004529532656266823\n",
      "train loss:0.012412978382429213\n",
      "train loss:0.0029732027357374053\n",
      "train loss:0.002201284421075504\n",
      "train loss:0.03261635571077644\n",
      "train loss:0.010772731028853585\n",
      "train loss:0.002075533463851254\n",
      "train loss:0.010346908462028145\n",
      "train loss:0.00957676987470678\n",
      "train loss:0.008136068815102775\n",
      "train loss:0.006859519789220007\n",
      "train loss:0.0028257075193644935\n",
      "train loss:0.005778193394887676\n",
      "train loss:0.04571199546878656\n",
      "train loss:0.00633619252853373\n",
      "train loss:0.006640453495157398\n",
      "train loss:0.006977748627512618\n",
      "train loss:0.0035612966183183733\n",
      "train loss:0.008180357359330056\n",
      "train loss:0.019290072363533764\n",
      "train loss:0.006373383107162166\n",
      "train loss:0.028777994665354054\n",
      "train loss:0.007230555676723855\n",
      "train loss:0.008840765079326794\n",
      "train loss:0.007208285526693883\n",
      "train loss:0.0043473302624556405\n",
      "train loss:0.011716019511431528\n",
      "train loss:0.006462989665384695\n",
      "train loss:0.002474188662592805\n",
      "train loss:0.015671892192135904\n",
      "train loss:0.04038642926959568\n",
      "train loss:0.004362716724560111\n",
      "train loss:0.007178326450698471\n",
      "train loss:0.012164185653951476\n",
      "train loss:0.00907466151546088\n",
      "train loss:0.0064508418984217785\n",
      "train loss:0.0036534342389133324\n",
      "train loss:0.0015185802452532472\n",
      "train loss:0.001928321014863739\n",
      "train loss:0.0031784094842307988\n",
      "train loss:0.00740617907398165\n",
      "train loss:0.007630076298517001\n",
      "train loss:0.017138979257463372\n",
      "train loss:0.0028583429388331417\n",
      "train loss:0.007970056763498498\n",
      "train loss:0.0038463556873941968\n",
      "train loss:0.0012272496075010182\n",
      "train loss:0.024418275189382825\n",
      "train loss:0.0009128478803713366\n",
      "train loss:0.011984142449544443\n",
      "train loss:0.0024138814633835\n",
      "train loss:0.00047469522649959173\n",
      "train loss:0.014000755890294873\n",
      "train loss:0.001753733642663025\n",
      "train loss:0.014845532977342972\n",
      "train loss:0.002425133460873558\n",
      "train loss:0.0023291396299706863\n",
      "train loss:0.0012666930149092088\n",
      "train loss:0.00883061954311235\n",
      "train loss:0.008620021776437871\n",
      "train loss:0.0025755981460559\n",
      "train loss:0.00163855578918486\n",
      "train loss:0.001925235886941911\n",
      "train loss:0.0017917396563661397\n",
      "train loss:0.002156610957202143\n",
      "train loss:0.009122729725596522\n",
      "train loss:0.00044143042488548485\n",
      "train loss:0.00590341202429746\n",
      "train loss:0.0013632449646631395\n",
      "train loss:0.003629922641730251\n",
      "train loss:0.0007250984554723044\n",
      "train loss:0.0002799688641605871\n",
      "train loss:0.002323762226154967\n",
      "train loss:0.029948465764805084\n",
      "train loss:0.002072695000025074\n",
      "train loss:0.024019576510389797\n",
      "train loss:0.0027562090871889825\n",
      "train loss:0.0041920483231028816\n",
      "train loss:0.0006626545042114804\n",
      "train loss:0.0019305054857661694\n",
      "train loss:0.0009927416794883554\n",
      "train loss:0.0031666088845631536\n",
      "train loss:0.0014690852815833603\n",
      "train loss:0.0005349340696550428\n",
      "train loss:0.018878962800029143\n",
      "train loss:0.0011768534792786925\n",
      "train loss:0.0038558228276991497\n",
      "train loss:0.012314416622582907\n",
      "train loss:0.0020678866892273108\n",
      "train loss:0.0006648799607332211\n",
      "train loss:0.008823517556456991\n",
      "train loss:0.0039734427810695395\n",
      "train loss:0.008133573360764595\n",
      "train loss:0.002080699584898606\n",
      "train loss:0.0027558852035353245\n",
      "train loss:0.0004735077554124575\n",
      "train loss:0.004171881181291942\n",
      "train loss:0.00424192976807938\n",
      "train loss:0.038265985751216994\n",
      "train loss:0.04340383803386415\n",
      "train loss:0.004979774271498219\n",
      "train loss:0.005977793340540923\n",
      "train loss:0.0024667850874736134\n",
      "train loss:0.015223088400628962\n",
      "train loss:0.005236771928801273\n",
      "train loss:0.014861051689253835\n",
      "train loss:0.0033770841126533183\n",
      "train loss:0.013794212954843304\n",
      "train loss:0.055998690228477484\n",
      "train loss:0.005200118759093496\n",
      "train loss:0.0013719550351499272\n",
      "train loss:0.0037367918561542987\n",
      "train loss:0.00416751833625171\n",
      "train loss:0.002920524984640818\n",
      "train loss:0.005897317741685834\n",
      "train loss:0.0069241861553729165\n",
      "train loss:0.0031313572374154853\n",
      "train loss:0.0021331179685936184\n",
      "train loss:0.003566797007296114\n",
      "train loss:0.0026853304504587234\n",
      "train loss:0.003881906113790541\n",
      "train loss:0.0027655733010393305\n",
      "train loss:0.007179308153924241\n",
      "train loss:0.001505567813648582\n",
      "train loss:0.004467251106441752\n",
      "train loss:0.0007754963959902981\n",
      "train loss:0.001753129071577442\n",
      "train loss:0.00527631437626173\n",
      "train loss:0.0013782172311711796\n",
      "train loss:0.009223372024366476\n",
      "train loss:0.003596704609766023\n",
      "train loss:0.018698902258389813\n",
      "train loss:0.001705930107703099\n",
      "train loss:0.0024313485771985536\n",
      "train loss:0.010721084839873253\n",
      "train loss:0.0057516050966320365\n",
      "train loss:0.003922481661152136\n",
      "train loss:0.002679806939600374\n",
      "train loss:0.0094805533188945\n",
      "train loss:0.04652907843626382\n",
      "train loss:0.003346191308440314\n",
      "train loss:0.0030998615696846282\n",
      "train loss:0.0029690825000313345\n",
      "train loss:0.0026084456933291003\n",
      "train loss:0.0022047832619056253\n",
      "train loss:0.0024675120590191375\n",
      "train loss:0.0006637872237834818\n",
      "train loss:0.001506554662262769\n",
      "train loss:0.0022215712815350118\n",
      "train loss:0.017630550000910232\n",
      "train loss:0.0023859066979026763\n",
      "train loss:0.0035474646631410544\n",
      "train loss:0.01847541731898562\n",
      "train loss:0.0033877634066997054\n",
      "train loss:0.0019002313179349107\n",
      "train loss:0.004871988008238749\n",
      "train loss:0.005714698592755997\n",
      "train loss:0.013053689143932436\n",
      "train loss:0.0013834562431754142\n",
      "train loss:0.0048403792314785496\n",
      "train loss:0.04480885442704225\n",
      "train loss:0.0023717324920504365\n",
      "train loss:0.01013675109887576\n",
      "train loss:0.004841323661003197\n",
      "train loss:0.003161330045915516\n",
      "train loss:0.009919363520141122\n",
      "train loss:0.0007060383869493913\n",
      "train loss:0.006583763429381612\n",
      "train loss:0.006328024221090892\n",
      "train loss:0.008541393811076861\n",
      "train loss:0.002817203029177044\n",
      "train loss:0.006208231780497268\n",
      "train loss:0.0038048120115676265\n",
      "train loss:0.008866565757281964\n",
      "train loss:0.0041180791598482\n",
      "train loss:0.0017093955584344225\n",
      "train loss:0.00259611320333315\n",
      "train loss:0.006107698692486944\n",
      "train loss:0.003943347653667577\n",
      "train loss:0.015355185145878416\n",
      "train loss:0.0014506591005690004\n",
      "train loss:0.00696757782828219\n",
      "train loss:0.008611042315828423\n",
      "train loss:0.015706596577101408\n",
      "train loss:0.0005447087293859954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.004720975632228374\n",
      "train loss:0.0024956586346856695\n",
      "train loss:0.0071826940362821\n",
      "train loss:0.0023418111689334636\n",
      "train loss:0.0006175600034606651\n",
      "train loss:0.007669420366053721\n",
      "train loss:0.001156153402548722\n",
      "train loss:0.001959435282618308\n",
      "train loss:0.0015171736400528782\n",
      "train loss:0.013056329657165123\n",
      "train loss:0.005135386638468071\n",
      "train loss:0.0039641711531439505\n",
      "train loss:0.0017454897609311029\n",
      "train loss:0.0001679707925117613\n",
      "train loss:0.0016736382495385602\n",
      "train loss:0.001036698429126184\n",
      "train loss:0.0015173439887853624\n",
      "train loss:0.0002360934581397534\n",
      "train loss:0.005958007926791647\n",
      "train loss:0.00048016279055045736\n",
      "train loss:0.004117986844464475\n",
      "train loss:0.0022612562069143378\n",
      "train loss:0.003849373385620147\n",
      "train loss:0.008384894741852706\n",
      "train loss:0.0033973398397617077\n",
      "train loss:0.001031755088014365\n",
      "train loss:0.046054784998543104\n",
      "train loss:0.0006360850455110397\n",
      "train loss:0.0021569393257261623\n",
      "train loss:0.006919898219905696\n",
      "train loss:0.0009466746714560758\n",
      "train loss:0.011258031818479237\n",
      "train loss:0.001019420377679169\n",
      "train loss:0.002202507162027468\n",
      "train loss:0.014604987436111846\n",
      "train loss:0.0020822808744599933\n",
      "train loss:0.0019343971109230503\n",
      "train loss:0.002542265928379638\n",
      "train loss:0.0018615441624929706\n",
      "train loss:0.006577065246936946\n",
      "train loss:0.014356466360039594\n",
      "train loss:0.001443304572350479\n",
      "train loss:0.0011111676442304157\n",
      "train loss:0.006345433059684975\n",
      "train loss:0.016020430236031512\n",
      "train loss:0.018258153805256947\n",
      "train loss:0.018534890229829726\n",
      "train loss:0.0014447225518210165\n",
      "train loss:0.0041965251791414755\n",
      "train loss:0.0003369977755995615\n",
      "train loss:0.003299422248367779\n",
      "train loss:0.00040702257864523697\n",
      "train loss:0.02454192575566272\n",
      "train loss:0.009471303416176718\n",
      "train loss:0.0008029613416890605\n",
      "train loss:0.003752943264483739\n",
      "train loss:0.0009531680397823071\n",
      "train loss:0.003529664903567543\n",
      "train loss:0.0023325856750929124\n",
      "train loss:0.001976844816848146\n",
      "train loss:0.009094472167729899\n",
      "train loss:0.006272944006267363\n",
      "train loss:0.0007635290732951444\n",
      "train loss:0.008225965542160383\n",
      "train loss:0.00410590975939354\n",
      "train loss:0.0017125095372747593\n",
      "train loss:0.0011971506069428466\n",
      "train loss:0.000937039178689604\n",
      "train loss:0.00509301423020487\n",
      "train loss:0.007784936789213235\n",
      "train loss:0.009724393010993182\n",
      "train loss:0.005496439912654397\n",
      "train loss:0.00290451131024978\n",
      "train loss:0.007268927429615605\n",
      "train loss:0.011087037304714331\n",
      "train loss:0.0027745048948353565\n",
      "train loss:0.0018271287863176103\n",
      "train loss:0.0025301426984301136\n",
      "train loss:0.0056579006273579\n",
      "train loss:0.002688355867614273\n",
      "train loss:0.0006696419476462071\n",
      "train loss:0.027429611869098557\n",
      "train loss:0.007146029407548936\n",
      "train loss:0.003116460715873732\n",
      "train loss:0.0023710790290418326\n",
      "train loss:0.004631990992691388\n",
      "train loss:0.016378067443738046\n",
      "train loss:0.009750346637101249\n",
      "train loss:0.001315720079571396\n",
      "train loss:0.0008934613502067962\n",
      "train loss:0.004963196126736246\n",
      "train loss:0.020630698020306572\n",
      "train loss:0.0020965151641489503\n",
      "train loss:0.0022383841572848687\n",
      "train loss:0.006689181472424227\n",
      "train loss:0.003140756396099553\n",
      "train loss:0.0053756428623637185\n",
      "train loss:0.0010776937233020459\n",
      "train loss:0.0036045965178174433\n",
      "train loss:0.014996903236919826\n",
      "train loss:0.015409723162603058\n",
      "train loss:0.018101801080270947\n",
      "train loss:0.0022491863130550778\n",
      "train loss:0.0008261271207832545\n",
      "train loss:0.002351315812004348\n",
      "train loss:0.03020346183087793\n",
      "train loss:0.004215989581099533\n",
      "train loss:0.001011420675100307\n",
      "train loss:0.0006954944755799723\n",
      "train loss:0.0009262366483277403\n",
      "train loss:0.0006358292488491785\n",
      "train loss:0.0036666314896238194\n",
      "train loss:0.0023510397230371077\n",
      "train loss:0.005493042454492341\n",
      "train loss:0.00951874183086488\n",
      "train loss:0.006193870202196648\n",
      "train loss:0.0009004170614002859\n",
      "train loss:0.005992721123153523\n",
      "train loss:0.003038570890913943\n",
      "train loss:0.010851983001430853\n",
      "train loss:0.0011624156082357554\n",
      "train loss:0.0014710728024633012\n",
      "train loss:0.0016813679235582674\n",
      "train loss:0.002001041452862531\n",
      "train loss:0.01121532110878896\n",
      "train loss:0.004594177159837756\n",
      "train loss:0.0051333241139357\n",
      "train loss:0.037440743868651045\n",
      "train loss:0.0010588411165736006\n",
      "train loss:0.015470728771976099\n",
      "train loss:0.0012124977725625158\n",
      "=== epoch:17, train acc:0.9968523550724637, test acc:0.9875 ===\n",
      "train loss:0.011473109889101564\n",
      "train loss:0.0007395680420904444\n",
      "train loss:0.004009851423035743\n",
      "train loss:0.008267630300131392\n",
      "train loss:0.0008014482543565273\n",
      "train loss:0.0012062484477395146\n",
      "train loss:0.010684937340168168\n",
      "train loss:0.006655380455644276\n",
      "train loss:0.007096375206186425\n",
      "train loss:0.02748158400051507\n",
      "train loss:0.007957621875523438\n",
      "train loss:0.009287162862588\n",
      "train loss:0.0007906110277984266\n",
      "train loss:0.002331753175958658\n",
      "train loss:0.0026770613627051395\n",
      "train loss:0.001835167643681305\n",
      "train loss:0.004187418787521443\n",
      "train loss:0.0006864148500986836\n",
      "train loss:0.005803110553558258\n",
      "train loss:0.003080532268656843\n",
      "train loss:0.003048510116383349\n",
      "train loss:0.00046124672379120455\n",
      "train loss:0.004338699091383708\n",
      "train loss:0.0006599258068702155\n",
      "train loss:0.005295796356173441\n",
      "train loss:0.02123068284722601\n",
      "train loss:0.003688786551911636\n",
      "train loss:0.0012491581374316667\n",
      "train loss:0.01628856598878425\n",
      "train loss:0.0028810248906300587\n",
      "train loss:0.025875465155652748\n",
      "train loss:0.003821658594642973\n",
      "train loss:0.0019098840173424406\n",
      "train loss:0.005546035338606638\n",
      "train loss:0.004170132203155453\n",
      "train loss:0.0024193987888490682\n",
      "train loss:0.011597871442082542\n",
      "train loss:0.0099611658868925\n",
      "train loss:0.01959241041810976\n",
      "train loss:0.003459859042741087\n",
      "train loss:0.003315529807512771\n",
      "train loss:0.0008600105404520964\n",
      "train loss:0.012235192110043643\n",
      "train loss:0.02140472999306691\n",
      "train loss:0.0018602606878825212\n",
      "train loss:0.0011908419661937053\n",
      "train loss:0.008310258649225442\n",
      "train loss:0.001888929271134998\n",
      "train loss:0.009069966392808505\n",
      "train loss:0.018910300947866394\n",
      "train loss:0.003522479484305119\n",
      "train loss:0.0010132358624903943\n",
      "train loss:0.0010897455139151993\n",
      "train loss:0.0023958544435058677\n",
      "train loss:0.0015604559777456764\n",
      "train loss:0.005262428796933046\n",
      "train loss:0.0041285416525625495\n",
      "train loss:0.0008021927392023958\n",
      "train loss:0.004493115253251815\n",
      "train loss:0.0034318261287808157\n",
      "train loss:0.02645345805177796\n",
      "train loss:0.002753212965604326\n",
      "train loss:0.01403438319619357\n",
      "train loss:0.007057659311973699\n",
      "train loss:0.0021895645480272956\n",
      "train loss:0.0014388835008106399\n",
      "train loss:0.0038103345993327482\n",
      "train loss:0.008546538755667642\n",
      "train loss:0.007172581448410639\n",
      "train loss:0.015084819642674763\n",
      "train loss:0.006352064624619275\n",
      "train loss:0.013315310611571282\n",
      "train loss:0.0035642403471107856\n",
      "train loss:0.004177027703405892\n",
      "train loss:0.010302971935317344\n",
      "train loss:0.0009314569969820082\n",
      "train loss:0.004520783009421035\n",
      "train loss:0.012656627831038733\n",
      "train loss:0.020276809705164933\n",
      "train loss:0.005747379351818256\n",
      "train loss:0.002669967252286197\n",
      "train loss:0.002879085140436407\n",
      "train loss:0.000632975931770155\n",
      "train loss:0.006467422281503749\n",
      "train loss:0.033073253782262564\n",
      "train loss:0.04110055565569794\n",
      "train loss:0.002217958296847493\n",
      "train loss:0.0015067368525039395\n",
      "train loss:0.00851888168735839\n",
      "train loss:0.0072227332904058915\n",
      "train loss:0.009152171400197444\n",
      "train loss:0.006333341400991651\n",
      "train loss:0.009899800935075028\n",
      "train loss:0.005888484100717852\n",
      "train loss:0.020146348398008187\n",
      "train loss:0.0065547105438251195\n",
      "train loss:0.013389655259475402\n",
      "train loss:0.0015300718759163972\n",
      "train loss:0.015573054457576674\n",
      "train loss:0.0036704890496360807\n",
      "train loss:0.012812333280089771\n",
      "train loss:0.0023583858328318566\n",
      "train loss:0.006084072478399287\n",
      "train loss:0.024643864602191534\n",
      "train loss:0.01520332477750371\n",
      "train loss:0.009731637877784778\n",
      "train loss:0.00518213082658952\n",
      "train loss:0.005965398795350549\n",
      "train loss:0.005130091609903428\n",
      "train loss:0.014242762406683115\n",
      "train loss:0.002266367780276052\n",
      "train loss:0.016136245114902803\n",
      "train loss:0.04897536896451699\n",
      "train loss:0.0036116749182708557\n",
      "train loss:0.00672156798656181\n",
      "train loss:0.004208714658833279\n",
      "train loss:0.005449135233638784\n",
      "train loss:0.004877627192217556\n",
      "train loss:0.02439590772705843\n",
      "train loss:0.006382048314573793\n",
      "train loss:0.0023168081732142117\n",
      "train loss:0.006425839404953516\n",
      "train loss:0.00844820133953682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.015638643307864138\n",
      "train loss:0.013260641640690877\n",
      "train loss:0.03326320560109845\n",
      "train loss:0.0025868866375718795\n",
      "train loss:0.00657243449417022\n",
      "train loss:0.002451306934006643\n",
      "train loss:0.006165242440576\n",
      "train loss:0.0030786951636601096\n",
      "train loss:0.005322127868729454\n",
      "train loss:0.0109022208096762\n",
      "train loss:0.00129152715842839\n",
      "train loss:0.0034937810363184314\n",
      "train loss:0.0028846160126075504\n",
      "train loss:0.0024996968590229717\n",
      "train loss:0.033755183225742065\n",
      "train loss:0.008620394894399036\n",
      "train loss:0.0014930348070318002\n",
      "train loss:0.004061580328253626\n",
      "train loss:0.0007990357763526937\n",
      "train loss:0.001692285668159425\n",
      "train loss:0.0022247321237942002\n",
      "train loss:0.006534114004231676\n",
      "train loss:0.011882711735271246\n",
      "train loss:0.025078349670564683\n",
      "train loss:0.0026367036244829846\n",
      "train loss:0.009960078384859636\n",
      "train loss:0.0032507877051465577\n",
      "train loss:0.008845968925052702\n",
      "train loss:0.0034442479178360143\n",
      "train loss:0.001033346740883829\n",
      "train loss:0.005661250866586905\n",
      "train loss:0.02761215908729721\n",
      "train loss:0.021302660780913942\n",
      "train loss:0.0007706614870291573\n",
      "train loss:0.0020561866567631653\n",
      "train loss:0.0012380891326268175\n",
      "train loss:0.0019877246000028137\n",
      "train loss:0.0007808820062370039\n",
      "train loss:0.004440574220165328\n",
      "train loss:0.0021302767855786724\n",
      "train loss:0.002353608000309912\n",
      "train loss:0.02164885880589174\n",
      "train loss:0.004437187489894185\n",
      "train loss:0.000890303686751649\n",
      "train loss:0.0018144405963706552\n",
      "train loss:0.016842044018964258\n",
      "train loss:0.006786555074835504\n",
      "train loss:0.0011797065682702184\n",
      "train loss:0.0028620072351958127\n",
      "train loss:0.005422548766098228\n",
      "train loss:0.0044950663321516975\n",
      "train loss:0.0028948404747342793\n",
      "train loss:0.004359666863594601\n",
      "train loss:0.005351021368470584\n",
      "train loss:0.0019114335054493852\n",
      "train loss:0.012306404528271912\n",
      "train loss:0.006500476448772183\n",
      "train loss:0.03175929197365439\n",
      "train loss:0.004275971179287519\n",
      "train loss:0.012189920532997027\n",
      "train loss:0.011979118110509812\n",
      "train loss:0.004590240910545642\n",
      "train loss:0.0007638415578749501\n",
      "train loss:0.005041114600156956\n",
      "train loss:0.004356325902985555\n",
      "train loss:0.0015459347106369914\n",
      "train loss:0.004056548686255457\n",
      "train loss:0.0020661393804520876\n",
      "train loss:0.0029908580578998168\n",
      "train loss:0.001233174384957576\n",
      "train loss:0.005140570600169481\n",
      "train loss:0.007555570764721681\n",
      "train loss:0.008346476567468649\n",
      "train loss:0.0004339582278860055\n",
      "train loss:0.003928409181454908\n",
      "train loss:0.0010379671454663077\n",
      "train loss:0.004633321287925393\n",
      "train loss:0.0012361141375323455\n",
      "train loss:0.009244896690267967\n",
      "train loss:0.0009277707367872873\n",
      "train loss:0.0018673717773441151\n",
      "train loss:0.0051481309742008345\n",
      "train loss:0.0019543824973423203\n",
      "train loss:0.0030170162557658013\n",
      "train loss:0.005881179165836799\n",
      "train loss:0.001546626814453711\n",
      "train loss:0.0019128640217806446\n",
      "train loss:0.00583563558757509\n",
      "train loss:0.0031409661416776625\n",
      "train loss:0.0009454112174055317\n",
      "train loss:0.009572864863240941\n",
      "train loss:0.004699154850221519\n",
      "train loss:0.0017380992428047974\n",
      "train loss:0.005001944843532602\n",
      "train loss:0.008119275722434314\n",
      "train loss:0.01223749565878984\n",
      "train loss:0.0030833117755257543\n",
      "train loss:0.004017402718853722\n",
      "train loss:0.0055383916890792305\n",
      "train loss:0.002857856382585474\n",
      "train loss:0.011084288817509308\n",
      "train loss:0.0006958561328323868\n",
      "train loss:0.008726067100283787\n",
      "train loss:0.03491079811084377\n",
      "train loss:0.002478058348614905\n",
      "train loss:0.002147426264519658\n",
      "train loss:0.0019557004461278377\n",
      "train loss:0.022486498998542062\n",
      "train loss:0.00852653962259745\n",
      "train loss:0.017809306825939956\n",
      "train loss:0.0199460459580132\n",
      "train loss:0.008111281570547744\n",
      "train loss:0.008143813694584322\n",
      "train loss:0.04924784681375611\n",
      "train loss:0.005239887620469039\n",
      "train loss:0.0010622179616530342\n",
      "train loss:0.007229907540373074\n",
      "train loss:0.005621109147203875\n",
      "train loss:0.0016112081847457415\n",
      "train loss:0.0036377091695008346\n",
      "train loss:0.008744487389186062\n",
      "train loss:0.002184838276080694\n",
      "train loss:0.0010283927763506384\n",
      "train loss:0.0016528192627855974\n",
      "train loss:0.0034402151055493314\n",
      "train loss:0.0015994170532464735\n",
      "train loss:0.002362285001115345\n",
      "train loss:0.0013425286964311232\n",
      "train loss:0.004858029063432978\n",
      "train loss:0.0004922689693025281\n",
      "train loss:0.0006199267732935221\n",
      "train loss:0.0037890344283482776\n",
      "train loss:0.001801352790063955\n",
      "train loss:0.0026651259004552936\n",
      "train loss:0.0006073510206232468\n",
      "train loss:0.0027758897440582206\n",
      "train loss:0.00391403164887335\n",
      "train loss:0.022021981401693643\n",
      "train loss:0.017997006493535268\n",
      "train loss:0.016231311587861813\n",
      "train loss:0.008065490544980769\n",
      "train loss:0.0029231227258449716\n",
      "train loss:0.00046917395136255696\n",
      "train loss:0.0008384071890688242\n",
      "train loss:0.0022026022486401143\n",
      "train loss:0.012052862475382986\n",
      "train loss:0.010467212769060532\n",
      "train loss:0.004800540253777189\n",
      "train loss:0.01163150699402917\n",
      "train loss:0.00218937925384554\n",
      "train loss:0.0014448141995151598\n",
      "train loss:0.003530131342100124\n",
      "train loss:0.0009441899842703146\n",
      "train loss:0.006063419732496835\n",
      "train loss:0.0009857480905517478\n",
      "train loss:0.003462585386855106\n",
      "train loss:0.013817359469906233\n",
      "train loss:0.0008841915854309455\n",
      "train loss:0.0016428122661583285\n",
      "train loss:0.0027896312145704594\n",
      "train loss:0.0015069389236699064\n",
      "train loss:0.002743027111163242\n",
      "train loss:0.023439236500164565\n",
      "train loss:0.010445720521862076\n",
      "train loss:0.006739949591467116\n",
      "train loss:0.004569060157784783\n",
      "train loss:0.0014516059120870705\n",
      "train loss:0.0023906326475436725\n",
      "train loss:0.0027989380238988477\n",
      "train loss:0.0030038810494640625\n",
      "train loss:0.0028629702479610122\n",
      "train loss:0.001122753784683679\n",
      "train loss:0.001506397219006087\n",
      "train loss:0.001371677156859417\n",
      "train loss:0.0011784090205910834\n",
      "train loss:0.0005872861967694522\n",
      "train loss:0.0036040984184995332\n",
      "train loss:0.009787592788574048\n",
      "train loss:0.012966782627802962\n",
      "train loss:0.022729684100758468\n",
      "train loss:0.009940171515621408\n",
      "train loss:0.008746460364831458\n",
      "train loss:0.023111339624768287\n",
      "train loss:0.001359087647788318\n",
      "train loss:0.0029884783340857067\n",
      "train loss:0.0011821339619808027\n",
      "train loss:0.02139610543590601\n",
      "train loss:0.007612667499272284\n",
      "train loss:0.0041389833151255105\n",
      "train loss:0.0076215929760167925\n",
      "train loss:0.0022054861139758604\n",
      "train loss:0.008555329019778983\n",
      "train loss:0.04422362210415534\n",
      "train loss:0.0034161999123264292\n",
      "train loss:0.0008893006674283617\n",
      "train loss:0.0005983765013890779\n",
      "train loss:0.0016829286409300172\n",
      "train loss:0.010409526603238071\n",
      "train loss:0.0017893612569704182\n",
      "train loss:0.0010292126651746587\n",
      "train loss:0.003627847836619184\n",
      "train loss:0.018438863129622136\n",
      "train loss:0.0026759206612961073\n",
      "train loss:0.0074014612311072035\n",
      "train loss:0.003851791170736031\n",
      "train loss:0.02144244648691523\n",
      "train loss:0.0018234955707342475\n",
      "train loss:0.001695335852167117\n",
      "train loss:0.0013136948837584545\n",
      "train loss:0.0010467440017560376\n",
      "train loss:0.0025851383652848487\n",
      "train loss:0.001069415643637473\n",
      "train loss:0.0037185902046927024\n",
      "train loss:0.017008835492403847\n",
      "train loss:0.007763977159312476\n",
      "train loss:0.001817946056243249\n",
      "train loss:0.0010330548184121352\n",
      "train loss:0.006815882038708776\n",
      "train loss:0.0016869280759480062\n",
      "train loss:0.005307473616847164\n",
      "train loss:0.0003517285700212138\n",
      "train loss:0.004881105260116066\n",
      "train loss:0.004237073541418167\n",
      "train loss:0.001649102897474681\n",
      "train loss:0.0022650821732865156\n",
      "train loss:0.004302244834028094\n",
      "train loss:0.002134885303700756\n",
      "train loss:0.0010154621223792718\n",
      "train loss:0.015911784906564228\n",
      "train loss:0.0005103703911396231\n",
      "train loss:0.009458097200126005\n",
      "train loss:0.006114619572800144\n",
      "train loss:0.005485157885104472\n",
      "train loss:0.0016607992960104882\n",
      "train loss:0.005084530576211345\n",
      "train loss:0.0016953409905114943\n",
      "train loss:0.038469816707448604\n",
      "train loss:0.0066268272055270494\n",
      "train loss:0.02986314772193407\n",
      "train loss:0.005209808324501109\n",
      "train loss:0.005951915806870472\n",
      "train loss:0.0032074306611349903\n",
      "train loss:0.0064104016854148035\n",
      "train loss:0.0019560348399090223\n",
      "train loss:0.002151686508781965\n",
      "=== epoch:18, train acc:0.9978713768115942, test acc:0.9877717391304348 ===\n",
      "train loss:0.011360898296578456\n",
      "train loss:0.010919816904422702\n",
      "train loss:0.01400809456110483\n",
      "train loss:0.006062346894703061\n",
      "train loss:0.0019312691435722522\n",
      "train loss:0.0026965386063187184\n",
      "train loss:0.0057208455014632585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00115851296848709\n",
      "train loss:0.009603981542138873\n",
      "train loss:0.01635592005730756\n",
      "train loss:0.0007598619410557272\n",
      "train loss:0.0039507371916454375\n",
      "train loss:0.009255745501141471\n",
      "train loss:0.003590247815457748\n",
      "train loss:0.007465707651955725\n",
      "train loss:0.005913362593341321\n",
      "train loss:0.01658526276972657\n",
      "train loss:0.0018433324346631946\n",
      "train loss:0.010215959621005176\n",
      "train loss:0.006660143211649695\n",
      "train loss:0.007511409119026644\n",
      "train loss:0.003566580959883922\n",
      "train loss:0.0028968206183318467\n",
      "train loss:0.002753108281671579\n",
      "train loss:0.002010825065024205\n",
      "train loss:0.002197723684246609\n",
      "train loss:0.005575141757622354\n",
      "train loss:0.0039223167544660716\n",
      "train loss:0.0033702344302701153\n",
      "train loss:0.004184197182130444\n",
      "train loss:0.0008237306608550254\n",
      "train loss:0.0036766288712300177\n",
      "train loss:0.0021079780726701567\n",
      "train loss:0.01469180355995668\n",
      "train loss:0.00043606893112076855\n",
      "train loss:0.0004739031718331307\n",
      "train loss:0.0022792684613302753\n",
      "train loss:0.0007130242687400067\n",
      "train loss:0.0027083577807825343\n",
      "train loss:0.018488290927045268\n",
      "train loss:0.0018685690240152722\n",
      "train loss:0.0047226109572565496\n",
      "train loss:0.0035796124593470225\n",
      "train loss:0.0030346388266472555\n",
      "train loss:0.012063956442840798\n",
      "train loss:0.0008175590788105827\n",
      "train loss:0.005923431375491076\n",
      "train loss:0.00995494131099368\n",
      "train loss:0.0012985210350173464\n",
      "train loss:0.0064737269263897375\n",
      "train loss:0.0045690978837305\n",
      "train loss:0.002638694069957396\n",
      "train loss:0.0011111075697830236\n",
      "train loss:0.006780320371912163\n",
      "train loss:0.00038067954819049797\n",
      "train loss:0.005774909965691883\n",
      "train loss:0.0007047206581865457\n",
      "train loss:0.013215618983256685\n",
      "train loss:0.002339831071104809\n",
      "train loss:0.0009974856689649542\n",
      "train loss:0.006506957502284003\n",
      "train loss:0.010180255569792313\n",
      "train loss:0.01562508206043339\n",
      "train loss:0.005015676688373303\n",
      "train loss:0.005113051806847467\n",
      "train loss:0.006873775991252936\n",
      "train loss:0.025687841564356974\n",
      "train loss:0.023721095193745538\n",
      "train loss:0.020617669322467055\n",
      "train loss:0.000676259279986101\n",
      "train loss:0.002252876033824086\n",
      "train loss:0.0028869325763544226\n",
      "train loss:0.002001809269242592\n",
      "train loss:0.0008686725205523435\n",
      "train loss:0.001781363380262282\n",
      "train loss:0.0013594672222791231\n",
      "train loss:0.002803837157827027\n",
      "train loss:0.0008935385936024986\n",
      "train loss:0.004739842041770427\n",
      "train loss:0.002205256000172934\n",
      "train loss:0.0032283729502957386\n",
      "train loss:0.0006581103211008917\n",
      "train loss:0.00515239663274891\n",
      "train loss:0.004893931312771308\n",
      "train loss:0.014273039108184421\n",
      "train loss:0.002811869923717569\n",
      "train loss:0.0066580563104080386\n",
      "train loss:0.0016964680595047009\n",
      "train loss:0.0008603201262370822\n",
      "train loss:0.009789068652301829\n",
      "train loss:0.003421075265376972\n",
      "train loss:0.0025172747972430454\n",
      "train loss:0.003138646543748674\n",
      "train loss:0.0016011699583299386\n",
      "train loss:0.0031993625311319625\n",
      "train loss:0.011149029570507011\n",
      "train loss:0.023622227010919473\n",
      "train loss:0.006466952710156049\n",
      "train loss:0.002495989527736418\n",
      "train loss:0.0020042798524847615\n",
      "train loss:0.006444238763472202\n",
      "train loss:0.008464651750992407\n",
      "train loss:0.014027251833444302\n",
      "train loss:0.0010503007780717619\n",
      "train loss:0.026972851888175862\n",
      "train loss:0.0029789383894277864\n",
      "train loss:0.012663488545870625\n",
      "train loss:0.02740361546043273\n",
      "train loss:0.0011337418931052117\n",
      "train loss:0.0031016310583210447\n",
      "train loss:0.008708685263166102\n",
      "train loss:0.005090303013133601\n",
      "train loss:0.0009806545228452414\n",
      "train loss:0.004221506628534579\n",
      "train loss:0.01854677787401226\n",
      "train loss:0.004492206385024946\n",
      "train loss:0.0022729546035940014\n",
      "train loss:0.0049832771133201245\n",
      "train loss:0.001717895220705753\n",
      "train loss:0.014774865366118132\n",
      "train loss:0.0021052528852636503\n",
      "train loss:0.006615772153107991\n",
      "train loss:0.016688665834612367\n",
      "train loss:0.01570337514860626\n",
      "train loss:0.0181147437275518\n",
      "train loss:0.0018429166623180105\n",
      "train loss:0.0066554204739539225\n",
      "train loss:0.0007974101868932469\n",
      "train loss:0.0007465344359799791\n",
      "train loss:0.0008997641024370943\n",
      "train loss:0.0025763282025343644\n",
      "train loss:0.0020953248743133504\n",
      "train loss:0.003176786338190198\n",
      "train loss:0.0011734739942034546\n",
      "train loss:0.00145929162735585\n",
      "train loss:0.0018135045880856892\n",
      "train loss:0.002922226435637314\n",
      "train loss:0.001282316138155228\n",
      "train loss:0.0005282472606652028\n",
      "train loss:0.0003847775712981744\n",
      "train loss:0.005965256007255073\n",
      "train loss:0.0009371477316207093\n",
      "train loss:0.011135475271071239\n",
      "train loss:0.0008688063647824753\n",
      "train loss:0.000428452925844488\n",
      "train loss:0.0006942467727960037\n",
      "train loss:0.001908028425328143\n",
      "train loss:0.0010553704425228388\n",
      "train loss:0.0001476925769205249\n",
      "train loss:0.010798464536558543\n",
      "train loss:0.005924992535982693\n",
      "train loss:0.00503393988794524\n",
      "train loss:0.00032450280211852167\n",
      "train loss:0.0014741174015738437\n",
      "train loss:0.004892619430939398\n",
      "train loss:0.004010456667340225\n",
      "train loss:0.0007382643249935508\n",
      "train loss:0.0003773443166479277\n",
      "train loss:0.00048473459230408546\n",
      "train loss:0.0015523397727374778\n",
      "train loss:0.006037878435028993\n",
      "train loss:0.0003557269495445327\n",
      "train loss:0.0019489917864235603\n",
      "train loss:0.004127339037887478\n",
      "train loss:0.008074775921569891\n",
      "train loss:0.002638787084195063\n",
      "train loss:0.007916280197224597\n",
      "train loss:0.014106750174994964\n",
      "train loss:0.00047503474869544\n",
      "train loss:0.004439239890776783\n",
      "train loss:0.001431714515864256\n",
      "train loss:0.0026814601270272265\n",
      "train loss:0.0002449678014946889\n",
      "train loss:0.011524224016704852\n",
      "train loss:0.0015616050433165044\n",
      "train loss:0.004746144725962013\n",
      "train loss:0.00795329205430806\n",
      "train loss:0.008832013177103233\n",
      "train loss:0.0072615461757928905\n",
      "train loss:0.0017172326347099135\n",
      "train loss:0.008411914767856252\n",
      "train loss:0.003952643806951755\n",
      "train loss:0.0018330385163619859\n",
      "train loss:0.0018327658772362232\n",
      "train loss:0.0010804122958650284\n",
      "train loss:0.0018753164436150637\n",
      "train loss:0.0021381424564721656\n",
      "train loss:0.0083265740144008\n",
      "train loss:0.0015445225783327528\n",
      "train loss:0.004301883278156367\n",
      "train loss:0.0012190025291534776\n",
      "train loss:0.0067962053721993985\n",
      "train loss:0.0010905320455344555\n",
      "train loss:0.0042525067244725875\n",
      "train loss:0.010077490429444075\n",
      "train loss:0.001139629104095918\n",
      "train loss:0.004269548541928343\n",
      "train loss:0.015939124313173333\n",
      "train loss:0.003384036029030895\n",
      "train loss:0.0019303344984686389\n",
      "train loss:0.007027450019557682\n",
      "train loss:0.007520846878551253\n",
      "train loss:0.004414801187205595\n",
      "train loss:0.05332131776967419\n",
      "train loss:0.010611507324324946\n",
      "train loss:0.00888275960120293\n",
      "train loss:0.0008303780870666785\n",
      "train loss:0.00385429140509004\n",
      "train loss:0.007782034709044771\n",
      "train loss:0.0033456781632371702\n",
      "train loss:0.0038024260279447457\n",
      "train loss:0.0018069590399880926\n",
      "train loss:0.0026922647324453663\n",
      "train loss:0.014780717832151007\n",
      "train loss:0.0005225151165936872\n",
      "train loss:0.0005047520154360295\n",
      "train loss:0.034951836440432954\n",
      "train loss:0.01413504698735035\n",
      "train loss:0.002470228200172593\n",
      "train loss:0.00023073721446150575\n",
      "train loss:0.003571853047501067\n",
      "train loss:0.022785131010517778\n",
      "train loss:0.0017813023274808144\n",
      "train loss:0.0016663506555777388\n",
      "train loss:0.004619098772862319\n",
      "train loss:0.027137877329566643\n",
      "train loss:0.00845670505798762\n",
      "train loss:0.004825582686195787\n",
      "train loss:0.019420332107519523\n",
      "train loss:0.009180671902126429\n",
      "train loss:0.013122801012017465\n",
      "train loss:0.004147403021559646\n",
      "train loss:0.005265649380357832\n",
      "train loss:0.015948627708317276\n",
      "train loss:0.0038499333999850336\n",
      "train loss:0.0026493188587341115\n",
      "train loss:0.007901127123247744\n",
      "train loss:0.01983005618291649\n",
      "train loss:0.0037818843792611426\n",
      "train loss:0.0026169732753519004\n",
      "train loss:0.003408811733063623\n",
      "train loss:0.004993176711567219\n",
      "train loss:0.002850583817292233\n",
      "train loss:0.007902339059301966\n",
      "train loss:0.008927318549465131\n",
      "train loss:0.002888349846960812\n",
      "train loss:0.017959509089695447\n",
      "train loss:0.006055486870397178\n",
      "train loss:0.02082180328182557\n",
      "train loss:0.006629763495232558\n",
      "train loss:0.007127667754021125\n",
      "train loss:0.0066194878882431555\n",
      "train loss:0.06002140386549629\n",
      "train loss:0.007294122314645411\n",
      "train loss:0.012849001593269873\n",
      "train loss:0.001662215634587284\n",
      "train loss:0.0034925087711724684\n",
      "train loss:0.0015836436326949592\n",
      "train loss:0.0016119779808528316\n",
      "train loss:0.007211879251045315\n",
      "train loss:0.011752470043095164\n",
      "train loss:0.019763243005589445\n",
      "train loss:0.003946753228500173\n",
      "train loss:0.006725582701387348\n",
      "train loss:0.003258515384974709\n",
      "train loss:0.004684152929205145\n",
      "train loss:0.0029800504806017894\n",
      "train loss:0.0033012792110149874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0011730472911607925\n",
      "train loss:0.01941173796110531\n",
      "train loss:0.0004484711031937941\n",
      "train loss:0.005376819864474287\n",
      "train loss:0.0009493334938514136\n",
      "train loss:0.001635020740113689\n",
      "train loss:0.013551026329097687\n",
      "train loss:0.007423545717942617\n",
      "train loss:0.0017971984950810851\n",
      "train loss:0.002393480543346357\n",
      "train loss:0.0023397633336470105\n",
      "train loss:0.014155846397856074\n",
      "train loss:0.0032482488466251966\n",
      "train loss:0.004656917806845052\n",
      "train loss:0.001994858214438926\n",
      "train loss:0.0022921144776922578\n",
      "train loss:0.0008991073996134749\n",
      "train loss:0.003371757752322758\n",
      "train loss:0.008593742115613241\n",
      "train loss:0.00701448083278257\n",
      "train loss:0.0033397042144768267\n",
      "train loss:0.004824397638393238\n",
      "train loss:0.0046542448080583824\n",
      "train loss:0.0016629070843952255\n",
      "train loss:0.0034304368263783206\n",
      "train loss:0.000768935867681581\n",
      "train loss:0.005016692756954979\n",
      "train loss:0.004906794886054632\n",
      "train loss:0.0011477741799950597\n",
      "train loss:0.0004229900381270823\n",
      "train loss:0.004645887362491496\n",
      "train loss:0.010950001499732942\n",
      "train loss:0.0014521139034159437\n",
      "train loss:0.001511915679353471\n",
      "train loss:0.014270919484974607\n",
      "train loss:0.0006422543261115477\n",
      "train loss:0.0019196754472548326\n",
      "train loss:0.000881735146509384\n",
      "train loss:0.01940931616346259\n",
      "train loss:0.001548132733858861\n",
      "train loss:0.013645702066372221\n",
      "train loss:0.007630332394337881\n",
      "train loss:0.01528722207449927\n",
      "train loss:0.0036346120701364034\n",
      "train loss:0.036430424659900755\n",
      "train loss:0.001459749295444356\n",
      "train loss:0.0015636103999539473\n",
      "train loss:0.0018677011848236044\n",
      "train loss:0.006608473663731342\n",
      "train loss:0.008458840407442233\n",
      "train loss:0.006811300124239453\n",
      "train loss:0.008978187360821628\n",
      "train loss:0.0051153667475142\n",
      "train loss:0.003438585309608839\n",
      "train loss:0.0027049550720919815\n",
      "train loss:0.00269509497981431\n",
      "train loss:0.005291269364990817\n",
      "train loss:0.005240388935744604\n",
      "train loss:0.0036943231043109572\n",
      "train loss:0.0048334776374012785\n",
      "train loss:0.0013112379145006382\n",
      "train loss:0.003144492422842863\n",
      "train loss:0.005315235582859259\n",
      "train loss:0.0045642561185507986\n",
      "train loss:0.0015733354319397096\n",
      "train loss:0.0019896230912518314\n",
      "train loss:0.0029306256535402884\n",
      "train loss:0.003888121464493667\n",
      "train loss:0.005827711120036639\n",
      "train loss:0.003889020142665745\n",
      "train loss:0.00129014652850776\n",
      "train loss:0.010618610791109644\n",
      "train loss:0.004222937892334628\n",
      "train loss:0.006483441839244321\n",
      "train loss:0.0006614278265869272\n",
      "train loss:0.0011273615491507198\n",
      "train loss:0.01959497956584159\n",
      "train loss:0.010081307974609783\n",
      "train loss:0.013384252795672529\n",
      "train loss:0.003579105074877154\n",
      "train loss:0.005444553088487787\n",
      "train loss:0.010191007978291592\n",
      "train loss:0.001651076447890798\n",
      "train loss:0.010080860126084055\n",
      "train loss:0.0021096783067161033\n",
      "train loss:0.0009517174347857656\n",
      "train loss:0.0008529038123090882\n",
      "train loss:0.013361332249682143\n",
      "train loss:0.0022535099078770443\n",
      "train loss:0.0016820306459743993\n",
      "train loss:0.005496678103527835\n",
      "train loss:0.002366709976027755\n",
      "train loss:0.0031935239035587077\n",
      "train loss:0.003317122369243668\n",
      "train loss:0.001171190829696438\n",
      "train loss:0.001794209896227864\n",
      "train loss:0.0012197595716743794\n",
      "train loss:0.0025068851364442478\n",
      "train loss:0.011322340470190768\n",
      "train loss:0.005982565231967067\n",
      "=== epoch:19, train acc:0.9982110507246377, test acc:0.9878623188405797 ===\n",
      "train loss:0.001965136908574224\n",
      "train loss:0.005307090169081726\n",
      "train loss:0.0010785194449320285\n",
      "train loss:0.0011606893093612633\n",
      "train loss:0.0004957615970687559\n",
      "train loss:0.003061762527200258\n",
      "train loss:0.0004130944287306483\n",
      "train loss:0.0008446023633082796\n",
      "train loss:0.0014832453663067994\n",
      "train loss:0.0008480827442972797\n",
      "train loss:0.000893083398685391\n",
      "train loss:0.004604957843932817\n",
      "train loss:0.00015328350695473054\n",
      "train loss:0.008343255538857852\n",
      "train loss:0.009146507180163932\n",
      "train loss:0.0010462377649938994\n",
      "train loss:0.001474578525891167\n",
      "train loss:0.024283059474198107\n",
      "train loss:0.010419825689990257\n",
      "train loss:0.006271581711328371\n",
      "train loss:0.0005712609840357464\n",
      "train loss:0.003138784367324042\n",
      "train loss:0.0008933551815483302\n",
      "train loss:0.0021097255338600306\n",
      "train loss:0.0010911813855878124\n",
      "train loss:0.009619813192576905\n",
      "train loss:0.000535107093056981\n",
      "train loss:0.0016320000599781134\n",
      "train loss:0.005245266585846054\n",
      "train loss:0.0022611304598972393\n",
      "train loss:0.0029452664668333075\n",
      "train loss:0.003102897337749877\n",
      "train loss:0.0026800058247788182\n",
      "train loss:0.0025090049191447816\n",
      "train loss:0.003476394851280379\n",
      "train loss:0.0034809913467009025\n",
      "train loss:0.012263009086524822\n",
      "train loss:0.01352194319010523\n",
      "train loss:0.011794760608232167\n",
      "train loss:0.003318526078279186\n",
      "train loss:0.0009959199363196454\n",
      "train loss:0.016561916285546655\n",
      "train loss:0.0034628016919227987\n",
      "train loss:0.0011845905085215113\n",
      "train loss:0.00997467049342206\n",
      "train loss:0.0010773735534384428\n",
      "train loss:0.016573510667364583\n",
      "train loss:0.018075514529464785\n",
      "train loss:0.002337234006818809\n",
      "train loss:0.021210757248631317\n",
      "train loss:0.00532660284258879\n",
      "train loss:0.0308525728244682\n",
      "train loss:0.0017408236569517355\n",
      "train loss:0.007086598181225098\n",
      "train loss:0.009028275213031577\n",
      "train loss:0.008240333265752357\n",
      "train loss:0.0027748860451392806\n",
      "train loss:0.004694157468384286\n",
      "train loss:0.044718122813916385\n",
      "train loss:0.01250334793729012\n",
      "train loss:0.00398408968540257\n",
      "train loss:0.007136651016291707\n",
      "train loss:0.026531784610276023\n",
      "train loss:0.004723064937390927\n",
      "train loss:0.0027848274378987144\n",
      "train loss:0.0028997989052421324\n",
      "train loss:0.0013896779988875369\n",
      "train loss:0.002104611652890837\n",
      "train loss:0.003144451405491097\n",
      "train loss:0.007043984373853217\n",
      "train loss:0.0012801472489767272\n",
      "train loss:0.0072167925655329\n",
      "train loss:0.0052703349436019516\n",
      "train loss:0.004925386094344981\n",
      "train loss:0.018669254719703352\n",
      "train loss:0.001341520140332525\n",
      "train loss:0.0014482366304122344\n",
      "train loss:0.006885353090741119\n",
      "train loss:0.004782810558538266\n",
      "train loss:0.0016080380020409694\n",
      "train loss:0.004210261980720786\n",
      "train loss:0.013975473175523282\n",
      "train loss:0.00466404978360078\n",
      "train loss:0.0025398345071330794\n",
      "train loss:0.0016934979875892194\n",
      "train loss:0.0016874470269365217\n",
      "train loss:0.000863381528326157\n",
      "train loss:0.0012719816227550077\n",
      "train loss:0.005247306194516878\n",
      "train loss:0.0010164764866620018\n",
      "train loss:0.004986480493930839\n",
      "train loss:0.004424538610153159\n",
      "train loss:0.0006215949203727226\n",
      "train loss:0.0029496013953645346\n",
      "train loss:0.005641848331352641\n",
      "train loss:0.0012715885755982005\n",
      "train loss:0.005583927056788502\n",
      "train loss:0.004214062145305153\n",
      "train loss:0.0017875620613343147\n",
      "train loss:0.005433249581026353\n",
      "train loss:0.0019280949523868334\n",
      "train loss:0.0015635819152167209\n",
      "train loss:0.0006797274870608411\n",
      "train loss:0.011974495372400175\n",
      "train loss:0.002602534903228726\n",
      "train loss:0.0071142042476493\n",
      "train loss:0.0012252508503782101\n",
      "train loss:0.0014613491189078742\n",
      "train loss:0.00871319990920246\n",
      "train loss:0.0026872982371669496\n",
      "train loss:0.01748369672484758\n",
      "train loss:0.0009326981243930977\n",
      "train loss:0.0006728584228596934\n",
      "train loss:0.005368742990344803\n",
      "train loss:0.0021558220714235064\n",
      "train loss:0.0009348451403042167\n",
      "train loss:0.00588509942999261\n",
      "train loss:0.002547466152108263\n",
      "train loss:0.002675969032709709\n",
      "train loss:0.0009180375023358851\n",
      "train loss:0.0036125842719044927\n",
      "train loss:0.0009784562481884105\n",
      "train loss:0.007264886711319837\n",
      "train loss:0.04080217976396568\n",
      "train loss:0.0014335585071228\n",
      "train loss:0.0061512471503873956\n",
      "train loss:0.0006405804273726535\n",
      "train loss:0.009285734070815259\n",
      "train loss:0.0013390138976368685\n",
      "train loss:0.0012036194150814577\n",
      "train loss:0.006918964272777331\n",
      "train loss:0.003340118789195035\n",
      "train loss:0.014419830751262816\n",
      "train loss:0.0037523525045937193\n",
      "train loss:0.004592752585144341\n",
      "train loss:0.003962924798646166\n",
      "train loss:0.00627000908494819\n",
      "train loss:0.0020430987357161474\n",
      "train loss:0.005336307553424108\n",
      "train loss:0.004250189222898146\n",
      "train loss:0.003862635507243298\n",
      "train loss:0.002465512858814674\n",
      "train loss:0.008347433658819379\n",
      "train loss:0.025116818066134704\n",
      "train loss:0.009203704484794323\n",
      "train loss:0.0025439881872763915\n",
      "train loss:0.012876855721847848\n",
      "train loss:0.001643629831240052\n",
      "train loss:0.0054425968646010315\n",
      "train loss:0.014868755157198286\n",
      "train loss:0.016005369922101262\n",
      "train loss:0.005421563693573838\n",
      "train loss:0.00372289627902606\n",
      "train loss:0.002005148184830195\n",
      "train loss:0.0007105899701191114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0038508509427604355\n",
      "train loss:0.00566425287527153\n",
      "train loss:0.022393005243422383\n",
      "train loss:0.015904004333851102\n",
      "train loss:0.0034344830961483015\n",
      "train loss:0.0004235631848471213\n",
      "train loss:0.00036763207774980454\n",
      "train loss:0.0011314023688143503\n",
      "train loss:0.0017415033247225144\n",
      "train loss:0.00699442609615275\n",
      "train loss:0.0005478357128726214\n",
      "train loss:0.004494103480825353\n",
      "train loss:0.008506216818759198\n",
      "train loss:0.006661399086824946\n",
      "train loss:0.00809556496294705\n",
      "train loss:0.004137300366973464\n",
      "train loss:0.01849993137356854\n",
      "train loss:0.0019213005601265744\n",
      "train loss:0.0016755098615323226\n",
      "train loss:0.001948689139629199\n",
      "train loss:0.007127279028692903\n",
      "train loss:0.0008003005989267523\n",
      "train loss:0.006416236229488997\n",
      "train loss:0.000820206047285743\n",
      "train loss:0.0057129085670210725\n",
      "train loss:0.0056580755993465995\n",
      "train loss:0.00436140110247024\n",
      "train loss:0.0024570002785221975\n",
      "train loss:0.00452233067853294\n",
      "train loss:0.0015321183335261072\n",
      "train loss:0.004302068637107615\n",
      "train loss:0.004184033673398791\n",
      "train loss:0.0012631590206300298\n",
      "train loss:0.009109067219628002\n",
      "train loss:0.005621621681531247\n",
      "train loss:0.003749844943625463\n",
      "train loss:0.0002778378613577306\n",
      "train loss:0.0004315858461068788\n",
      "train loss:0.004329117777277791\n",
      "train loss:0.0018941904728171\n",
      "train loss:0.001182531293540577\n",
      "train loss:0.0016812520170012336\n",
      "train loss:0.0065930378550906055\n",
      "train loss:0.0033176704819113984\n",
      "train loss:0.008149164000880265\n",
      "train loss:0.0015642645241817975\n",
      "train loss:0.0034626023013270965\n",
      "train loss:0.013700225831577597\n",
      "train loss:0.001914909098908089\n",
      "train loss:0.002680954713506283\n",
      "train loss:0.007305727331941934\n",
      "train loss:0.0039639977931477605\n",
      "train loss:0.0019966274404031215\n",
      "train loss:0.00307827664517996\n",
      "train loss:0.001948060137338701\n",
      "train loss:0.005418180095540079\n",
      "train loss:0.005332418975548573\n",
      "train loss:0.004305333249561481\n",
      "train loss:0.003425377976151366\n",
      "train loss:0.005071728395130926\n",
      "train loss:0.008526300197917617\n",
      "train loss:0.011018079202669976\n",
      "train loss:0.008947922879503948\n",
      "train loss:0.004477316656460877\n",
      "train loss:0.010314953861505858\n",
      "train loss:0.0022314252127444523\n",
      "train loss:0.022345959166194942\n",
      "train loss:0.001977663046235459\n",
      "train loss:0.0012274184484814467\n",
      "train loss:0.0030039692476851046\n",
      "train loss:0.0020105817693040016\n",
      "train loss:0.014575023337316322\n",
      "train loss:0.009457643861846282\n",
      "train loss:0.002505897053408576\n",
      "train loss:0.0030005325312586306\n",
      "train loss:0.0015228326876167658\n",
      "train loss:0.00183662638572403\n",
      "train loss:0.00568708271619813\n",
      "train loss:0.0033583492613806294\n",
      "train loss:0.002524855495868966\n",
      "train loss:0.0017768646401243981\n",
      "train loss:0.02332278624501153\n",
      "train loss:0.0006192136385049641\n",
      "train loss:0.002494286536976921\n",
      "train loss:0.004690117407646227\n",
      "train loss:0.0019064332048750923\n",
      "train loss:0.002994439762171485\n",
      "train loss:0.003839201341080988\n",
      "train loss:0.0012688609469287692\n",
      "train loss:0.00331440961562109\n",
      "train loss:0.0011341829553975426\n",
      "train loss:0.0003802348779969644\n",
      "train loss:0.0012459980809155086\n",
      "train loss:0.0017781500637396826\n",
      "train loss:0.0035482365251846945\n",
      "train loss:0.0018640378277843793\n",
      "train loss:0.0011282104922602494\n",
      "train loss:0.008044798048383199\n",
      "train loss:0.008273383629391338\n",
      "train loss:0.005034629291417177\n",
      "train loss:0.002185908758859157\n",
      "train loss:0.000934417901522938\n",
      "train loss:0.01638545869679934\n",
      "train loss:0.0037184775894323493\n",
      "train loss:0.008623890492343193\n",
      "train loss:0.005537805026115633\n",
      "train loss:0.00511495977926707\n",
      "train loss:0.0007375597603183936\n",
      "train loss:0.0015763795626435036\n",
      "train loss:0.0038566474858167735\n",
      "train loss:0.0017151880596870278\n",
      "train loss:0.007443072768714896\n",
      "train loss:0.003476550469992507\n",
      "train loss:0.002892628764409764\n",
      "train loss:0.0007444006501620607\n",
      "train loss:0.00376693518625285\n",
      "train loss:0.0010624569746925819\n",
      "train loss:0.002688639156992962\n",
      "train loss:0.0044594266467379625\n",
      "train loss:0.011562197774463812\n",
      "train loss:0.0011463748471208226\n",
      "train loss:0.013192733945768123\n",
      "train loss:0.003935796885345903\n",
      "train loss:0.005309035745077863\n",
      "train loss:0.007817850625902371\n",
      "train loss:0.0035753645885194555\n",
      "train loss:0.002988724117545385\n",
      "train loss:0.002653711713983513\n",
      "train loss:0.0010243820941401428\n",
      "train loss:0.005755514819108675\n",
      "train loss:0.0034174439550140826\n",
      "train loss:0.013027137569897923\n",
      "train loss:0.033683840498547564\n",
      "train loss:0.0012788906125475068\n",
      "train loss:0.005085561017056994\n",
      "train loss:0.0029552508381971787\n",
      "train loss:0.0011821592729139255\n",
      "train loss:0.008493346195928697\n",
      "train loss:0.0029120724795837012\n",
      "train loss:0.0005281556215387677\n",
      "train loss:0.0016067935075152159\n",
      "train loss:0.004024744680416609\n",
      "train loss:0.0016156279648067299\n",
      "train loss:0.005554216558879833\n",
      "train loss:0.0011994811177030824\n",
      "train loss:0.0004472796855761257\n",
      "train loss:0.0055047677174309365\n",
      "train loss:0.0012983597777804234\n",
      "train loss:0.0043941523189410464\n",
      "train loss:0.000764812364240216\n",
      "train loss:0.002472683959841615\n",
      "train loss:0.0018860992331458132\n",
      "train loss:0.0033552713015225027\n",
      "train loss:0.0031081061666781494\n",
      "train loss:0.011278276022669112\n",
      "train loss:0.0003050056130428551\n",
      "train loss:0.0010840565838653234\n",
      "train loss:0.0008382033665488205\n",
      "train loss:0.00043083777612271286\n",
      "train loss:0.00041202485376812666\n",
      "train loss:0.007487213420315742\n",
      "train loss:0.0009048118902639841\n",
      "train loss:0.00280900799600806\n",
      "train loss:0.0006081890423148252\n",
      "train loss:0.002395169282137761\n",
      "train loss:0.009647694243960078\n",
      "train loss:0.0016966498568695262\n",
      "train loss:0.01634728489740052\n",
      "train loss:0.0032854766452305997\n",
      "train loss:0.0014865576096097489\n",
      "train loss:0.004735217094249798\n",
      "train loss:0.0003062719385169641\n",
      "train loss:0.0019195606697744148\n",
      "train loss:0.0064796636985883374\n",
      "train loss:0.0014498449729031413\n",
      "train loss:0.003917822727122972\n",
      "train loss:0.0034469729112262303\n",
      "train loss:0.004796572742263714\n",
      "train loss:0.0044531066036102235\n",
      "train loss:0.0036103431713910904\n",
      "train loss:0.0014006528211606474\n",
      "train loss:0.002107987500822068\n",
      "train loss:0.0017196989098739088\n",
      "train loss:0.005905193676934273\n",
      "train loss:0.0034674500578116673\n",
      "train loss:0.0012666469738046792\n",
      "train loss:0.008043281126219461\n",
      "train loss:0.0005507666998370351\n",
      "train loss:0.003083085439013678\n",
      "train loss:0.0014602368378552654\n",
      "train loss:0.000664951102462784\n",
      "train loss:0.012463491975755157\n",
      "train loss:0.0030207890095633387\n",
      "train loss:0.0005016159516657609\n",
      "train loss:0.0011907729269107865\n",
      "train loss:0.001544967316547639\n",
      "train loss:0.00842381173959666\n",
      "train loss:0.0009577124158165403\n",
      "train loss:0.005233277727431805\n",
      "train loss:0.0006120279680459698\n",
      "train loss:0.0007797626036132565\n",
      "train loss:0.005533101135950047\n",
      "train loss:0.0015265373800505014\n",
      "train loss:0.009015638337462717\n",
      "train loss:0.0027132518118362977\n",
      "train loss:0.0007267454019538064\n",
      "train loss:0.043891126457460336\n",
      "train loss:0.0018173145354588\n",
      "train loss:0.0018211594278795937\n",
      "train loss:0.006591892472850436\n",
      "train loss:0.0038324568772041914\n",
      "train loss:0.0019618405314802676\n",
      "train loss:0.013537575760062476\n",
      "=== epoch:20, train acc:0.998143115942029, test acc:0.9870471014492753 ===\n",
      "train loss:0.0014929028068215292\n",
      "train loss:0.00382238688058847\n",
      "train loss:0.00339758348929444\n",
      "train loss:0.0009635570809069133\n",
      "train loss:0.0029223011306773895\n",
      "train loss:0.0007217227811199103\n",
      "train loss:0.006487583205968234\n",
      "train loss:0.001955619998194823\n",
      "train loss:0.002637278734065696\n",
      "train loss:0.0012916614684177425\n",
      "train loss:0.0019074798161397982\n",
      "train loss:0.006422770501551352\n",
      "train loss:0.0012014733453000474\n",
      "train loss:0.004328339345313141\n",
      "train loss:0.006110328253911159\n",
      "train loss:0.005687876204526577\n",
      "train loss:0.010176168757147218\n",
      "train loss:0.001601070428909234\n",
      "train loss:0.0008646051101230471\n",
      "train loss:0.003400359385482459\n",
      "train loss:0.004244236723006048\n",
      "train loss:0.0010823674229910147\n",
      "train loss:0.019628270621897804\n",
      "train loss:0.0005276650098626675\n",
      "train loss:0.0013018528458235725\n",
      "train loss:0.0013961567782883645\n",
      "train loss:0.001199218470545177\n",
      "train loss:0.0022547154472563743\n",
      "train loss:0.0015268306495948417\n",
      "train loss:0.00043380791596307907\n",
      "train loss:0.010739260323139667\n",
      "train loss:0.0018312908900846871\n",
      "train loss:0.0006089287413365045\n",
      "train loss:0.00325547725658512\n",
      "train loss:0.0025165473300578054\n",
      "train loss:0.0013558752688742695\n",
      "train loss:0.0017445435825352456\n",
      "train loss:0.0017492078569856274\n",
      "train loss:0.0012304335110311748\n",
      "train loss:0.0034724478558948724\n",
      "train loss:0.0012739537093713487\n",
      "train loss:0.0024214171485134523\n",
      "train loss:0.0008651256301160481\n",
      "train loss:0.001269773537450604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0012686041720895741\n",
      "train loss:0.011565040103769419\n",
      "train loss:0.000630003726883735\n",
      "train loss:0.001206751740274571\n",
      "train loss:0.001278792540509911\n",
      "train loss:0.005850624812532648\n",
      "train loss:0.0012917449838169795\n",
      "train loss:0.015858633990069825\n",
      "train loss:0.01459986899321417\n",
      "train loss:0.0019676760263173825\n",
      "train loss:0.0038929955137055466\n",
      "train loss:0.007006996342328546\n",
      "train loss:0.0008665027928836225\n",
      "train loss:0.0038265016737665304\n",
      "train loss:0.0009644885632024374\n",
      "train loss:0.017001861473955045\n",
      "train loss:0.000526514077887138\n",
      "train loss:0.0027190951888911028\n",
      "train loss:0.0026479057698245113\n",
      "train loss:0.005209605487640355\n",
      "train loss:0.0006658491977892833\n",
      "train loss:0.004262703548552832\n",
      "train loss:0.001685326519306929\n",
      "train loss:0.0020344102402885264\n",
      "train loss:0.0026339063052502236\n",
      "train loss:0.0018951753516606032\n",
      "train loss:0.007953567530949656\n",
      "train loss:0.0018752369230405066\n",
      "train loss:0.0036352113073303037\n",
      "train loss:0.0006569712787580998\n",
      "train loss:0.0048650888016855106\n",
      "train loss:0.005332050746305902\n",
      "train loss:0.0004886752626199986\n",
      "train loss:0.0023656076437105823\n",
      "train loss:0.0462662744667404\n",
      "train loss:0.0036092261615508184\n",
      "train loss:0.03926565628629054\n",
      "train loss:0.0032911873496753214\n",
      "train loss:0.0005237297432106971\n",
      "train loss:0.013472930854928991\n",
      "train loss:0.0024954167712647407\n",
      "train loss:0.002857220683799589\n",
      "train loss:0.0012518045601664825\n",
      "train loss:0.0029504279228336023\n",
      "train loss:0.002493055140808163\n",
      "train loss:0.005167794361115374\n",
      "train loss:0.0007717959536401836\n",
      "train loss:0.0009511035922752051\n",
      "train loss:0.006408418771366509\n",
      "train loss:0.005020747310154427\n",
      "train loss:0.005501312019473193\n",
      "train loss:0.0060692142412568425\n",
      "train loss:0.0007043545286304374\n",
      "train loss:0.015012986413662048\n",
      "train loss:0.001531324842656821\n",
      "train loss:0.0030722912146634355\n",
      "train loss:0.017089027136823413\n",
      "train loss:0.0011470474011804155\n",
      "train loss:0.0015882642050427114\n",
      "train loss:0.007314271828379709\n",
      "train loss:0.0040742848117326985\n",
      "train loss:0.020114954300021904\n",
      "train loss:0.0045187112900580365\n",
      "train loss:0.015761202347955843\n",
      "train loss:0.005124176485297448\n",
      "train loss:0.006378214483359261\n",
      "train loss:0.0012826647092964226\n",
      "train loss:0.0030690945983568997\n",
      "train loss:0.0017247223262992753\n",
      "train loss:0.0016222303753800735\n",
      "train loss:0.013987212022919773\n",
      "train loss:0.005113240343333407\n",
      "train loss:0.004195875106564008\n",
      "train loss:0.0036880843725069015\n",
      "train loss:0.0029191823891256685\n",
      "train loss:0.0020976031376951196\n",
      "train loss:0.0014096911463117741\n",
      "train loss:0.010927465190695089\n",
      "train loss:0.001305523831401982\n",
      "train loss:0.0004431781645350529\n",
      "train loss:0.0004440844055374915\n",
      "train loss:0.020623291132487197\n",
      "train loss:0.0005945062705204226\n",
      "train loss:0.0006112617037736785\n",
      "train loss:0.0010327330205917248\n",
      "train loss:0.0013862120436143844\n",
      "train loss:0.0003531829959247804\n",
      "train loss:0.00453419495702075\n",
      "train loss:0.01089579735593872\n",
      "train loss:0.001162247312823326\n",
      "train loss:0.0008452183989074828\n",
      "train loss:0.002413666579988133\n",
      "train loss:0.01088949414599883\n",
      "train loss:0.010380138127800689\n",
      "train loss:0.001619101595156921\n",
      "train loss:0.02234037874022442\n",
      "train loss:0.0008447439780116778\n",
      "train loss:0.0025101087277684736\n",
      "train loss:0.0030518562563271456\n",
      "train loss:0.006388798184189484\n",
      "train loss:0.005570855949308266\n",
      "train loss:0.000704938050792246\n",
      "train loss:0.00437649956078896\n",
      "train loss:0.001490860070276457\n",
      "train loss:0.0019120154258073783\n",
      "train loss:0.005344166481312436\n",
      "train loss:0.005755401653997726\n",
      "train loss:0.0047570615760341405\n",
      "train loss:0.03928509535908989\n",
      "train loss:0.0005255404102397492\n",
      "train loss:0.007358598266281593\n",
      "train loss:0.0007904654635213935\n",
      "train loss:0.0020595455498805294\n",
      "train loss:0.0032738210367509023\n",
      "train loss:0.0006882686249162606\n",
      "train loss:0.0007544012365154589\n",
      "train loss:0.0014245562338167054\n",
      "train loss:0.009480961488384674\n",
      "train loss:0.002125162612090617\n",
      "train loss:0.0011720958481532247\n",
      "train loss:0.01058559717554763\n",
      "train loss:0.0007421673579179588\n",
      "train loss:0.008235095975045867\n",
      "train loss:0.025469825979263845\n",
      "train loss:0.0021032121678168088\n",
      "train loss:0.0016081248573574518\n",
      "train loss:0.0006924286987083094\n",
      "train loss:0.012989631867606728\n",
      "train loss:0.01098387389414439\n",
      "train loss:0.0014437968225483177\n",
      "train loss:0.0017516443410330632\n",
      "train loss:0.0008763376346741851\n",
      "train loss:0.002817721905810017\n",
      "train loss:0.0019096000778947128\n",
      "train loss:0.0052223864832398725\n",
      "train loss:0.008920514353652091\n",
      "train loss:0.0002128219370356018\n",
      "train loss:0.0007528627176890674\n",
      "train loss:0.0033832645078631794\n",
      "train loss:0.001024215314041579\n",
      "train loss:0.0010177520562770408\n",
      "train loss:0.005282789614104842\n",
      "train loss:0.002017059427695898\n",
      "train loss:0.0020177850059809274\n",
      "train loss:0.0024668495015714\n",
      "train loss:0.002514148081500279\n",
      "train loss:0.0005523178546964987\n",
      "train loss:0.0021219017086162736\n",
      "train loss:0.003881342193220408\n",
      "train loss:0.0012116027678078926\n",
      "train loss:0.0023395079244727716\n",
      "train loss:0.0022893691306414914\n",
      "train loss:0.001350875847706056\n",
      "train loss:0.0014586832266550614\n",
      "train loss:0.0017466267217243813\n",
      "train loss:0.0018098143690732402\n",
      "train loss:0.0006406299650859715\n",
      "train loss:0.0065137958562085435\n",
      "train loss:0.003433973570896693\n",
      "train loss:0.0007596177127224028\n",
      "train loss:0.0026154830464251376\n",
      "train loss:0.0048323765917307115\n",
      "train loss:0.006048686455451341\n",
      "train loss:0.002115916753674485\n",
      "train loss:0.007519299415642592\n",
      "train loss:0.005570039231960959\n",
      "train loss:0.00016193420201139903\n",
      "train loss:0.0018588106899142226\n",
      "train loss:0.0021456746945697916\n",
      "train loss:0.018812389149217405\n",
      "train loss:0.014854460234508813\n",
      "train loss:0.013135014409619767\n",
      "train loss:0.022842281913075302\n",
      "train loss:0.0006379383310354369\n",
      "train loss:0.0005456338435231298\n",
      "train loss:0.0033696644280574828\n",
      "train loss:0.001948383562526488\n",
      "train loss:0.00774723418861385\n",
      "train loss:0.004254983545935253\n",
      "train loss:0.0023597450488347447\n",
      "train loss:0.0005858177329215842\n",
      "train loss:0.006688362032850238\n",
      "train loss:0.0068565595598999215\n",
      "train loss:0.002761390477953075\n",
      "train loss:0.0009526486482746484\n",
      "train loss:0.0020256126840779514\n",
      "train loss:0.0020088240015582225\n",
      "train loss:0.013474620970325084\n",
      "train loss:0.0013382588440849152\n",
      "train loss:0.002416320652542297\n",
      "train loss:0.001208935426448359\n",
      "train loss:0.006365765181903452\n",
      "train loss:0.0025920310357911893\n",
      "train loss:0.011158649161583916\n",
      "train loss:0.001642673083235797\n",
      "train loss:0.013705220868221327\n",
      "train loss:0.045645462889267505\n",
      "train loss:0.003243784023496914\n",
      "train loss:0.005852392699325893\n",
      "train loss:0.001727736974188127\n",
      "train loss:0.0004679909772243755\n",
      "train loss:0.0017895777581215097\n",
      "train loss:0.0015773184711415156\n",
      "train loss:0.0023288578625300254\n",
      "train loss:0.001979425761834025\n",
      "train loss:0.009294608909143449\n",
      "train loss:0.0023965669480286883\n",
      "train loss:0.0040167732805383456\n",
      "train loss:0.0017488414955860814\n",
      "train loss:0.0029640360547911098\n",
      "train loss:0.010211866129150102\n",
      "train loss:0.010282609956021344\n",
      "train loss:0.002476522728176682\n",
      "train loss:0.006004449969671546\n",
      "train loss:0.00488036413247755\n",
      "train loss:0.03969901126630518\n",
      "train loss:0.008486927956640094\n",
      "train loss:0.006559551978258018\n",
      "train loss:0.003749005713748824\n",
      "train loss:0.016239687120890633\n",
      "train loss:0.0010960127539918951\n",
      "train loss:0.0025075946463883705\n",
      "train loss:0.0017433188019666957\n",
      "train loss:0.019869750908193214\n",
      "train loss:0.0017819558684162558\n",
      "train loss:0.003919157653781106\n",
      "train loss:0.010349071992458927\n",
      "train loss:0.0017162676284911858\n",
      "train loss:0.002063360132657999\n",
      "train loss:0.003261015451636499\n",
      "train loss:0.004519161265817373\n",
      "train loss:0.0052184903586192195\n",
      "train loss:0.0009129083645160416\n",
      "train loss:0.0036116696690074797\n",
      "train loss:0.00166391407869611\n",
      "train loss:0.006303927298497243\n",
      "train loss:0.0042661827664637105\n",
      "train loss:0.002005276715795258\n",
      "train loss:0.0070537019595094015\n",
      "train loss:0.0022777502937851076\n",
      "train loss:0.006948079635930288\n",
      "train loss:0.003450668257305932\n",
      "train loss:0.01396567227119544\n",
      "train loss:0.021164752070575183\n",
      "train loss:0.0010842511557766434\n",
      "train loss:0.0012409265997553394\n",
      "train loss:0.0013278824636608353\n",
      "train loss:0.0012679776897929808\n",
      "train loss:0.0030478291408343934\n",
      "train loss:0.006020139109820082\n",
      "train loss:0.001992631086425287\n",
      "train loss:0.016098443189452673\n",
      "train loss:0.0006525959644307459\n",
      "train loss:0.0013796242955167349\n",
      "train loss:0.0005428199961734349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0005201587243854243\n",
      "train loss:0.00881422641878185\n",
      "train loss:0.002225028380059588\n",
      "train loss:0.0009197793411665628\n",
      "train loss:0.002549989267603021\n",
      "train loss:0.0006725312423967237\n",
      "train loss:0.0054208305829077175\n",
      "train loss:0.009830588276882902\n",
      "train loss:0.0013817075763367347\n",
      "train loss:0.0010891260352168294\n",
      "train loss:0.020313792976374513\n",
      "train loss:0.0010482587488074985\n",
      "train loss:0.0015438034676266371\n",
      "train loss:0.015014873516415546\n",
      "train loss:0.0037946964613079495\n",
      "train loss:0.0006392897556237953\n",
      "train loss:0.00920949405375407\n",
      "train loss:0.001485490523877493\n",
      "train loss:0.0019446814403198322\n",
      "train loss:0.001196508607676154\n",
      "train loss:0.020449646734179577\n",
      "train loss:0.0005959137114654835\n",
      "train loss:0.001534584028369245\n",
      "train loss:0.0018249944740673292\n",
      "train loss:0.0007785816274675187\n",
      "train loss:0.008070149535693039\n",
      "train loss:0.0038100462796533717\n",
      "train loss:0.004844125263733101\n",
      "train loss:0.0013176252900195574\n",
      "train loss:0.0020227510926486775\n",
      "train loss:0.0028648140662406647\n",
      "train loss:0.003409424182473646\n",
      "train loss:0.0007524992456160119\n",
      "train loss:0.0069342179837620106\n",
      "train loss:0.003077503632942443\n",
      "train loss:0.003754080881204424\n",
      "train loss:0.0031544519572903195\n",
      "train loss:0.009532936973354233\n",
      "train loss:0.0012950114849365516\n",
      "train loss:0.0053986648705354485\n",
      "train loss:0.0015067097437548138\n",
      "train loss:0.0016384700223390563\n",
      "train loss:0.002368386156154653\n",
      "train loss:0.0007130525974125823\n",
      "train loss:0.000977552756522087\n",
      "train loss:0.0023361623516523207\n",
      "train loss:0.0060550577641497304\n",
      "train loss:0.0011302064243837696\n",
      "train loss:0.0029914334068161723\n",
      "train loss:0.0006990119195829018\n",
      "train loss:0.005942795729206757\n",
      "train loss:0.0013115589363885346\n",
      "train loss:0.0027104165469470616\n",
      "train loss:0.003953111832373862\n",
      "train loss:0.0067991198313518955\n",
      "train loss:0.004153464188721946\n",
      "train loss:0.007115511933005818\n",
      "train loss:0.0009856698261033448\n",
      "train loss:0.004998658952582262\n",
      "train loss:0.0008687202589157946\n",
      "train loss:0.0023079280373124603\n",
      "train loss:0.0018521825842106063\n",
      "train loss:0.003375558093417949\n",
      "train loss:0.0003208098569358096\n",
      "train loss:0.003990561011897696\n",
      "train loss:0.0015201920092133081\n",
      "train loss:0.005778434410663154\n",
      "train loss:0.006851010810538907\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9871376811594202\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 멀티레이어넷(ch06)\n",
    "\n",
    "# coding: utf-8\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.img_oxt import load_oxt\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_oxt()\n",
    "\n",
    "# 드롭아웃 사용 유무와 비울 설정 ========================\n",
    "use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False\n",
    "dropout_ratio = 0.2\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=3, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=120,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
    "\n",
    "# 그래프 그리기==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0.9, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.0966406529948574\n",
      "=== epoch:1, train acc:0.448, test acc:0.464 ===\n",
      "train loss:1.093666124484739\n",
      "train loss:1.0911517919690281\n",
      "train loss:1.0846822068155475\n",
      "train loss:1.0916642613689926\n",
      "train loss:1.095192690281254\n",
      "train loss:1.0700340862290711\n",
      "train loss:1.0736064712773432\n",
      "train loss:1.0557651877918905\n",
      "train loss:1.0517808196680565\n",
      "train loss:1.0212571484716575\n",
      "train loss:1.0201205258834092\n",
      "train loss:1.0007515350555505\n",
      "train loss:0.9784532499473463\n",
      "train loss:0.944191698645763\n",
      "train loss:0.9390660265769609\n",
      "train loss:0.8929242288066739\n",
      "train loss:0.8896076030017069\n",
      "train loss:0.8845637740274871\n",
      "train loss:0.8287375915358228\n",
      "train loss:0.8277277416851027\n",
      "train loss:0.7533550919990688\n",
      "train loss:0.8133686985465066\n",
      "train loss:0.8282455751979779\n",
      "train loss:0.8070329894109268\n",
      "train loss:0.6888978233824093\n",
      "train loss:0.6871541260392274\n",
      "train loss:0.751945560787782\n",
      "train loss:0.8460166978061946\n",
      "train loss:0.7844218504922138\n",
      "train loss:0.7566393576656618\n",
      "train loss:0.7735436907272901\n",
      "train loss:0.7741119444530362\n",
      "train loss:0.7289673869399441\n",
      "train loss:0.7480028237067683\n",
      "train loss:0.7538141354848621\n",
      "train loss:0.7646127120618065\n",
      "train loss:0.7095318270242505\n",
      "train loss:0.6375489264876667\n",
      "train loss:0.6647221343254575\n",
      "train loss:0.6801791311600589\n",
      "train loss:0.705431528510807\n",
      "train loss:0.6950713358487668\n",
      "train loss:0.7414557698026857\n",
      "train loss:0.7268355186066835\n",
      "train loss:0.6706638959421656\n",
      "train loss:0.6991889942080488\n",
      "train loss:0.6667888984312147\n",
      "train loss:0.7349465192099505\n",
      "train loss:0.6957835061435493\n",
      "train loss:0.5060615327651588\n",
      "train loss:0.5757443450607875\n",
      "train loss:0.610155763911161\n",
      "train loss:0.5735613104714877\n",
      "train loss:0.6611134343221009\n",
      "train loss:0.5309793579783061\n",
      "train loss:0.6190065965183875\n",
      "train loss:0.5958154863350129\n",
      "train loss:0.5034279773985476\n",
      "train loss:0.634990963198055\n",
      "train loss:0.5768395925613246\n",
      "train loss:0.5365916359860156\n",
      "train loss:0.5291129391748637\n",
      "train loss:0.4466668519538119\n",
      "train loss:0.5572231470674635\n",
      "train loss:0.5412404498830377\n",
      "train loss:0.4595234559516114\n",
      "train loss:0.5318067728896204\n",
      "train loss:0.48240048173658306\n",
      "train loss:0.476621361450299\n",
      "train loss:0.47395522891476954\n",
      "train loss:0.4354908825394668\n",
      "train loss:0.5003396045398344\n",
      "train loss:0.5247717493228009\n",
      "train loss:0.44933517870174644\n",
      "train loss:0.4995797150818186\n",
      "train loss:0.4939172784136337\n",
      "train loss:0.5070809135405311\n",
      "train loss:0.4054122294570598\n",
      "train loss:0.47823471539325796\n",
      "train loss:0.4393040146545605\n",
      "train loss:0.4354765435382885\n",
      "train loss:0.39687392409630207\n",
      "train loss:0.38895326490688603\n",
      "train loss:0.456830110905627\n",
      "train loss:0.35696680665000496\n",
      "train loss:0.35142055830472513\n",
      "train loss:0.29905999158782065\n",
      "train loss:0.3506767580289441\n",
      "train loss:0.41635661506416527\n",
      "train loss:0.25618733408454947\n",
      "train loss:0.38467586934332504\n",
      "train loss:0.290567633097616\n",
      "train loss:0.3470462807739798\n",
      "train loss:0.48045469960858284\n",
      "train loss:0.3437934048642787\n",
      "train loss:0.30022618799867984\n",
      "train loss:0.3328883414330734\n",
      "train loss:0.40789568444825597\n",
      "train loss:0.4244929530400884\n",
      "train loss:0.3608470431992733\n",
      "train loss:0.3116679199148965\n",
      "train loss:0.3568060993926583\n",
      "train loss:0.38604502940318974\n",
      "train loss:0.2998781312658353\n",
      "train loss:0.36097219820958937\n",
      "train loss:0.3330951618180999\n",
      "train loss:0.3144327882896106\n",
      "train loss:0.3331294034509957\n",
      "train loss:0.22100024220758596\n",
      "train loss:0.28805456333683904\n",
      "train loss:0.3062780739304285\n",
      "train loss:0.28940711114702533\n",
      "train loss:0.29514357651479195\n",
      "train loss:0.3156979907018861\n",
      "train loss:0.2983456642484557\n",
      "train loss:0.288929862144862\n",
      "train loss:0.3100714279421755\n",
      "train loss:0.34518438632218673\n",
      "train loss:0.29375336558116083\n",
      "train loss:0.20146080172593267\n",
      "train loss:0.1914258160888957\n",
      "train loss:0.2722154863579175\n",
      "train loss:0.29392151908418024\n",
      "train loss:0.25494878220129885\n",
      "train loss:0.2773708953753429\n",
      "train loss:0.1983943610479923\n",
      "train loss:0.23223596469803465\n",
      "train loss:0.30948627034390225\n",
      "train loss:0.18821834577734908\n",
      "train loss:0.26276257140403775\n",
      "train loss:0.22195915417982107\n",
      "train loss:0.2492519938111765\n",
      "train loss:0.23465496179964546\n",
      "train loss:0.22692575321787684\n",
      "train loss:0.2308145129459944\n",
      "train loss:0.2807301972986204\n",
      "train loss:0.30546001289358776\n",
      "train loss:0.19236494447661212\n",
      "train loss:0.3111144426928742\n",
      "train loss:0.1869363808832928\n",
      "train loss:0.17426815408008384\n",
      "train loss:0.21327322938644092\n",
      "train loss:0.2325321147475242\n",
      "train loss:0.18361826463492664\n",
      "train loss:0.2120067685028379\n",
      "train loss:0.19801818129204019\n",
      "train loss:0.19934652768663721\n",
      "train loss:0.21011349760086595\n",
      "train loss:0.27141629777893245\n",
      "train loss:0.2397164408098378\n",
      "train loss:0.2112896174977261\n",
      "train loss:0.24255439399053558\n",
      "train loss:0.1916060324171759\n",
      "train loss:0.17087978818919888\n",
      "train loss:0.1947284584717441\n",
      "train loss:0.19208902230417066\n",
      "train loss:0.21019128359966885\n",
      "train loss:0.19407159282800593\n",
      "train loss:0.28163153713325556\n",
      "train loss:0.26040767740714743\n",
      "train loss:0.20947075068051102\n",
      "train loss:0.22781367465923946\n",
      "train loss:0.19379735721649444\n",
      "train loss:0.23904545778278813\n",
      "train loss:0.17417110927536858\n",
      "train loss:0.18357008718196532\n",
      "train loss:0.21698779277371685\n",
      "train loss:0.1435101082193275\n",
      "train loss:0.17363659556685793\n",
      "train loss:0.20732238371792622\n",
      "train loss:0.24892857625282788\n",
      "train loss:0.14651713324187327\n",
      "train loss:0.21572422724099702\n",
      "train loss:0.13573312898604495\n",
      "train loss:0.23799200352254862\n",
      "train loss:0.2093999144384232\n",
      "train loss:0.1608494479604993\n",
      "train loss:0.2104310795443413\n",
      "train loss:0.18639993314010514\n",
      "train loss:0.2850654635701905\n",
      "train loss:0.12023487483928792\n",
      "train loss:0.20698027554570142\n",
      "train loss:0.20850340729545175\n",
      "train loss:0.1761843516685394\n",
      "train loss:0.1276866825429373\n",
      "train loss:0.12457259677164287\n",
      "train loss:0.12425826371724827\n",
      "train loss:0.23136916863074658\n",
      "train loss:0.18624321089119508\n",
      "train loss:0.19331498323149368\n",
      "train loss:0.10762529571773255\n",
      "train loss:0.16003921801399465\n",
      "train loss:0.1664783535332875\n",
      "train loss:0.18209038599895985\n",
      "train loss:0.1563662357465933\n",
      "train loss:0.24906998668741398\n",
      "train loss:0.15857256477758197\n",
      "train loss:0.16498875637679003\n",
      "train loss:0.15658114635745693\n",
      "train loss:0.13344554970022476\n",
      "train loss:0.1632372662416314\n",
      "train loss:0.11703346283468102\n",
      "train loss:0.168389673386166\n",
      "train loss:0.19318340254050234\n",
      "train loss:0.16550735539683117\n",
      "train loss:0.20799133194850977\n",
      "train loss:0.13719482204891756\n",
      "train loss:0.21659562000084084\n",
      "train loss:0.17516040506307098\n",
      "train loss:0.16285313101200702\n",
      "train loss:0.1928543473684142\n",
      "train loss:0.2114896967764959\n",
      "train loss:0.16937740999794723\n",
      "train loss:0.17848017381181977\n",
      "train loss:0.198879627245972\n",
      "train loss:0.17317437505534225\n",
      "train loss:0.19259943685895362\n",
      "train loss:0.22488541797340442\n",
      "train loss:0.16767145042106582\n",
      "train loss:0.15396831376114978\n",
      "train loss:0.1540961722144603\n",
      "train loss:0.1729820775690666\n",
      "train loss:0.216557794293478\n",
      "train loss:0.17610833930436195\n",
      "train loss:0.15159730318371864\n",
      "train loss:0.1608108175588647\n",
      "train loss:0.1998638541494073\n",
      "train loss:0.11912597669434913\n",
      "train loss:0.1920280788461556\n",
      "train loss:0.14281774637820693\n",
      "train loss:0.1244091954319509\n",
      "train loss:0.13154309470620215\n",
      "train loss:0.18757140435789305\n",
      "train loss:0.16250473816342392\n",
      "train loss:0.10579096694796757\n",
      "train loss:0.18467658582268698\n",
      "train loss:0.11601676788752105\n",
      "train loss:0.1529476999123456\n",
      "train loss:0.19181822209171956\n",
      "train loss:0.10722152614279387\n",
      "train loss:0.1315968087760068\n",
      "train loss:0.12640304980268333\n",
      "train loss:0.11405772854816769\n",
      "train loss:0.14821829067630077\n",
      "train loss:0.1135913816299\n",
      "train loss:0.08783128351414835\n",
      "train loss:0.14610143577036278\n",
      "train loss:0.12365576700189013\n",
      "train loss:0.10473612588600445\n",
      "train loss:0.11639443117620346\n",
      "train loss:0.143187850503307\n",
      "train loss:0.1537526498552141\n",
      "train loss:0.15509461514150438\n",
      "train loss:0.1340118283440295\n",
      "train loss:0.1252408363804736\n",
      "train loss:0.12147462586164974\n",
      "train loss:0.10232435438436188\n",
      "train loss:0.10742852185931637\n",
      "train loss:0.1275048221017553\n",
      "train loss:0.12275863532491733\n",
      "train loss:0.04569232473679847\n",
      "train loss:0.12235629454468348\n",
      "train loss:0.07855715618135597\n",
      "train loss:0.05651106638388132\n",
      "train loss:0.053292874963628556\n",
      "train loss:0.09573067427425576\n",
      "train loss:0.13481117740530452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.12124176039400862\n",
      "train loss:0.1874079776422664\n",
      "train loss:0.07560434964289048\n",
      "train loss:0.08654858741914549\n",
      "train loss:0.09502278162755859\n",
      "train loss:0.060030240353800224\n",
      "train loss:0.16972233988169017\n",
      "train loss:0.123359201522915\n",
      "train loss:0.22715883780967125\n",
      "train loss:0.14397493896519603\n",
      "train loss:0.11820541706637666\n",
      "train loss:0.12066859490679827\n",
      "train loss:0.08824796662705975\n",
      "train loss:0.1705358253325299\n",
      "train loss:0.11512946294781568\n",
      "train loss:0.15299867121756155\n",
      "train loss:0.18418213201438655\n",
      "train loss:0.12502462881005136\n",
      "train loss:0.13706082325112467\n",
      "train loss:0.07470313571433455\n"
     ]
    }
   ],
   "source": [
    "## simple convolutional network(cnn)(chap07)\n",
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.img_oxt import load_oxt\n",
    "from common.simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_oxt(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=3, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=120,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0.9, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep convolutional Network(chap08)\n",
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.img_oxt import load_oxt\n",
    "from common.deep_convnet import DeepConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_oxt(flatten=False)\n",
    "\n",
    "network = DeepConvNet()  \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=120,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보관\n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0.9, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from PIL import ImageTk, Image\n",
    "from tkinter import filedialog\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "window = Tk()\n",
    "window.title('oxt 예측하기')\n",
    "    \n",
    "oldx = oldy = -1\n",
    "\n",
    "def on_mouse(event, x, y, flags, param):\n",
    "    global oldx, oldy\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        oldx, oldy = x, y\n",
    "        # print('EVENT_LBUTTONDOWN: %d, %d' % (x, y))\n",
    "\n",
    "    # elif event == cv2.EVENT_LBUTTONUP:\n",
    "        # print('EVENT_LBUTTONUP: %d, %d' % (x, y))\n",
    "\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "            cv2.line(img, (oldx, oldy), (x, y), 0, 5, cv2.LINE_AA)\n",
    "            cv2.imshow('image', img)\n",
    "            oldx, oldy = x, y\n",
    "\n",
    "def crt():\n",
    "    global img, tmp_img\n",
    "    img = np.ones((280, 280), dtype=np.uint8) * 255\n",
    "\n",
    "    cv2.namedWindow('image')\n",
    "    cv2.setMouseCallback('image', on_mouse, img)\n",
    "\n",
    "    cv2.imshow('image', img)\n",
    "    cv2.waitKey(3000)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    img = cv2.resize(img, (28,28), interpolation=cv2.INTER_AREA)     # 28*28 resize\n",
    "    cv2.imwrite('tmp.png', img)\n",
    "    img = ~img  # invert\n",
    "    img=img.reshape(-1,1,28,28)\n",
    "    \n",
    "    labels_view=['o', 'x', '△']\n",
    "\n",
    "    y=network.predict(img)\n",
    "    pred_num=np.argmax(y)\n",
    "    result = \"my predict is %s\"%(labels_view[pred_num])\n",
    "    \n",
    "    tmp_img=Image.open('tmp.png')\n",
    "    tmp_img=ImageTk.PhotoImage(tmp_img)\n",
    "    \n",
    "    Label(window, text=\"파일경로: new\").grid(row=2) # 파일경로 view\n",
    "    Label(window, image=tmp_img).grid(row=3) #사진 view\n",
    "    Label(window, text=result).grid(row=4) # 예측 결과 출력    \n",
    "\n",
    "def open():\n",
    "    global my_image # 함수에서 이미지를 기억하도록 전역변수 선언 (안하면 사진이 안보임)\n",
    "    window.filename = filedialog.askopenfilename(initialdir='', title='파일선택', filetypes=(\n",
    "    ('png files', '*.png'), ('jpg files', '*.jpg'), ('all files', '*.*')))\n",
    " \n",
    "    Label(window, text=\"파일경로: \"+window.filename).grid(row=2) # 파일경로 view\n",
    "    \n",
    "    img = Image.open(window.filename)\n",
    "    my_image = ImageTk.PhotoImage(img)\n",
    "    \n",
    "    img=img.convert(\"L\")                         # gray 저장\n",
    "    img=np.invert(img)                           # 흑백을 반전\n",
    "    \n",
    "    # print(img.shape)                           # img의 shape 확인\n",
    "    img=img.reshape(-1, 1, 28, 28)\n",
    "    \n",
    "    Label(window, image=my_image).grid(row=3) #사진 view\n",
    "    \n",
    "    labels_view=['o', 'x', '△']\n",
    "\n",
    "    y=network.predict(img)\n",
    "    pred_num=np.argmax(y)\n",
    "    result = \"my predict is %s\"%(labels_view[pred_num])\n",
    "    Label(window, text=result).grid(row=4) # 예측 결과 출력\n",
    "    \n",
    "\n",
    "b_create=Button(window, text='그리기(아무키나 누르면 닫기)', command=crt).grid(row=0)\n",
    "b_open = Button(window, text='파일열기', command=open).grid(row=1)\n",
    "Label(window, text=\"파일 경로\").grid(row=2)\n",
    "Label(window).grid(row=3)\n",
    "Label(window, text=\"예측 결과\").grid(row=4)\n",
    "\n",
    "window.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
