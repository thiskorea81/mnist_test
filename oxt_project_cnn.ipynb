{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8oO8bawjMUs5"
   },
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "##########         피클 데이터를 이용하려면 아래 코드부터 실행하면 됨.                   ################\n",
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cWkFbl8iMUs6",
    "outputId": "6328c5d5-f0af-41a2-b4a3-61f9d43586e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data proofing\n",
      "(12000, 1, 28, 28) (12000,) (3000, 1, 28, 28) (3000,)\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import pickle\n",
    "\n",
    "with open('oxt_data_img_cnn.pkl', 'rb') as f1:\n",
    "    oxt_data1=pickle.load(f1)\n",
    "    \n",
    "print(\"Read data proofing\")\n",
    "x_train=oxt_data1[\"x_train\"]\n",
    "t_train=oxt_data1['t_train']\n",
    "x_test=oxt_data1['x_test']\n",
    "t_test=oxt_data1['t_test']\n",
    "\n",
    "print(x_train.shape, t_train.shape, x_test.shape, t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UngnXcjTMUs6"
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"단순한 합성곱 신경망\n",
    "    \n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다（수치미분）.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Hf0kab_0MUs7"
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.optimizer import *\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"신경망 훈련을 대신 해주는 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 optimizer='SGD', optimizer_param={'lr':0.01}, \n",
    "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
    "        self.network = network\n",
    "        self.verbose = verbose\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = mini_batch_size\n",
    "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
    "\n",
    "        # optimzer\n",
    "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
    "                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
    "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
    "        \n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = int(max(self.train_size / mini_batch_size, 1))\n",
    "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "        \n",
    "        grads = self.network.gradient(x_batch, t_batch)\n",
    "        self.optimizer.update(self.network.params, grads)\n",
    "        \n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "        if self.verbose: print(\"train loss:\" + str(loss))\n",
    "        \n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
    "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
    "            if not self.evaluate_sample_num_per_epoch is None:\n",
    "                t = self.evaluate_sample_num_per_epoch\n",
    "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
    "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
    "                \n",
    "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
    "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "\n",
    "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
    "            \n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "\n",
    "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(test_acc))\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KXhXJU07MUs8",
    "outputId": "2b007b18-1607-43a2-9450-a356e464edd0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 갯수: 15000 ( 5000 5000 5000 )\n",
      "15000\n",
      "0\n",
      "1\n",
      "2\n",
      "train loss:0.814686461774607\n",
      "=== epoch:1, train acc:0.484, test acc:0.483 ===\n",
      "train loss:0.7656378962462853\n",
      "train loss:0.6959002713612762\n",
      "train loss:0.5768914385221158\n",
      "train loss:0.5214377725455764\n",
      "train loss:0.45349782566458274\n",
      "train loss:0.4521956894090257\n",
      "train loss:0.585811991048049\n",
      "train loss:0.6297156697877223\n",
      "train loss:0.5466045038869692\n",
      "train loss:0.433530030444762\n",
      "train loss:0.5399220370849518\n",
      "train loss:0.421568249092105\n",
      "train loss:0.41988327696669253\n",
      "train loss:0.32290571202690915\n",
      "train loss:0.26151813430958926\n",
      "train loss:0.30270655674496727\n",
      "train loss:0.23169594960193382\n",
      "train loss:0.20339085636565957\n",
      "train loss:0.20980885587613934\n",
      "train loss:0.18913918907226318\n",
      "train loss:0.11920868311796266\n",
      "train loss:0.19558788007177239\n",
      "train loss:0.20436674038355984\n",
      "train loss:0.2227591916718754\n",
      "train loss:0.2408300467780943\n",
      "train loss:0.29139909401008846\n",
      "train loss:0.05230455289579945\n",
      "train loss:0.22001710943591007\n",
      "train loss:0.08298139030078473\n",
      "train loss:0.14476568839020215\n",
      "train loss:0.14388132733926293\n",
      "train loss:0.1906141460446368\n",
      "train loss:0.14013778662956428\n",
      "train loss:0.11216798621040976\n",
      "train loss:0.16728097762186128\n",
      "train loss:0.16642166994565152\n",
      "train loss:0.16083175786584888\n",
      "train loss:0.15584033700126873\n",
      "train loss:0.12071625815267562\n",
      "train loss:0.14826618778301603\n",
      "train loss:0.11032832009406615\n",
      "train loss:0.08106674300506272\n",
      "train loss:0.03484395085104726\n",
      "train loss:0.044711982634263715\n",
      "train loss:0.18695531556554695\n",
      "train loss:0.059335520762320605\n",
      "train loss:0.061981427703265826\n",
      "train loss:0.10206305486960388\n",
      "train loss:0.05217090650362021\n",
      "train loss:0.06298342677880465\n",
      "train loss:0.04165758948117434\n",
      "train loss:0.0826055575916588\n",
      "train loss:0.09108508471575295\n",
      "train loss:0.09594549149939681\n",
      "train loss:0.03288979525006266\n",
      "train loss:0.04857804154624838\n",
      "train loss:0.038485045901330464\n",
      "train loss:0.04646843057208896\n",
      "train loss:0.018426828211173175\n",
      "train loss:0.051278657515539165\n",
      "train loss:0.07138668033792568\n",
      "train loss:0.054168994931999975\n",
      "train loss:0.07882332171671509\n",
      "train loss:0.04396813331284538\n",
      "train loss:0.041369290275574296\n",
      "train loss:0.05350943756677354\n",
      "train loss:0.026851583883209254\n",
      "train loss:0.0585383542729178\n",
      "train loss:0.07216698241182962\n",
      "train loss:0.05094923511624093\n",
      "train loss:0.02090218353896773\n",
      "train loss:0.026334304632166884\n",
      "train loss:0.053667612540784664\n",
      "train loss:0.05536759761821504\n",
      "train loss:0.07967992245243416\n",
      "train loss:0.04574425563595427\n",
      "train loss:0.022109108429681775\n",
      "train loss:0.036598761078228036\n",
      "train loss:0.061643700085272576\n",
      "train loss:0.02531819163906298\n",
      "train loss:0.015865096326086913\n",
      "train loss:0.031233554913952993\n",
      "train loss:0.0389925939343289\n",
      "train loss:0.06058353162284888\n",
      "train loss:0.06938965052053651\n",
      "train loss:0.08493013058599697\n",
      "train loss:0.028047871027471402\n",
      "train loss:0.04587453267227593\n",
      "train loss:0.04514269065714651\n",
      "train loss:0.01818598913133451\n",
      "train loss:0.025903914223515657\n",
      "train loss:0.011984157804582807\n",
      "train loss:0.012106386826715426\n",
      "train loss:0.03152205718133062\n",
      "train loss:0.009669696205579885\n",
      "train loss:0.013267068722675149\n",
      "train loss:0.034972703093101\n",
      "train loss:0.05596287993662549\n",
      "train loss:0.04484284139041508\n",
      "train loss:0.04397423787072005\n",
      "train loss:0.04697995472998667\n",
      "train loss:0.01871151632715605\n",
      "train loss:0.05183739105607594\n",
      "train loss:0.006325370791645306\n",
      "train loss:0.04205429428468028\n",
      "train loss:0.039598840871294824\n",
      "train loss:0.016258218078558026\n",
      "train loss:0.06978112299387455\n",
      "train loss:0.022948346279755272\n",
      "train loss:0.031055472175361795\n",
      "train loss:0.015150478963673643\n",
      "train loss:0.020378019292931402\n",
      "train loss:0.019107732519647543\n",
      "train loss:0.06527291752732067\n",
      "train loss:0.03174025018379292\n",
      "train loss:0.008695669194144738\n",
      "train loss:0.007706747786739135\n",
      "train loss:0.02333367259516775\n",
      "train loss:0.022234226600951746\n",
      "train loss:0.014867680605289226\n",
      "=== epoch:2, train acc:0.987, test acc:0.987 ===\n",
      "train loss:0.022461942043287254\n",
      "train loss:0.08442493933910086\n",
      "train loss:0.017351832625004757\n",
      "train loss:0.033994953554173875\n",
      "train loss:0.03251019471641837\n",
      "train loss:0.05169368202026586\n",
      "train loss:0.012138828007354713\n",
      "train loss:0.010341619765805799\n",
      "train loss:0.042964890290582265\n",
      "train loss:0.05826909964474151\n",
      "train loss:0.031584739323669585\n",
      "train loss:0.01744269335557115\n",
      "train loss:0.024514647800381307\n",
      "train loss:0.04049137736641102\n",
      "train loss:0.024056908331343673\n",
      "train loss:0.006246735625052302\n",
      "train loss:0.005520145587632656\n",
      "train loss:0.024006187602523557\n",
      "train loss:0.014107076529485165\n",
      "train loss:0.017713668235866432\n",
      "train loss:0.012225713587282715\n",
      "train loss:0.010709150118372977\n",
      "train loss:0.007431814130331889\n",
      "train loss:0.024272673635577374\n",
      "train loss:0.005621616623466252\n",
      "train loss:0.04305664252859406\n",
      "train loss:0.00863728981143222\n",
      "train loss:0.006824802880509665\n",
      "train loss:0.006593044366736946\n",
      "train loss:0.005909122399450919\n",
      "train loss:0.006869864442258003\n",
      "train loss:0.0056668216467232815\n",
      "train loss:0.013303561650880142\n",
      "train loss:0.00941619648076231\n",
      "train loss:0.010460373467142254\n",
      "train loss:0.00676239425329462\n",
      "train loss:0.02125109738232484\n",
      "train loss:0.004123627693443975\n",
      "train loss:0.00648273322901174\n",
      "train loss:0.010519472140093961\n",
      "train loss:0.007747982171160838\n",
      "train loss:0.010429687939860415\n",
      "train loss:0.010755381345146713\n",
      "train loss:0.009303895869585264\n",
      "train loss:0.0029526888049957083\n",
      "train loss:0.013237651569859786\n",
      "train loss:0.012265418753951445\n",
      "train loss:0.01411672955501557\n",
      "train loss:0.0026037838052497954\n",
      "train loss:0.0020566861209747166\n",
      "train loss:0.009722767241607104\n",
      "train loss:0.012283709311956907\n",
      "train loss:0.005444674647081358\n",
      "train loss:0.016834873148361892\n",
      "train loss:0.0319438555530075\n",
      "train loss:0.0038347753431473043\n",
      "train loss:0.019121284132295184\n",
      "train loss:0.002474401271950544\n",
      "train loss:0.0136035934810464\n",
      "train loss:0.006661000778686684\n",
      "train loss:0.0025534048242289884\n",
      "train loss:0.004092237645211837\n",
      "train loss:0.007601540976225361\n",
      "train loss:0.0035572874231537825\n",
      "train loss:0.004859655636589134\n",
      "train loss:0.009862510334036862\n",
      "train loss:0.008345735209950102\n",
      "train loss:0.002186772971545792\n",
      "train loss:0.004879342641796451\n",
      "train loss:0.010246539158854508\n",
      "train loss:0.0033525019929102062\n",
      "train loss:0.0014052247519773606\n",
      "train loss:0.002729277481677189\n",
      "train loss:0.0019985712781123\n",
      "train loss:0.0035503556255145137\n",
      "train loss:0.003802477759702926\n",
      "train loss:0.00537108590021594\n",
      "train loss:0.009113633981223182\n",
      "train loss:0.003771419427588855\n",
      "train loss:0.006203246358485607\n",
      "train loss:0.005291050195713902\n",
      "train loss:0.001403004775878427\n",
      "train loss:0.00582656514850097\n",
      "train loss:0.008783831901396396\n",
      "train loss:0.0057627685414289136\n",
      "train loss:0.005614405448261866\n",
      "train loss:0.014369425248321435\n",
      "train loss:0.007204980576362964\n",
      "train loss:0.013272457078515414\n",
      "train loss:0.012071445859037721\n",
      "train loss:0.005727793539550605\n",
      "train loss:0.0024795702628761635\n",
      "train loss:0.003864734959605336\n",
      "train loss:0.0431439687337415\n",
      "train loss:0.0016045044348181833\n",
      "train loss:0.010887579176077357\n",
      "train loss:0.06381927489586148\n",
      "train loss:0.021783336434100405\n",
      "train loss:0.011330822696816123\n",
      "train loss:0.007486932179248983\n",
      "train loss:0.019462839396260277\n",
      "train loss:0.016877888614781705\n",
      "train loss:0.02296972629105796\n",
      "train loss:0.00815342481079891\n",
      "train loss:0.014444373494454705\n",
      "train loss:0.006751521472314112\n",
      "train loss:0.009026668705016625\n",
      "train loss:0.0019421847224145967\n",
      "train loss:0.004121518193507103\n",
      "train loss:0.0033714401358751894\n",
      "train loss:0.005631553253977312\n",
      "train loss:0.006195312516178521\n",
      "train loss:0.006377373790577627\n",
      "train loss:0.014270539507524714\n",
      "train loss:0.011417796212246565\n",
      "train loss:0.0027641475686823215\n",
      "train loss:0.010146779926400607\n",
      "train loss:0.002441126928586865\n",
      "train loss:0.002859219312656465\n",
      "train loss:0.011328494910632208\n",
      "=== epoch:3, train acc:0.996, test acc:0.992 ===\n",
      "train loss:0.016565901671528507\n",
      "train loss:0.001995739856272293\n",
      "train loss:0.007196203938685952\n",
      "train loss:0.006937623042552568\n",
      "train loss:0.006640367388852481\n",
      "train loss:0.006162043843813327\n",
      "train loss:0.003911928830551039\n",
      "train loss:0.017956408099842815\n",
      "train loss:0.0038115474371253084\n",
      "train loss:0.00514586152289245\n",
      "train loss:0.004269484090215141\n",
      "train loss:0.004016954721871565\n",
      "train loss:0.005828500134352268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.019705115709283284\n",
      "train loss:0.0024624531567743807\n",
      "train loss:0.0006972917314697308\n",
      "train loss:0.04042677008978556\n",
      "train loss:0.0032099052467480355\n",
      "train loss:0.0071439633241727\n",
      "train loss:0.0021577967313708636\n",
      "train loss:0.013336880093806913\n",
      "train loss:0.0036644061269676814\n",
      "train loss:0.006188847261375815\n",
      "train loss:0.003929485157675297\n",
      "train loss:0.002573214487594056\n",
      "train loss:0.0012951155824098691\n",
      "train loss:0.0021440451680750793\n",
      "train loss:0.012255060899665366\n",
      "train loss:0.005327317309982047\n",
      "train loss:0.010051808836668395\n",
      "train loss:0.0019872328363375004\n",
      "train loss:0.0013165596728519527\n",
      "train loss:0.0027717296194732376\n",
      "train loss:0.01953841780985357\n",
      "train loss:0.004941790995945466\n",
      "train loss:0.007131936183365777\n",
      "train loss:0.019403068105890563\n",
      "train loss:0.007966058119628822\n",
      "train loss:0.02056068626514874\n",
      "train loss:0.003199529328060486\n",
      "train loss:0.007665897193160771\n",
      "train loss:0.0006180461514632966\n",
      "train loss:0.004868417317169564\n",
      "train loss:0.0018330737154951132\n",
      "train loss:0.0033692064355221422\n",
      "train loss:0.008643271581401789\n",
      "train loss:0.009465383168757698\n",
      "train loss:0.009655331000101557\n",
      "train loss:0.004024490194102285\n",
      "train loss:0.00967641476163322\n",
      "train loss:0.0018822744072623054\n",
      "train loss:0.0019501459513972166\n",
      "train loss:0.012667424992596725\n",
      "train loss:0.030111122896170937\n",
      "train loss:0.001144356280236954\n",
      "train loss:0.003946658589560859\n",
      "train loss:0.0035970094504612403\n",
      "train loss:0.000759397437992798\n",
      "train loss:0.0014611416568720704\n",
      "train loss:0.0011249534328130703\n",
      "train loss:0.0042555775855423385\n",
      "train loss:0.008170299969809773\n",
      "train loss:0.004140728946731895\n",
      "train loss:0.0013352925231206222\n",
      "train loss:0.0009246134569381803\n",
      "train loss:0.0012001095562539314\n",
      "train loss:0.0036404379210538077\n",
      "train loss:0.0043051548086377196\n",
      "train loss:0.005758270050037778\n",
      "train loss:0.004224658901133879\n",
      "train loss:0.004611254467501161\n",
      "train loss:0.0013846227925875503\n",
      "train loss:0.0014059902205552147\n",
      "train loss:0.0030237728744443733\n",
      "train loss:0.027251580873593938\n",
      "train loss:0.0028069254245169516\n",
      "train loss:0.0009375379322182393\n",
      "train loss:0.002391966144659983\n",
      "train loss:0.004795252642323269\n",
      "train loss:0.007559599510419732\n",
      "train loss:0.005861161592183127\n",
      "train loss:0.005750278683502042\n",
      "train loss:0.000911745581060465\n",
      "train loss:0.0011992060336904148\n",
      "train loss:0.003991239649893231\n",
      "train loss:0.0019747979856718457\n",
      "train loss:0.0027945570486004327\n",
      "train loss:0.004270097859227183\n",
      "train loss:0.007252845003014597\n",
      "train loss:0.0017358388585409041\n",
      "train loss:0.0004097655792275862\n",
      "train loss:0.014292428807055259\n",
      "train loss:0.006387909149938674\n",
      "train loss:0.0038285659304270925\n",
      "train loss:0.0026845579296258222\n",
      "train loss:0.010852665980877488\n",
      "train loss:0.008935115481553763\n",
      "train loss:0.0031465389767672512\n",
      "train loss:0.006062467458265989\n",
      "train loss:0.0015034470186271731\n",
      "train loss:0.0005923085076988666\n",
      "train loss:0.003576147198630744\n",
      "train loss:0.00557051415445808\n",
      "train loss:0.010068909542596281\n",
      "train loss:0.022557028523964705\n",
      "train loss:0.0005332189067522724\n",
      "train loss:0.005713902658701909\n",
      "train loss:0.0033468142677305574\n",
      "train loss:0.0063796120489248735\n",
      "train loss:0.0020935140130684832\n",
      "train loss:0.001415819091656659\n",
      "train loss:0.0062975180106179755\n",
      "train loss:0.001990045728554239\n",
      "train loss:0.0051283389659252734\n",
      "train loss:0.004706279879631864\n",
      "train loss:0.002028195242413416\n",
      "train loss:0.0009978482598143124\n",
      "train loss:0.0006509909507889983\n",
      "train loss:0.0013292024183434554\n",
      "train loss:0.0023629271780106157\n",
      "=== epoch:4, train acc:0.997, test acc:0.997 ===\n",
      "train loss:0.0006892347715649646\n",
      "train loss:0.004762943437817651\n",
      "train loss:0.0019520415433065528\n",
      "train loss:0.004782117246232661\n",
      "train loss:0.0007691151518190298\n",
      "train loss:0.0010492429716053883\n",
      "train loss:0.0003667916300297342\n",
      "train loss:0.0019286012700501349\n",
      "train loss:0.0009077137093767821\n",
      "train loss:0.0033405674467926355\n",
      "train loss:0.002954120204367769\n",
      "train loss:0.0008562241858002493\n",
      "train loss:0.0048713779660880955\n",
      "train loss:0.001155807290451456\n",
      "train loss:0.005934588928359027\n",
      "train loss:0.0047608890993384\n",
      "train loss:0.001840813198723334\n",
      "train loss:0.0011104317149960028\n",
      "train loss:0.008474615255931595\n",
      "train loss:0.0011342256939908776\n",
      "train loss:0.002230294448102034\n",
      "train loss:0.001223445894049551\n",
      "train loss:0.006407461256621122\n",
      "train loss:0.004960606315971563\n",
      "train loss:0.0017941682543733991\n",
      "train loss:0.004390976993517277\n",
      "train loss:0.008818450156299383\n",
      "train loss:0.0013014614279048247\n",
      "train loss:0.0057090211559401504\n",
      "train loss:0.0068463338249468405\n",
      "train loss:0.0037202195451336113\n",
      "train loss:0.005530669664696525\n",
      "train loss:0.002115840486930756\n",
      "train loss:0.0020495579876519186\n",
      "train loss:0.00047254396192662165\n",
      "train loss:0.004736310584351899\n",
      "train loss:0.005112580116415449\n",
      "train loss:0.0012305900067172714\n",
      "train loss:0.002943709828351971\n",
      "train loss:0.004505731842406364\n",
      "train loss:0.0026669253451195233\n",
      "train loss:0.02834162593938291\n",
      "train loss:0.0018531992780746037\n",
      "train loss:0.004569095435679107\n",
      "train loss:0.006093523936371181\n",
      "train loss:0.003209662550705655\n",
      "train loss:0.002154304062585142\n",
      "train loss:0.009684962792798603\n",
      "train loss:0.0031451105877981366\n",
      "train loss:0.001194649705325892\n",
      "train loss:0.0018552605618509727\n",
      "train loss:0.001435879597044759\n",
      "train loss:0.0012032001049493025\n",
      "train loss:0.006156766633196734\n",
      "train loss:0.0011609151187493528\n",
      "train loss:0.0009333458872840832\n",
      "train loss:0.002751407880811564\n",
      "train loss:0.0018691374345220098\n",
      "train loss:0.0027092150465395943\n",
      "train loss:0.004384528589219253\n",
      "train loss:0.0029530277243738044\n",
      "train loss:0.0008125018749171976\n",
      "train loss:0.002482702011263808\n",
      "train loss:0.0006948156159564156\n",
      "train loss:0.003762561477192146\n",
      "train loss:0.003579100514787014\n",
      "train loss:0.00637162105118127\n",
      "train loss:0.0014493221808479935\n",
      "train loss:0.0005637118867041947\n",
      "train loss:0.0020037726253116835\n",
      "train loss:0.0007724235781413363\n",
      "train loss:0.001176163938367018\n",
      "train loss:0.0010240687944145926\n",
      "train loss:0.004229203667044558\n",
      "train loss:0.00032347984474028775\n",
      "train loss:0.005902976972659978\n",
      "train loss:0.05984808731255917\n",
      "train loss:0.0038015562966037713\n",
      "train loss:0.00034254588053151064\n",
      "train loss:0.011073595941007852\n",
      "train loss:0.001962997129361231\n",
      "train loss:0.0023379526033823488\n",
      "train loss:0.0006574371764927033\n",
      "train loss:0.0006658123660568529\n",
      "train loss:0.007032216946247857\n",
      "train loss:0.004808345416399273\n",
      "train loss:0.0011708179025046572\n",
      "train loss:0.005745215531635748\n",
      "train loss:0.002938888823696123\n",
      "train loss:0.0018718863890337212\n",
      "train loss:0.0014367355194791528\n",
      "train loss:0.0015989505573987333\n",
      "train loss:0.00043458877962274073\n",
      "train loss:0.0035640742637511906\n",
      "train loss:0.0021701310725121166\n",
      "train loss:0.001132899552169018\n",
      "train loss:0.0010267720817764172\n",
      "train loss:0.0024402728305570357\n",
      "train loss:0.002620132156828916\n",
      "train loss:0.0009170499107972746\n",
      "train loss:0.0008973068270288674\n",
      "train loss:0.0003713040591249083\n",
      "train loss:0.0009269330105413505\n",
      "train loss:0.0005099270931756834\n",
      "train loss:0.020453332589561998\n",
      "train loss:0.0032240533485462857\n",
      "train loss:0.0033064410167019877\n",
      "train loss:0.0010031178551272004\n",
      "train loss:0.002823233775940164\n",
      "train loss:0.00018967093624018137\n",
      "train loss:0.0003131201213204847\n",
      "train loss:0.0038863548445093542\n",
      "train loss:0.0015028088588720115\n",
      "train loss:0.0006743094293696663\n",
      "train loss:0.0013326620354314873\n",
      "train loss:0.0015724158733849355\n",
      "train loss:0.00020486443311872057\n",
      "train loss:0.0013337189727519782\n",
      "train loss:0.0013886750986584378\n",
      "train loss:0.0002601786641483521\n",
      "=== epoch:5, train acc:0.998, test acc:0.996 ===\n",
      "train loss:9.616341095602033e-05\n",
      "train loss:0.002363234866987272\n",
      "train loss:0.0022861672867308443\n",
      "train loss:0.00020182058262301138\n",
      "train loss:0.00074351871422745\n",
      "train loss:0.0006961308914367377\n",
      "train loss:0.0024380889265855506\n",
      "train loss:0.0002221009058918149\n",
      "train loss:0.0027077593024772877\n",
      "train loss:6.956326103878347e-05\n",
      "train loss:0.00021375344004207725\n",
      "train loss:0.0005601121856286512\n",
      "train loss:0.00016184236502899028\n",
      "train loss:0.000890953059892516\n",
      "train loss:0.0019736966314797253\n",
      "train loss:0.000962716811205079\n",
      "train loss:0.002193464847813502\n",
      "train loss:0.0047256514719338375\n",
      "train loss:0.0007339732843896921\n",
      "train loss:0.0004618210631466714\n",
      "train loss:0.0010587642223033047\n",
      "train loss:0.0012402078095123223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0008192308996536178\n",
      "train loss:0.0028428193867075804\n",
      "train loss:0.0034948800413741083\n",
      "train loss:0.0015920283653236998\n",
      "train loss:0.0011223218361350092\n",
      "train loss:0.0011169437490842029\n",
      "train loss:0.0003481027280666645\n",
      "train loss:0.0008392638243940743\n",
      "train loss:0.0011199646951016858\n",
      "train loss:0.004127529211005756\n",
      "train loss:0.0008778426266067963\n",
      "train loss:0.0004741346647366874\n",
      "train loss:0.000701240581924936\n",
      "train loss:0.00124529139113265\n",
      "train loss:0.0001289664424773388\n",
      "train loss:0.0009600818177519236\n",
      "train loss:0.0009091787295714872\n",
      "train loss:0.0006888359300027075\n",
      "train loss:0.0012315036339844924\n",
      "train loss:0.0007081572585567309\n",
      "train loss:0.0013351525400322037\n",
      "train loss:0.0005849507742794459\n",
      "train loss:0.0009377522691588331\n",
      "train loss:0.0005559412240914928\n",
      "train loss:0.0005534743352670515\n",
      "train loss:0.0029058190981585087\n",
      "train loss:0.0016637641877634646\n",
      "train loss:0.0008521383300495345\n",
      "train loss:0.0008300754975225913\n",
      "train loss:0.0013833002501402896\n",
      "train loss:0.0005782484138688803\n",
      "train loss:0.002726565966192231\n",
      "train loss:0.003614001427383147\n",
      "train loss:0.0011377681438714208\n",
      "train loss:0.0009413295839361111\n",
      "train loss:0.0014166974137580265\n",
      "train loss:0.0024059097696550688\n",
      "train loss:0.002509467020736854\n",
      "train loss:0.0065721275489319395\n",
      "train loss:0.007855089717409915\n",
      "train loss:0.0022019200583574166\n",
      "train loss:0.0016270753359049247\n",
      "train loss:0.006132031863903424\n",
      "train loss:0.0030083210807950135\n",
      "train loss:0.002392710778682331\n",
      "train loss:0.00034146731222496396\n",
      "train loss:0.003687367168367274\n",
      "train loss:0.0019790604188198433\n",
      "train loss:0.004950490232838543\n",
      "train loss:0.012932204789190032\n",
      "train loss:0.0028846501797048503\n",
      "train loss:0.0012045361702683042\n",
      "train loss:0.0007178965351809312\n",
      "train loss:0.000809597433890968\n",
      "train loss:0.003256307786735334\n",
      "train loss:0.00044690337780360444\n",
      "train loss:0.00036452899305774127\n",
      "train loss:0.0013680332565941403\n",
      "train loss:0.0012221932255129353\n",
      "train loss:0.0008128141073082265\n",
      "train loss:0.0009684837328795183\n",
      "train loss:0.0013225153453050973\n",
      "train loss:0.000939193618609072\n",
      "train loss:0.0002361900081115769\n",
      "train loss:0.0010121678040699539\n",
      "train loss:0.0005243892425527074\n",
      "train loss:0.0007324462247098039\n",
      "train loss:0.0013777114272490656\n",
      "train loss:0.0004401696371422218\n",
      "train loss:0.00041797429729156565\n",
      "train loss:0.00020613823995847336\n",
      "train loss:0.0016770958643945017\n",
      "train loss:0.0008556198426763117\n",
      "train loss:0.002633252345989329\n",
      "train loss:0.0010986176894688148\n",
      "train loss:0.0004145695648102587\n",
      "train loss:0.0018364993365960986\n",
      "train loss:0.002127888116464552\n",
      "train loss:0.0008457100720215843\n",
      "train loss:0.0017606019316285615\n",
      "train loss:0.000539431280005325\n",
      "train loss:0.00015986662119770164\n",
      "train loss:0.0009762537197225897\n",
      "train loss:0.0014199745681823985\n",
      "train loss:0.0034629993730626836\n",
      "train loss:0.0012111287658376482\n",
      "train loss:0.0002155394982880788\n",
      "train loss:0.001416300375412877\n",
      "train loss:0.004021300907809208\n",
      "train loss:0.0015896492050044972\n",
      "train loss:0.0015407492144769751\n",
      "train loss:0.0027668848997857505\n",
      "train loss:0.0031412318102925583\n",
      "train loss:0.01927411365743956\n",
      "train loss:0.0025964554441466462\n",
      "train loss:0.0007453197913571993\n",
      "train loss:0.007525974044223671\n",
      "train loss:0.0005550483078903521\n",
      "=== epoch:6, train acc:0.999, test acc:0.996 ===\n",
      "train loss:0.0006585856736133953\n",
      "train loss:0.00016141428898159974\n",
      "train loss:0.0018351243479574977\n",
      "train loss:0.0016877185924489139\n",
      "train loss:0.0025600554115509573\n",
      "train loss:7.09754370651394e-05\n",
      "train loss:0.0005315511164327386\n",
      "train loss:0.00038476851586155265\n",
      "train loss:0.0009509490222771194\n",
      "train loss:0.018208006445044295\n",
      "train loss:0.001085132481545696\n",
      "train loss:0.0005538968214138739\n",
      "train loss:0.0006289481465707351\n",
      "train loss:0.0008364343407037619\n",
      "train loss:0.000686692071262621\n",
      "train loss:0.0003402460934728662\n",
      "train loss:0.0014759243436617768\n",
      "train loss:0.00012280763873241267\n",
      "train loss:0.0003018958982904391\n",
      "train loss:0.0001413943955201207\n",
      "train loss:0.0010229426159544374\n",
      "train loss:0.0007205963908954374\n",
      "train loss:0.00016639913202560222\n",
      "train loss:0.000419543921283819\n",
      "train loss:0.0013749570760603655\n",
      "train loss:0.0006357435294774609\n",
      "train loss:0.0007853184668661331\n",
      "train loss:0.00025639579394145124\n",
      "train loss:0.00035218474504694104\n",
      "train loss:0.000671526835101849\n",
      "train loss:0.0017832950955649612\n",
      "train loss:0.002845655998759492\n",
      "train loss:0.0016639815374982416\n",
      "train loss:0.00043477619898478774\n",
      "train loss:0.0023732375892005463\n",
      "train loss:0.00025427193216524243\n",
      "train loss:0.0005104782390196217\n",
      "train loss:0.0011230118854210509\n",
      "train loss:0.00023234370351779345\n",
      "train loss:0.00057723359077436\n",
      "train loss:0.00025958166892481777\n",
      "train loss:0.0010766619358727793\n",
      "train loss:0.0005594764456181496\n",
      "train loss:5.309460974902561e-05\n",
      "train loss:6.278791846715004e-05\n",
      "train loss:0.00025958421027396023\n",
      "train loss:0.0009893262905959653\n",
      "train loss:0.00013796767064714704\n",
      "train loss:0.0017341160558901022\n",
      "train loss:0.000461571030888811\n",
      "train loss:0.0004159406229771586\n",
      "train loss:4.268624831260375e-05\n",
      "train loss:0.0002832414821482541\n",
      "train loss:0.0001433406497107133\n",
      "train loss:0.0002835695163162733\n",
      "train loss:8.71411408018901e-05\n",
      "train loss:7.585497472331908e-05\n",
      "train loss:0.00024868079160557686\n",
      "train loss:0.00020342205910390973\n",
      "train loss:0.00014664830557666037\n",
      "train loss:0.0007908785210527379\n",
      "train loss:0.00012089708569949185\n",
      "train loss:0.00026830244981754237\n",
      "train loss:0.00016688248609305164\n",
      "train loss:0.0019137059536553855\n",
      "train loss:0.00047038447761260584\n",
      "train loss:0.0009447683285433895\n",
      "train loss:0.00019271720346503659\n",
      "train loss:0.0016748019494844855\n",
      "train loss:0.0004906917759267497\n",
      "train loss:9.051238848103684e-05\n",
      "train loss:0.00015391761356352057\n",
      "train loss:7.800232250946724e-05\n",
      "train loss:0.0004265586204577465\n",
      "train loss:0.0002474954180602577\n",
      "train loss:0.0005293827989472218\n",
      "train loss:0.0003980572801859392\n",
      "train loss:7.645926638777362e-05\n",
      "train loss:0.00014536405888231115\n",
      "train loss:4.786494239235998e-05\n",
      "train loss:0.001000257631761241\n",
      "train loss:0.0011479578381884518\n",
      "train loss:0.00031418123874186977\n",
      "train loss:3.9510587440535715e-05\n",
      "train loss:4.2850756766217845e-05\n",
      "train loss:7.862619844851194e-05\n",
      "train loss:0.0001446608225053991\n",
      "train loss:0.0005610489573106469\n",
      "train loss:0.0009828654046464718\n",
      "train loss:0.00011197601819118231\n",
      "train loss:0.0001415655069479371\n",
      "train loss:0.0003395566115274847\n",
      "train loss:9.911371081677425e-05\n",
      "train loss:0.00010786460157056709\n",
      "train loss:0.02796371734551351\n",
      "train loss:0.0005507418520996202\n",
      "train loss:0.001633579183034647\n",
      "train loss:0.0005976184299579114\n",
      "train loss:0.0024020806670359022\n",
      "train loss:0.02620123449048717\n",
      "train loss:0.0004293124335206771\n",
      "train loss:0.0003584708481439367\n",
      "train loss:0.00016498068066808427\n",
      "train loss:0.0011297830207114843\n",
      "train loss:0.00013292534635767384\n",
      "train loss:0.00019837641768983584\n",
      "train loss:0.0010685780870839426\n",
      "train loss:0.0002639739838799923\n",
      "train loss:0.00024168932797524878\n",
      "train loss:0.0032628480201144106\n",
      "train loss:0.00021983899042517517\n",
      "train loss:0.00104521793458698\n",
      "train loss:0.0001828200457618709\n",
      "train loss:0.00024128530399111252\n",
      "train loss:0.001150920877481533\n",
      "train loss:0.000730741865708852\n",
      "train loss:0.0008143650932104704\n",
      "train loss:0.001982415205344403\n",
      "train loss:0.0001915223383126189\n",
      "train loss:9.336565422788076e-05\n",
      "=== epoch:7, train acc:0.999, test acc:0.996 ===\n",
      "train loss:0.0002150711538112549\n",
      "train loss:0.0002253587116333711\n",
      "train loss:0.0002552070264075658\n",
      "train loss:0.0004212297264552852\n",
      "train loss:0.0008551401195855893\n",
      "train loss:0.00011725724929998448\n",
      "train loss:0.000565859093239049\n",
      "train loss:0.0026651592610606846\n",
      "train loss:0.0003285589989908943\n",
      "train loss:0.00047734478329508064\n",
      "train loss:0.0011334887891607446\n",
      "train loss:0.0003543726236649217\n",
      "train loss:0.00030215614701553954\n",
      "train loss:0.003106352226390469\n",
      "train loss:0.0006819113392710288\n",
      "train loss:0.0010004943251012378\n",
      "train loss:0.0015655124580754346\n",
      "train loss:0.0001563023260947626\n",
      "train loss:8.04190887820131e-05\n",
      "train loss:0.0013107776079690494\n",
      "train loss:0.0009587312692594662\n",
      "train loss:0.0004047161321070058\n",
      "train loss:0.0003991146277354739\n",
      "train loss:0.000491597195526703\n",
      "train loss:0.0003074764374204452\n",
      "train loss:0.00011100675947638814\n",
      "train loss:0.0016826904930841216\n",
      "train loss:3.1158747833116046e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:4.3943393138180616e-05\n",
      "train loss:0.00013361481466020578\n",
      "train loss:0.0004016456022390694\n",
      "train loss:0.0005572127794819819\n",
      "train loss:0.00023274507298993503\n",
      "train loss:0.00026767277697653486\n",
      "train loss:0.0013091201475660622\n",
      "train loss:0.0008675746673183616\n",
      "train loss:0.0010811540461401728\n",
      "train loss:4.861573215274466e-05\n",
      "train loss:0.00029701739072214437\n",
      "train loss:0.0005312465646257322\n",
      "train loss:0.0002552295096314734\n",
      "train loss:0.0005866690838344627\n",
      "train loss:0.00029082268746391017\n",
      "train loss:5.426745075724739e-05\n",
      "train loss:3.15668548101456e-05\n",
      "train loss:0.0005824040506379527\n",
      "train loss:0.0002488168380593626\n",
      "train loss:5.925050986998861e-05\n",
      "train loss:0.00010449659429040031\n",
      "train loss:3.155314904297456e-05\n",
      "train loss:0.0004229008138072019\n",
      "train loss:0.0006263812356644073\n",
      "train loss:0.0001727043678090815\n",
      "train loss:3.5288178240233653e-05\n",
      "train loss:2.5507240359770287e-05\n",
      "train loss:0.00021216070437065236\n",
      "train loss:0.0005015060734677472\n",
      "train loss:0.0007873931532369445\n",
      "train loss:0.0001339376077196942\n",
      "train loss:0.0002269382194731005\n",
      "train loss:4.3148133313565835e-05\n",
      "train loss:0.0003620082643677809\n",
      "train loss:0.00024263220474343296\n",
      "train loss:0.00020935800850481617\n",
      "train loss:0.0004881749452382557\n",
      "train loss:0.00018960910504863218\n",
      "train loss:0.00017067908957567326\n",
      "train loss:0.0004563075931259564\n",
      "train loss:0.00015480458221815322\n",
      "train loss:0.0001264472809134596\n",
      "train loss:0.000944117928330271\n",
      "train loss:0.001171934897339614\n",
      "train loss:0.00024252021539629547\n",
      "train loss:0.0010101968750106441\n",
      "train loss:5.498855441411656e-05\n",
      "train loss:0.00015622384124577913\n",
      "train loss:3.837762687505646e-05\n",
      "train loss:1.9227657916189577e-05\n",
      "train loss:2.639120272478536e-05\n",
      "train loss:5.8802764954695904e-05\n",
      "train loss:0.000424881315417081\n",
      "train loss:0.00024243771647442132\n",
      "train loss:3.7162293953678735e-05\n",
      "train loss:0.00038219318922393005\n",
      "train loss:5.549926480862597e-05\n",
      "train loss:0.00012767985295232534\n",
      "train loss:0.0005380300373608789\n",
      "train loss:7.260229364131009e-05\n",
      "train loss:0.000980700172575121\n",
      "train loss:4.839453359248242e-05\n",
      "train loss:0.0007678499519241711\n",
      "train loss:1.6434464983037e-05\n",
      "train loss:3.9275987200109934e-05\n",
      "train loss:0.00014787088480476264\n",
      "train loss:0.0004666756652843805\n",
      "train loss:7.619577381726513e-05\n",
      "train loss:0.0004681690948984152\n",
      "train loss:4.356430823362727e-05\n",
      "train loss:0.000462528616236962\n",
      "train loss:0.00040282416744736314\n",
      "train loss:0.00019864956770296638\n",
      "train loss:0.0009753091117781073\n",
      "train loss:0.0014271225505223045\n",
      "train loss:0.0006512522993753614\n",
      "train loss:0.0007796742723615532\n",
      "train loss:0.0004138449744994124\n",
      "train loss:3.3356236434900816e-05\n",
      "train loss:0.000392079793267416\n",
      "train loss:8.216470699963285e-05\n",
      "train loss:9.304842319137206e-05\n",
      "train loss:0.0005275912857433384\n",
      "train loss:3.775706415053079e-05\n",
      "train loss:0.00017238072160087142\n",
      "train loss:0.00033249555903447825\n",
      "train loss:0.00021064439496632856\n",
      "train loss:0.0007427135394693699\n",
      "train loss:0.00010563791568536734\n",
      "train loss:0.0003737454338826409\n",
      "train loss:6.35954968984291e-05\n",
      "train loss:0.0003048084493977391\n",
      "=== epoch:8, train acc:1.0, test acc:0.993 ===\n",
      "train loss:0.00031557334862060833\n",
      "train loss:8.594775882686631e-05\n",
      "train loss:0.00044437570601546023\n",
      "train loss:5.9909062006490164e-05\n",
      "train loss:0.00044017950895009384\n",
      "train loss:0.00038699642363402225\n",
      "train loss:4.749028176856578e-05\n",
      "train loss:0.00021323614550947513\n",
      "train loss:0.0008485567639932931\n",
      "train loss:3.5208806268313065e-05\n",
      "train loss:0.00011759058253806784\n",
      "train loss:0.00019193734815658534\n",
      "train loss:0.0004715197975336778\n",
      "train loss:5.62986076552447e-05\n",
      "train loss:3.410385574541873e-05\n",
      "train loss:0.00012008053599154741\n",
      "train loss:3.644050033600115e-05\n",
      "train loss:0.0004407709181738409\n",
      "train loss:7.587059725024538e-05\n",
      "train loss:1.7464404170580417e-05\n",
      "train loss:3.199600025860119e-05\n",
      "train loss:0.00015740489050901043\n",
      "train loss:0.0001030343943061724\n",
      "train loss:3.830949743597986e-05\n",
      "train loss:0.0007337510631525706\n",
      "train loss:0.00047997938522247307\n",
      "train loss:0.00020682010607864048\n",
      "train loss:1.1655506717090938e-05\n",
      "train loss:0.0004150510619648573\n",
      "train loss:0.00010018524582641394\n",
      "train loss:6.86962034740155e-05\n",
      "train loss:4.242529329873461e-05\n",
      "train loss:0.0002641387384139986\n",
      "train loss:5.2158400368612596e-05\n",
      "train loss:0.0001115759161345812\n",
      "train loss:2.1153391831634137e-05\n",
      "train loss:3.894301472510283e-05\n",
      "train loss:4.177798416690425e-05\n",
      "train loss:0.00019603199743987282\n",
      "train loss:6.37375572602604e-05\n",
      "train loss:9.185153197134596e-05\n",
      "train loss:0.00016858533366674925\n",
      "train loss:0.00011348018758557165\n",
      "train loss:2.2636300971970502e-05\n",
      "train loss:3.0534847948041944e-05\n",
      "train loss:5.902354302204409e-05\n",
      "train loss:1.577570000564734e-05\n",
      "train loss:5.8173562020679724e-05\n",
      "train loss:0.00022018199187151159\n",
      "train loss:1.4484438135169511e-05\n",
      "train loss:2.8896132029364176e-05\n",
      "train loss:0.00025888248221770597\n",
      "train loss:5.1201277845202974e-05\n",
      "train loss:5.736762543008099e-05\n",
      "train loss:4.397125359981041e-05\n",
      "train loss:2.4665281958532282e-05\n",
      "train loss:0.00013509636913310247\n",
      "train loss:4.9706804497663794e-05\n",
      "train loss:1.4240203453046417e-05\n",
      "train loss:1.2394203188584448e-05\n",
      "train loss:5.506693694222263e-05\n",
      "train loss:0.0002921133115799671\n",
      "train loss:0.00010130316510644837\n",
      "train loss:0.0008100081699380155\n",
      "train loss:1.0373866132237279e-05\n",
      "train loss:7.481505275780496e-05\n",
      "train loss:4.146922455003697e-05\n",
      "train loss:2.2457696565346756e-05\n",
      "train loss:0.000499228417007546\n",
      "train loss:3.508364310130026e-05\n",
      "train loss:8.138213143978591e-05\n",
      "train loss:7.046359857850658e-05\n",
      "train loss:3.9052208399428504e-05\n",
      "train loss:0.00015459051571603622\n",
      "train loss:2.6876656759818104e-05\n",
      "train loss:8.084911909147633e-05\n",
      "train loss:8.215760623453731e-05\n",
      "train loss:3.362643990440204e-05\n",
      "train loss:0.000131573537209178\n",
      "train loss:2.6115516066021236e-05\n",
      "train loss:8.427078332678884e-05\n",
      "train loss:6.974738600841479e-05\n",
      "train loss:2.117072490190185e-05\n",
      "train loss:7.677727743127214e-05\n",
      "train loss:1.4588579513698424e-05\n",
      "train loss:1.772493016444857e-05\n",
      "train loss:6.347747121520778e-05\n",
      "train loss:0.0001285789752886375\n",
      "train loss:0.00012980027968097194\n",
      "train loss:4.285826060002708e-05\n",
      "train loss:7.851128067010321e-05\n",
      "train loss:3.47143102216873e-05\n",
      "train loss:0.00012789488856287205\n",
      "train loss:0.0003949512275898649\n",
      "train loss:0.0004497004369520091\n",
      "train loss:3.915103809908893e-05\n",
      "train loss:1.4725672347557266e-05\n",
      "train loss:0.00010836075465058683\n",
      "train loss:2.5041674414578945e-05\n",
      "train loss:3.759632541222466e-05\n",
      "train loss:3.79403353449367e-05\n",
      "train loss:6.073114936634207e-05\n",
      "train loss:7.611831709895505e-05\n",
      "train loss:5.5283183350982426e-05\n",
      "train loss:4.316091708668243e-05\n",
      "train loss:0.00010699865343754365\n",
      "train loss:7.961225908789684e-05\n",
      "train loss:0.00011682223075148174\n",
      "train loss:0.0002421761995178723\n",
      "train loss:0.00019308266043654044\n",
      "train loss:2.178015118904098e-05\n",
      "train loss:7.458571423354029e-05\n",
      "train loss:9.004589781865912e-06\n",
      "train loss:0.0007120039222082692\n",
      "train loss:0.00019765739656165044\n",
      "train loss:0.00014716748449263714\n",
      "train loss:0.00033426043320953483\n",
      "train loss:0.00013256096803795236\n",
      "train loss:0.0013186588041251771\n",
      "train loss:0.0007787123838533111\n",
      "=== epoch:9, train acc:0.999, test acc:0.997 ===\n",
      "train loss:0.0007363570550398869\n",
      "train loss:0.00012390866082351344\n",
      "train loss:0.00010096070853007942\n",
      "train loss:8.707938814261295e-05\n",
      "train loss:7.508706414408122e-05\n",
      "train loss:0.00014083374238282612\n",
      "train loss:9.718970015402918e-05\n",
      "train loss:0.000853513145417514\n",
      "train loss:0.00016098351081794763\n",
      "train loss:0.0007028570895164305\n",
      "train loss:0.00029322764323361255\n",
      "train loss:5.659513713388671e-05\n",
      "train loss:0.0009975190641232806\n",
      "train loss:8.966807985258805e-05\n",
      "train loss:0.00013289693550418564\n",
      "train loss:0.0003460926788249866\n",
      "train loss:0.00019705127597706697\n",
      "train loss:0.00030076547425332746\n",
      "train loss:5.120274849247207e-05\n",
      "train loss:2.4477812447667183e-05\n",
      "train loss:1.4504109631236988e-05\n",
      "train loss:4.256848931072795e-05\n",
      "train loss:5.484139122576389e-05\n",
      "train loss:2.2808861363967802e-05\n",
      "train loss:0.0002782420810723893\n",
      "train loss:0.000258022551882395\n",
      "train loss:1.3136178677853479e-05\n",
      "train loss:0.00012957970632367983\n",
      "train loss:4.306523390776546e-05\n",
      "train loss:6.509526320419267e-06\n",
      "train loss:3.500701226204437e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00011008004543561694\n",
      "train loss:3.8738261223777466e-05\n",
      "train loss:3.6471406586822895e-05\n",
      "train loss:8.774714667700858e-05\n",
      "train loss:2.9102865401894277e-05\n",
      "train loss:1.4736344449845256e-05\n",
      "train loss:1.352419485493397e-05\n",
      "train loss:1.1265239505169513e-05\n",
      "train loss:7.520508327829975e-06\n",
      "train loss:6.199366929103947e-05\n",
      "train loss:1.677025220343766e-05\n",
      "train loss:7.515804595373043e-06\n",
      "train loss:3.268371173580874e-06\n",
      "train loss:0.00036769233730213075\n",
      "train loss:9.890923297294825e-05\n",
      "train loss:2.7434067253983268e-05\n",
      "train loss:1.577994575413721e-05\n",
      "train loss:0.00031358265520546017\n",
      "train loss:0.00017421417174178888\n",
      "train loss:5.343178802823424e-06\n",
      "train loss:1.3076085823997086e-05\n",
      "train loss:1.441684509157251e-05\n",
      "train loss:7.38974665583591e-05\n",
      "train loss:0.00015360535194355526\n",
      "train loss:6.320040148355652e-05\n",
      "train loss:1.509214689709476e-05\n",
      "train loss:3.4814081481167864e-05\n",
      "train loss:0.0003248577021070708\n",
      "train loss:2.2490959955746912e-05\n",
      "train loss:0.00014152120974256558\n",
      "train loss:0.00015516069885155315\n",
      "train loss:0.00033797292655284283\n",
      "train loss:1.8240386143696193e-05\n",
      "train loss:1.1400898006646797e-05\n",
      "train loss:0.00017364416572733431\n",
      "train loss:7.894864025471718e-05\n",
      "train loss:2.2751311941103532e-05\n",
      "train loss:1.178594078611827e-05\n",
      "train loss:0.0003455415747470964\n",
      "train loss:5.071938034956447e-05\n",
      "train loss:0.0002787808742307592\n",
      "train loss:1.3827577102095055e-05\n",
      "train loss:1.3801092780183823e-05\n",
      "train loss:1.5457371369129233e-05\n",
      "train loss:2.5547972623560347e-05\n",
      "train loss:0.0001296152716404848\n",
      "train loss:5.4874439421255706e-05\n",
      "train loss:2.6002465864672745e-05\n",
      "train loss:8.874668475874688e-05\n",
      "train loss:3.70308778321203e-05\n",
      "train loss:6.2089715722056e-05\n",
      "train loss:2.5433451140747327e-05\n",
      "train loss:2.2317995007586954e-05\n",
      "train loss:1.3758501777269511e-05\n",
      "train loss:5.403731245265683e-05\n",
      "train loss:1.9577659201944474e-05\n",
      "train loss:9.099109996492357e-05\n",
      "train loss:2.0481468631622944e-05\n",
      "train loss:5.433872953763121e-05\n",
      "train loss:1.7414848484300482e-05\n",
      "train loss:0.0001386970862639058\n",
      "train loss:1.7359484028348107e-05\n",
      "train loss:2.278743523383757e-05\n",
      "train loss:0.00021274479535414315\n",
      "train loss:4.6955386404453485e-05\n",
      "train loss:2.564972499055218e-05\n",
      "train loss:0.00029240057294235204\n",
      "train loss:2.2185468103590434e-05\n",
      "train loss:4.045695776567962e-05\n",
      "train loss:0.00012757160268465976\n",
      "train loss:8.049814918200575e-06\n",
      "train loss:0.00011875048380166881\n",
      "train loss:9.009329761948854e-05\n",
      "train loss:3.785032504183749e-05\n",
      "train loss:1.1154045937581505e-05\n",
      "train loss:1.4491785330796552e-05\n",
      "train loss:7.271266024509145e-05\n",
      "train loss:1.8094465801581178e-05\n",
      "train loss:0.00020579396501971302\n",
      "train loss:9.742930227596559e-06\n",
      "train loss:0.000136721560079716\n",
      "train loss:1.2650102564963213e-05\n",
      "train loss:5.314679642033105e-05\n",
      "train loss:1.0397129164250588e-05\n",
      "train loss:4.192596518815431e-05\n",
      "train loss:5.138274501563331e-06\n",
      "train loss:1.2591044974922077e-05\n",
      "train loss:2.721283303409159e-05\n",
      "train loss:5.4080018003570786e-05\n",
      "=== epoch:10, train acc:1.0, test acc:0.998 ===\n",
      "train loss:2.097732370658099e-05\n",
      "train loss:1.791839491149033e-05\n",
      "train loss:2.6777422869768676e-05\n",
      "train loss:0.00016870055503035092\n",
      "train loss:7.024899772662717e-05\n",
      "train loss:6.257501590046098e-05\n",
      "train loss:4.642136533308887e-06\n",
      "train loss:4.370706026748415e-05\n",
      "train loss:8.447379685710099e-05\n",
      "train loss:0.00011399494933166747\n",
      "train loss:6.593833707076064e-05\n",
      "train loss:9.595424339351037e-06\n",
      "train loss:2.1885540990794044e-05\n",
      "train loss:6.36606195910179e-06\n",
      "train loss:3.7657990112038685e-05\n",
      "train loss:1.8937969458103376e-05\n",
      "train loss:2.4554473099186606e-05\n",
      "train loss:0.00010050203645763553\n",
      "train loss:1.2577813779982233e-05\n",
      "train loss:2.942889385960528e-05\n",
      "train loss:3.7916284743577195e-05\n",
      "train loss:4.414244784817534e-06\n",
      "train loss:2.013900364039397e-05\n",
      "train loss:0.00016436699559258745\n",
      "train loss:1.1940021608776459e-05\n",
      "train loss:4.727404223415664e-06\n",
      "train loss:9.897648467110883e-05\n",
      "train loss:0.00012088814395436099\n",
      "train loss:4.116233826242371e-05\n",
      "train loss:9.358544701776072e-05\n",
      "train loss:1.4225621800066775e-05\n",
      "train loss:3.3842674298801676e-05\n",
      "train loss:1.1295772096583817e-05\n",
      "train loss:2.04132076010396e-05\n",
      "train loss:5.347863682656786e-05\n",
      "train loss:4.212743671030179e-05\n",
      "train loss:1.0264902234279462e-05\n",
      "train loss:1.1860031645044148e-05\n",
      "train loss:6.050513513310515e-05\n",
      "train loss:2.605613242772659e-05\n",
      "train loss:5.2548959444405684e-05\n",
      "train loss:1.7562424460990328e-05\n",
      "train loss:1.567531525562053e-05\n",
      "train loss:2.8975981736266584e-05\n",
      "train loss:1.1903550012116834e-05\n",
      "train loss:4.2740244938663117e-07\n",
      "train loss:9.2732083123553e-05\n",
      "train loss:9.897125239791296e-06\n",
      "train loss:8.618214503955237e-06\n",
      "train loss:9.072549095419926e-06\n",
      "train loss:8.480901799432838e-06\n",
      "train loss:2.243257358613629e-05\n",
      "train loss:4.920891878300733e-06\n",
      "train loss:9.226026303659752e-06\n",
      "train loss:7.312282810896442e-06\n",
      "train loss:4.314968458078567e-05\n",
      "train loss:1.4157910644492944e-05\n",
      "train loss:1.342705947499e-05\n",
      "train loss:2.534401279657763e-05\n",
      "train loss:7.776916029382938e-06\n",
      "train loss:1.751987714608527e-05\n",
      "train loss:4.444789912819855e-06\n",
      "train loss:7.281273794264596e-06\n",
      "train loss:2.6643632010605348e-05\n",
      "train loss:2.9349508009554e-05\n",
      "train loss:7.207091984341196e-05\n",
      "train loss:1.3238158426500704e-05\n",
      "train loss:5.700220495460389e-05\n",
      "train loss:5.981696164714162e-05\n",
      "train loss:8.472778958629402e-06\n",
      "train loss:2.2592695445122923e-05\n",
      "train loss:0.0005527457830655134\n",
      "train loss:1.1679622119019865e-05\n",
      "train loss:2.3927186646983157e-05\n",
      "train loss:3.922519867547761e-05\n",
      "train loss:5.7797948970309766e-05\n",
      "train loss:1.3423771886319959e-05\n",
      "train loss:2.1610335523772538e-05\n",
      "train loss:9.608524451624476e-06\n",
      "train loss:9.261170974409493e-05\n",
      "train loss:1.8827272278073834e-05\n",
      "train loss:2.0204327388274458e-05\n",
      "train loss:4.043259960703381e-06\n",
      "train loss:1.1094370353970801e-05\n",
      "train loss:4.3309339803344786e-05\n",
      "train loss:3.743801315161603e-06\n",
      "train loss:1.1325140209544784e-05\n",
      "train loss:2.018402399358746e-05\n",
      "train loss:2.984940012713527e-05\n",
      "train loss:3.274637775177844e-05\n",
      "train loss:8.065284796168492e-06\n",
      "train loss:3.658459170835943e-06\n",
      "train loss:5.386063307162727e-05\n",
      "train loss:4.1364707620502056e-05\n",
      "train loss:8.864067176314482e-06\n",
      "train loss:4.977221406416424e-06\n",
      "train loss:1.8137000299899536e-05\n",
      "train loss:8.514258066134047e-05\n",
      "train loss:1.5975376630247635e-06\n",
      "train loss:0.00023280362781274798\n",
      "train loss:0.00016340179665626415\n",
      "train loss:1.3367783295788097e-05\n",
      "train loss:1.7935632821443383e-05\n",
      "train loss:8.77091502010326e-06\n",
      "train loss:2.2032876898703788e-05\n",
      "train loss:4.479165904275724e-05\n",
      "train loss:1.3819453864435053e-05\n",
      "train loss:2.4865556607068106e-06\n",
      "train loss:7.4914866446511435e-06\n",
      "train loss:3.410844849988831e-05\n",
      "train loss:2.5450543871831206e-05\n",
      "train loss:2.627747919842836e-05\n",
      "train loss:1.2856021034105994e-05\n",
      "train loss:7.916498654809485e-05\n",
      "train loss:3.324872363973427e-05\n",
      "train loss:0.0002343049214063864\n",
      "train loss:0.00018779955513549264\n",
      "train loss:1.1198570172434195e-05\n",
      "train loss:3.779353831409895e-06\n",
      "train loss:2.2136459403379355e-05\n",
      "=== epoch:11, train acc:1.0, test acc:0.998 ===\n",
      "train loss:4.414687301301845e-06\n",
      "train loss:0.00011889476174902313\n",
      "train loss:2.597509429231951e-05\n",
      "train loss:3.830865289295789e-05\n",
      "train loss:1.4556998884019836e-05\n",
      "train loss:3.6860465332135545e-05\n",
      "train loss:1.2108446872432546e-05\n",
      "train loss:2.1802822324287225e-05\n",
      "train loss:1.7150928981472857e-06\n",
      "train loss:2.162208583230291e-05\n",
      "train loss:2.022954486833124e-05\n",
      "train loss:9.115836876368743e-06\n",
      "train loss:4.515870439708435e-05\n",
      "train loss:2.3508017184406265e-05\n",
      "train loss:7.598113751100858e-06\n",
      "train loss:6.4850678628764e-06\n",
      "train loss:3.204494647629865e-05\n",
      "train loss:2.1477996712086347e-05\n",
      "train loss:1.101201435522927e-05\n",
      "train loss:2.9967106556017536e-05\n",
      "train loss:0.00022277797106631073\n",
      "train loss:2.2160948492153205e-05\n",
      "train loss:6.865278251096934e-06\n",
      "train loss:7.788612309825581e-05\n",
      "train loss:0.00030095831336915494\n",
      "train loss:0.0007575912621255657\n",
      "train loss:1.3337138274240736e-05\n",
      "train loss:0.00015259923002121728\n",
      "train loss:2.3169234296159146e-05\n",
      "train loss:0.0004917843227891956\n",
      "train loss:0.0002154970448698752\n",
      "train loss:2.842718009081432e-05\n",
      "train loss:0.0005536925586556557\n",
      "train loss:4.03771628794491e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:8.141434318895998e-06\n",
      "train loss:7.222900894552356e-05\n",
      "train loss:6.319214111421429e-06\n",
      "train loss:1.2829638952526947e-05\n",
      "train loss:2.3237454582462026e-05\n",
      "train loss:0.00010255338042267706\n",
      "train loss:4.205725964429424e-05\n",
      "train loss:4.436346893307123e-05\n",
      "train loss:1.341046928934463e-05\n",
      "train loss:8.86863984635571e-06\n",
      "train loss:0.0006146404988343898\n",
      "train loss:4.041825520693788e-05\n",
      "train loss:8.651473514318823e-05\n",
      "train loss:8.25395760422512e-05\n",
      "train loss:2.3393562262180064e-05\n",
      "train loss:9.128266894305127e-05\n",
      "train loss:0.00030955637491069575\n",
      "train loss:2.298813516404001e-05\n",
      "train loss:1.4571152444845413e-05\n",
      "train loss:1.5233215670438966e-05\n",
      "train loss:6.023375175324794e-06\n",
      "train loss:2.9101327870077664e-05\n",
      "train loss:0.0006284134918370154\n",
      "train loss:1.4669156055600183e-05\n",
      "train loss:0.00041329346041781234\n",
      "train loss:0.00026244041934920056\n",
      "train loss:0.00015026806979641616\n",
      "train loss:9.777535746799777e-05\n",
      "train loss:2.8403363051792472e-05\n",
      "train loss:0.0004284949759797913\n",
      "train loss:1.1727381200092754e-05\n",
      "train loss:0.0004952894755275315\n",
      "train loss:0.0007632056336259407\n",
      "train loss:3.737131882204086e-05\n",
      "train loss:0.00010670499427866218\n",
      "train loss:0.00017181492208012648\n",
      "train loss:2.0142100190977095e-05\n",
      "train loss:0.000260650855278396\n",
      "train loss:3.75711814505763e-05\n",
      "train loss:3.2813675189488126e-06\n",
      "train loss:4.100100132548674e-05\n",
      "train loss:0.00013784082860175792\n",
      "train loss:9.486111889566405e-05\n",
      "train loss:0.00032579475253979693\n",
      "train loss:1.5882149892888498e-05\n",
      "train loss:0.00019697532085622793\n",
      "train loss:3.201063642519985e-05\n",
      "train loss:0.0001040311064868666\n",
      "train loss:0.0003024589130857442\n",
      "train loss:3.281214691714766e-06\n",
      "train loss:0.0002859910939077013\n",
      "train loss:0.00028434293312700246\n",
      "train loss:0.00025215364982958085\n",
      "train loss:3.3149934177613985e-05\n",
      "train loss:2.292945679791822e-05\n",
      "train loss:0.00020169373791141076\n",
      "train loss:4.051716315435177e-06\n",
      "train loss:0.00016079054313151091\n",
      "train loss:0.0005783518774536323\n",
      "train loss:1.9708487715498214e-05\n",
      "train loss:8.74406166735343e-05\n",
      "train loss:1.5056741235565748e-05\n",
      "train loss:6.017858471491687e-05\n",
      "train loss:3.110179970299007e-05\n",
      "train loss:0.00018492799499103085\n",
      "train loss:0.0015015526292746439\n",
      "train loss:0.00020101790225731612\n",
      "train loss:0.00017186793658958472\n",
      "train loss:0.000827506947078758\n",
      "train loss:0.0004967114703886772\n",
      "train loss:0.00023424758986610817\n",
      "train loss:0.002594863890492517\n",
      "train loss:0.00041641257686439903\n",
      "train loss:0.00038102744326126036\n",
      "train loss:0.00022174842271791104\n",
      "train loss:0.00020520151398500654\n",
      "train loss:0.0005575675668214737\n",
      "train loss:0.00045661181354220696\n",
      "train loss:0.0004636656303006207\n",
      "train loss:0.002711750310882592\n",
      "train loss:0.0004587337709053782\n",
      "train loss:0.00030486154887885504\n",
      "train loss:4.398307019540615e-05\n",
      "train loss:0.0002601079888196917\n",
      "train loss:0.0009271153336629621\n",
      "train loss:0.00012287285203121525\n",
      "=== epoch:12, train acc:0.999, test acc:0.996 ===\n",
      "train loss:2.2102405703686936e-06\n",
      "train loss:1.3215671498087106e-05\n",
      "train loss:0.0003661930877593844\n",
      "train loss:0.0006602238593293734\n",
      "train loss:6.687748303863716e-05\n",
      "train loss:8.225606285516674e-06\n",
      "train loss:0.00031368543771083466\n",
      "train loss:0.04157905766710453\n",
      "train loss:0.0008969078148559367\n",
      "train loss:0.0013985442674085014\n",
      "train loss:0.00043973700493797537\n",
      "train loss:0.0009424078328985553\n",
      "train loss:0.0003662037331037017\n",
      "train loss:0.0007744349130719277\n",
      "train loss:0.0019525149583617795\n",
      "train loss:0.0008515217191707383\n",
      "train loss:0.0008108125092096431\n",
      "train loss:0.00024295171628330343\n",
      "train loss:0.0007480506257435348\n",
      "train loss:0.0017875823400410626\n",
      "train loss:0.002897338384328972\n",
      "train loss:0.0007761795157519689\n",
      "train loss:0.0005251537140936649\n",
      "train loss:0.0006180143579147215\n",
      "train loss:0.00025448377149761895\n",
      "train loss:0.0009474229480479757\n",
      "train loss:5.341608623131634e-05\n",
      "train loss:0.0007727910342170391\n",
      "train loss:0.0005056091805009971\n",
      "train loss:0.0012461481509638635\n",
      "train loss:0.000636332424548198\n",
      "train loss:0.0008702358670669782\n",
      "train loss:0.0006074555311833226\n",
      "train loss:6.659229980693907e-05\n",
      "train loss:0.0005649218583271057\n",
      "train loss:0.0017424387597499633\n",
      "train loss:0.0030033735121811943\n",
      "train loss:0.0013767382376381628\n",
      "train loss:0.0017817716478574991\n",
      "train loss:0.0005582792737239651\n",
      "train loss:0.00043284081927589824\n",
      "train loss:0.0009259891454317877\n",
      "train loss:0.0017202204842642701\n",
      "train loss:0.0002704581796467986\n",
      "train loss:0.0005427710099648903\n",
      "train loss:0.00036736992598946917\n",
      "train loss:0.0015809135539435256\n",
      "train loss:0.002367797369862267\n",
      "train loss:0.0006861485587722423\n",
      "train loss:0.00024392890361342814\n",
      "train loss:0.0002995904724614575\n",
      "train loss:0.0005993192917655448\n",
      "train loss:9.47459201690004e-05\n",
      "train loss:0.0002802913651861532\n",
      "train loss:0.0020655972227685703\n",
      "train loss:0.001456782851738178\n",
      "train loss:0.00023434058241028952\n",
      "train loss:0.00011752652322703189\n",
      "train loss:0.00037972938490554844\n",
      "train loss:0.00021775879314337145\n",
      "train loss:0.0002634452587732542\n",
      "train loss:0.00038860457038357465\n",
      "train loss:0.0003956778736994818\n",
      "train loss:0.001145017236154048\n",
      "train loss:0.0001900720744937558\n",
      "train loss:2.049356084983821e-05\n",
      "train loss:0.0006045759215567971\n",
      "train loss:0.0003272623746429814\n",
      "train loss:8.825826097647719e-05\n",
      "train loss:0.0029744886493421603\n",
      "train loss:0.004777092182694686\n",
      "train loss:0.0017589737891746152\n",
      "train loss:0.001268557669934372\n",
      "train loss:0.0009326453284023564\n",
      "train loss:0.0003910615312927401\n",
      "train loss:0.0005394385935963039\n",
      "train loss:0.002215514884595739\n",
      "train loss:0.002027504902312994\n",
      "train loss:0.000508565122583992\n",
      "train loss:0.0018005486095521279\n",
      "train loss:0.0024365833532292934\n",
      "train loss:0.0036838144299174485\n",
      "train loss:0.03750493492007528\n",
      "train loss:0.0009246041614480408\n",
      "train loss:0.00043305233984583806\n",
      "train loss:0.017131669078119735\n",
      "train loss:0.008318707962846868\n",
      "train loss:0.000666609187771329\n",
      "train loss:0.006888867185020997\n",
      "train loss:0.0008591422214863402\n",
      "train loss:0.012782793974247838\n",
      "train loss:0.0032906012884416167\n",
      "train loss:0.0007895437847945425\n",
      "train loss:0.010956432234636281\n",
      "train loss:0.003297440341193929\n",
      "train loss:0.000490631489479784\n",
      "train loss:0.0014520845323942263\n",
      "train loss:0.0036495088333406587\n",
      "train loss:0.0018223783166786136\n",
      "train loss:0.0008537797885475693\n",
      "train loss:0.0007420152560989468\n",
      "train loss:0.0016564776041314308\n",
      "train loss:0.0006521181004582232\n",
      "train loss:0.00077673412084127\n",
      "train loss:0.0007813309046504627\n",
      "train loss:0.0008274457815396748\n",
      "train loss:0.000989223364127675\n",
      "train loss:0.0002957321519648547\n",
      "train loss:0.006978308130411064\n",
      "train loss:0.001207542965313687\n",
      "train loss:0.000996406011718532\n",
      "train loss:0.0005218730146359265\n",
      "train loss:0.0004674780987513121\n",
      "train loss:0.02084955911397863\n",
      "train loss:0.0010814753605071004\n",
      "train loss:0.001734554052869079\n",
      "train loss:0.001821734964452481\n",
      "train loss:0.004378364638895155\n",
      "train loss:0.0027288124827359434\n",
      "train loss:0.016475395768078247\n",
      "=== epoch:13, train acc:0.995, test acc:0.99 ===\n",
      "train loss:0.0010910980713078166\n",
      "train loss:0.0007257692608500566\n",
      "train loss:0.007387236920900078\n",
      "train loss:0.00012049532130948087\n",
      "train loss:0.0004067631066384974\n",
      "train loss:0.000926463291140498\n",
      "train loss:0.00018957181931759072\n",
      "train loss:0.0017457252802699488\n",
      "train loss:0.0008476167069195715\n",
      "train loss:0.0020329740804995782\n",
      "train loss:0.004459050222242051\n",
      "train loss:0.0019600055770685825\n",
      "train loss:0.0006107490657627813\n",
      "train loss:0.0021669761084282817\n",
      "train loss:0.0005465351716940227\n",
      "train loss:0.0015816182900128043\n",
      "train loss:0.0019390987236497662\n",
      "train loss:0.0006184140703839529\n",
      "train loss:0.0017253564869949378\n",
      "train loss:0.0008083212770425265\n",
      "train loss:0.0003589106440334795\n",
      "train loss:0.0006439938032873703\n",
      "train loss:0.00302903699030887\n",
      "train loss:0.0003766463208731778\n",
      "train loss:0.0005992026982376188\n",
      "train loss:0.001532705598884021\n",
      "train loss:0.0006367116216686548\n",
      "train loss:0.0014822123267252652\n",
      "train loss:0.003500842262314529\n",
      "train loss:0.006168019739724191\n",
      "train loss:0.0007634429405339663\n",
      "train loss:0.002150026531533061\n",
      "train loss:0.0024559930067566942\n",
      "train loss:0.0035983356545397018\n",
      "train loss:0.0002958927247299051\n",
      "train loss:0.00046564804897781934\n",
      "train loss:0.0005575532422316598\n",
      "train loss:0.004020808117891361\n",
      "train loss:0.0008291537930606549\n",
      "train loss:0.0002606284945366676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0005197309713307888\n",
      "train loss:0.0026010925600146216\n",
      "train loss:0.0345365941077595\n",
      "train loss:0.0001567669950305539\n",
      "train loss:0.0024600564260678316\n",
      "train loss:0.0002708636598243861\n",
      "train loss:0.00011692740709466543\n",
      "train loss:0.0004836114881893861\n",
      "train loss:0.004638047385545101\n",
      "train loss:0.004956470408676403\n",
      "train loss:0.00317544900402083\n",
      "train loss:0.0021011170586759127\n",
      "train loss:0.004818938723326876\n",
      "train loss:0.0024799279929942847\n",
      "train loss:0.0013792387307775536\n",
      "train loss:0.0016206727827200937\n",
      "train loss:0.0005038886770033641\n",
      "train loss:0.00011828523055304338\n",
      "train loss:0.0001276554317399174\n",
      "train loss:0.002331375951999719\n",
      "train loss:0.0003609436685682281\n",
      "train loss:0.0002606625724231204\n",
      "train loss:0.0011271786605547296\n",
      "train loss:0.0026971913711938\n",
      "train loss:0.000642946870637066\n",
      "train loss:0.00012046918318107804\n",
      "train loss:0.0006718651057947956\n",
      "train loss:0.00021964448401130303\n",
      "train loss:0.00047931312950602904\n",
      "train loss:8.241743976595716e-05\n",
      "train loss:0.0002147103695087068\n",
      "train loss:0.0015906330577909018\n",
      "train loss:0.0012797044914123637\n",
      "train loss:1.2110357916868353e-05\n",
      "train loss:0.003536226122796631\n",
      "train loss:0.0005798269215744963\n",
      "train loss:0.0007479669110236649\n",
      "train loss:0.00021373743836928093\n",
      "train loss:0.00034598753479545447\n",
      "train loss:0.0010390451131880932\n",
      "train loss:0.00023762056439843336\n",
      "train loss:0.0007855898168060813\n",
      "train loss:0.00015965371655130074\n",
      "train loss:6.840736038701553e-06\n",
      "train loss:0.000991092945804619\n",
      "train loss:0.0006429782701335026\n",
      "train loss:0.0011306105940288135\n",
      "train loss:0.0001521997990595712\n",
      "train loss:0.0008366355853348469\n",
      "train loss:0.0005911800896150847\n",
      "train loss:0.000616877461447983\n",
      "train loss:0.001017748022041056\n",
      "train loss:0.00013482728884671913\n",
      "train loss:0.0004935247546435817\n",
      "train loss:0.0012428575934241041\n",
      "train loss:7.99153897944427e-05\n",
      "train loss:5.926353578825562e-05\n",
      "train loss:0.00029941971311101456\n",
      "train loss:0.00013510954987523786\n",
      "train loss:0.002155861511795573\n",
      "train loss:2.3236309691940074e-05\n",
      "train loss:0.0010043633131473417\n",
      "train loss:9.342165457468715e-06\n",
      "train loss:0.00031118168359343663\n",
      "train loss:0.0003710852777311168\n",
      "train loss:0.0005718662648767265\n",
      "train loss:3.104071843706312e-05\n",
      "train loss:0.0002940473018583105\n",
      "train loss:0.0003291698247452495\n",
      "train loss:0.0005298423449090292\n",
      "train loss:0.0006510851219249037\n",
      "train loss:8.37805435250042e-05\n",
      "train loss:1.7691999994413424e-05\n",
      "train loss:3.178178596890992e-05\n",
      "train loss:0.0012768016080757735\n",
      "train loss:2.671743782446783e-05\n",
      "train loss:4.6531563566618305e-05\n",
      "train loss:0.0007210887494103868\n",
      "train loss:0.0005086352824606285\n",
      "train loss:0.0007943956673970718\n",
      "=== epoch:14, train acc:0.999, test acc:0.989 ===\n",
      "train loss:0.03142315412492945\n",
      "train loss:6.480157593585569e-05\n",
      "train loss:0.00031947486938336834\n",
      "train loss:0.0003917231365064002\n",
      "train loss:5.198438951507999e-05\n",
      "train loss:0.00104414176924901\n",
      "train loss:0.0024425432813105024\n",
      "train loss:0.01507136733440488\n",
      "train loss:0.0035980036234329817\n",
      "train loss:0.00033102420571397365\n",
      "train loss:0.002405986227445558\n",
      "train loss:0.0007372895442857806\n",
      "train loss:0.0007792126105243402\n",
      "train loss:0.00047951793624113626\n",
      "train loss:0.003452879408726644\n",
      "train loss:0.001333061622035387\n",
      "train loss:4.357474391960251e-05\n",
      "train loss:0.003512533935832725\n",
      "train loss:0.00232673701903135\n",
      "train loss:0.00016604278104648227\n",
      "train loss:0.0013841322089848554\n",
      "train loss:9.429714141950398e-05\n",
      "train loss:0.0016771530550221748\n",
      "train loss:0.001023618731725929\n",
      "train loss:0.009020880004543541\n",
      "train loss:4.910717944621387e-05\n",
      "train loss:0.0003167555541676393\n",
      "train loss:0.0004714726830681386\n",
      "train loss:0.000173628371617278\n",
      "train loss:0.0013963900718743378\n",
      "train loss:0.0007425923301678546\n",
      "train loss:0.004668949202185361\n",
      "train loss:0.002564968300920437\n",
      "train loss:0.0023555092365474834\n",
      "train loss:0.0009853108580358777\n",
      "train loss:0.00050952980808041\n",
      "train loss:0.0011615449526401348\n",
      "train loss:0.0001336712466156754\n",
      "train loss:4.500882709687748e-05\n",
      "train loss:0.0006083244398562307\n",
      "train loss:0.00010883301974770864\n",
      "train loss:0.0008423205069173346\n",
      "train loss:0.0012199390978777736\n",
      "train loss:0.0028625892873690604\n",
      "train loss:0.00019089521697945627\n",
      "train loss:0.0013610911031624884\n",
      "train loss:7.155614162379709e-05\n",
      "train loss:0.00033261823957833657\n",
      "train loss:0.0011773182597332981\n",
      "train loss:0.0006632853241879621\n",
      "train loss:0.0007636651594576653\n",
      "train loss:1.1532289619257971e-05\n",
      "train loss:0.002514953038742611\n",
      "train loss:0.0018608963871237793\n",
      "train loss:0.00016189072108320848\n",
      "train loss:0.03965383506812742\n",
      "train loss:0.0007346839206297552\n",
      "train loss:0.0007223271275545232\n",
      "train loss:0.0011591318848855874\n",
      "train loss:0.0018498293758848015\n",
      "train loss:0.00022667962453558083\n",
      "train loss:0.0034772666125043807\n",
      "train loss:0.000485973872961912\n",
      "train loss:0.0005617441160724665\n",
      "train loss:0.0010950856457377792\n",
      "train loss:0.0005768065310410611\n",
      "train loss:2.9197838916099652e-05\n",
      "train loss:1.7709961806659787e-05\n",
      "train loss:0.0011720548696514254\n",
      "train loss:0.0025660963481089934\n",
      "train loss:0.02120777453534977\n",
      "train loss:7.187842122877513e-05\n",
      "train loss:2.5522499555924222e-05\n",
      "train loss:0.0005171310441995497\n",
      "train loss:0.0031077251005195237\n",
      "train loss:0.00655602689609878\n",
      "train loss:0.0039665825700829225\n",
      "train loss:0.0008214581964712869\n",
      "train loss:0.00209613702236523\n",
      "train loss:0.004042599112273095\n",
      "train loss:0.012553423257357445\n",
      "train loss:0.00394025388576008\n",
      "train loss:0.0036948559144733702\n",
      "train loss:0.00033873326408379273\n",
      "train loss:0.0003802458812830922\n",
      "train loss:0.0005770525910317863\n",
      "train loss:4.611212916236174e-05\n",
      "train loss:0.0014455556409365903\n",
      "train loss:0.00035009322669339127\n",
      "train loss:0.0014053816836200807\n",
      "train loss:0.0004897151667704832\n",
      "train loss:0.002092483485471723\n",
      "train loss:0.0030404788656466274\n",
      "train loss:0.0005176768521255664\n",
      "train loss:0.001303755725546574\n",
      "train loss:0.0023839295810988066\n",
      "train loss:0.0004667363599186792\n",
      "train loss:0.020646273944997597\n",
      "train loss:0.00040951210742798015\n",
      "train loss:3.4816213174395725e-05\n",
      "train loss:0.0004890592351728625\n",
      "train loss:0.00023636455262350384\n",
      "train loss:0.0014926546529727852\n",
      "train loss:0.0010900402173452059\n",
      "train loss:0.0025822408260188535\n",
      "train loss:3.169527081294955e-05\n",
      "train loss:0.0013272416390927167\n",
      "train loss:0.0011489948818937438\n",
      "train loss:0.0002860478663213363\n",
      "train loss:0.00015888368715331183\n",
      "train loss:0.0017492234099093535\n",
      "train loss:0.00018749536906807526\n",
      "train loss:0.0006417665781753092\n",
      "train loss:0.0006572453306216823\n",
      "train loss:0.0009171357844623074\n",
      "train loss:0.00048528625239968874\n",
      "train loss:3.742791078848984e-05\n",
      "train loss:3.421243432855853e-05\n",
      "train loss:0.00035793172549485476\n",
      "train loss:0.0008879221230255554\n",
      "=== epoch:15, train acc:0.999, test acc:0.998 ===\n",
      "train loss:9.563446416444859e-05\n",
      "train loss:0.0001699645758430431\n",
      "train loss:0.0020244848799579078\n",
      "train loss:1.5324171237290445e-05\n",
      "train loss:0.0007760352754467308\n",
      "train loss:0.00046496423553187824\n",
      "train loss:0.00042026532771024584\n",
      "train loss:0.0006505852373398313\n",
      "train loss:0.00020734320730296694\n",
      "train loss:0.004517209959908986\n",
      "train loss:0.0001317025633670903\n",
      "train loss:0.00011574591197863051\n",
      "train loss:0.00033639293387710117\n",
      "train loss:1.2968336966765743e-05\n",
      "train loss:6.208145081704018e-05\n",
      "train loss:0.0001705322743543373\n",
      "train loss:0.001511236599445211\n",
      "train loss:2.2082525281136218e-05\n",
      "train loss:0.004370596826067117\n",
      "train loss:0.003447332415329811\n",
      "train loss:2.5065528789607843e-05\n",
      "train loss:0.0009878977279844125\n",
      "train loss:0.00016947449336659302\n",
      "train loss:0.003708608825268825\n",
      "train loss:0.00021603629963787497\n",
      "train loss:0.000850751653000589\n",
      "train loss:0.00028756531226533456\n",
      "train loss:0.0006246506051954412\n",
      "train loss:0.00041662666469879976\n",
      "train loss:0.0017231592476579288\n",
      "train loss:7.644210566474645e-05\n",
      "train loss:0.00037915786670542767\n",
      "train loss:0.0007292333953356416\n",
      "train loss:0.0001113762146572821\n",
      "train loss:0.0003621570151791586\n",
      "train loss:0.00016264254176477188\n",
      "train loss:0.00036123636532207627\n",
      "train loss:6.48451782592038e-05\n",
      "train loss:2.938231273346144e-05\n",
      "train loss:0.00020081496208549214\n",
      "train loss:0.0006507968749274061\n",
      "train loss:0.0001893226448196775\n",
      "train loss:0.0001957274495641934\n",
      "train loss:0.00040551204890539044\n",
      "train loss:0.00019237147490102851\n",
      "train loss:0.0003589798080709431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:4.618620670914746e-05\n",
      "train loss:0.0005488456353902573\n",
      "train loss:0.0008631936563496086\n",
      "train loss:0.00019077470136054465\n",
      "train loss:0.0005981561004452685\n",
      "train loss:0.00032147835129062966\n",
      "train loss:1.955485680325301e-05\n",
      "train loss:0.00012276299947436699\n",
      "train loss:0.0002940439724960047\n",
      "train loss:0.0005969628280073486\n",
      "train loss:7.763568211631216e-05\n",
      "train loss:0.00023933454447342024\n",
      "train loss:0.00024721661011539454\n",
      "train loss:0.00021165145863735534\n",
      "train loss:3.144026706025581e-05\n",
      "train loss:0.0004790534429917314\n",
      "train loss:2.0319684050512723e-05\n",
      "train loss:0.00029681367494389975\n",
      "train loss:0.0002113170678789061\n",
      "train loss:2.053812622376397e-05\n",
      "train loss:0.00012408320835751514\n",
      "train loss:5.2079690116981906e-05\n",
      "train loss:4.492694980710843e-05\n",
      "train loss:0.00010265395029551823\n",
      "train loss:0.00010418725551852612\n",
      "train loss:0.00010334369393910444\n",
      "train loss:0.00011139354334561619\n",
      "train loss:1.2820750794440759e-05\n",
      "train loss:0.000605448003587722\n",
      "train loss:0.00018994844917879911\n",
      "train loss:0.000749002754572073\n",
      "train loss:0.00011231581864119957\n",
      "train loss:6.965849474227129e-06\n",
      "train loss:4.816998957160673e-05\n",
      "train loss:2.9390798123318508e-05\n",
      "train loss:0.0001246146345484664\n",
      "train loss:0.0012687166375330969\n",
      "train loss:2.1625860832211907e-05\n",
      "train loss:0.0003781034597009233\n",
      "train loss:2.0622523156182417e-05\n",
      "train loss:0.0016027882008525683\n",
      "train loss:0.0017525946882683315\n",
      "train loss:0.0001371619700901572\n",
      "train loss:0.0016172858615196644\n",
      "train loss:6.609800025436386e-05\n",
      "train loss:0.0011473135003094743\n",
      "train loss:6.775853661953723e-05\n",
      "train loss:0.0008238649873720023\n",
      "train loss:0.0011578902555261963\n",
      "train loss:0.00017752081319506645\n",
      "train loss:0.00027213517420131424\n",
      "train loss:0.025828691819148517\n",
      "train loss:0.0005399642825476674\n",
      "train loss:0.00010656231915604083\n",
      "train loss:5.8541142845883424e-05\n",
      "train loss:0.0007155499842908237\n",
      "train loss:0.0002753337953469877\n",
      "train loss:0.00015765813145837752\n",
      "train loss:0.00021112997272378632\n",
      "train loss:0.00013113587850151662\n",
      "train loss:0.0004788700712512124\n",
      "train loss:9.406035294047564e-05\n",
      "train loss:0.0010455577858281247\n",
      "train loss:0.003249118339285492\n",
      "train loss:0.0012204260582645034\n",
      "train loss:0.00011195979337438034\n",
      "train loss:0.00034824732994175343\n",
      "train loss:0.0019716477427021927\n",
      "train loss:0.002048875137602101\n",
      "train loss:0.00013249789358231893\n",
      "train loss:0.0006734613422374716\n",
      "train loss:0.00013213547050518144\n",
      "train loss:0.0003161237959763946\n",
      "train loss:0.00012075837247598464\n",
      "=== epoch:16, train acc:0.998, test acc:0.996 ===\n",
      "train loss:0.001992619868721118\n",
      "train loss:0.00026080555528037\n",
      "train loss:0.00012471564230625852\n",
      "train loss:9.914258388679934e-05\n",
      "train loss:0.0003959529277445298\n",
      "train loss:0.0003318652761082643\n",
      "train loss:0.00010881577502864396\n",
      "train loss:0.0006993676689771293\n",
      "train loss:0.0003904535889380821\n",
      "train loss:6.530636251560356e-05\n",
      "train loss:0.0011078648479198963\n",
      "train loss:0.00022298613674816826\n",
      "train loss:0.00015306575283963595\n",
      "train loss:0.00041789553940140554\n",
      "train loss:0.00032261089900686453\n",
      "train loss:0.0004475327329234814\n",
      "train loss:0.00044102546617344284\n",
      "train loss:0.0012588744702692189\n",
      "train loss:0.00020190270533436124\n",
      "train loss:0.0005560337232136067\n",
      "train loss:8.337482681332391e-05\n",
      "train loss:6.182675254693946e-05\n",
      "train loss:0.00013980417909952296\n",
      "train loss:0.0001520808933162218\n",
      "train loss:3.630504004873516e-05\n",
      "train loss:0.0003465868456828817\n",
      "train loss:0.0001370135463771633\n",
      "train loss:0.00019111450701111323\n",
      "train loss:3.1556162930275324e-05\n",
      "train loss:5.7170923653167897e-05\n",
      "train loss:2.8995683832800057e-06\n",
      "train loss:5.274787743517569e-06\n",
      "train loss:0.0009434751494822516\n",
      "train loss:0.00012476644266855188\n",
      "train loss:1.4987243399402922e-05\n",
      "train loss:0.0001372991714610443\n",
      "train loss:0.00014673277264112578\n",
      "train loss:4.799515837680053e-07\n",
      "train loss:3.524448890138371e-05\n",
      "train loss:0.0004191552844343192\n",
      "train loss:1.4119286782934585e-05\n",
      "train loss:0.00029535746104448156\n",
      "train loss:7.24177727110772e-07\n",
      "train loss:3.231263731578084e-05\n",
      "train loss:7.24384653400914e-05\n",
      "train loss:6.51508305465271e-05\n",
      "train loss:7.0558061628487325e-06\n",
      "train loss:3.597142868197988e-05\n",
      "train loss:1.2705472190309667e-05\n",
      "train loss:1.980146162759881e-06\n",
      "train loss:2.6595703945338637e-05\n",
      "train loss:0.00010658548196314628\n",
      "train loss:2.409107593887503e-06\n",
      "train loss:2.4713176932997122e-05\n",
      "train loss:8.784310848169988e-05\n",
      "train loss:0.00025016569240137343\n",
      "train loss:2.706269437869916e-05\n",
      "train loss:0.00028996069775669636\n",
      "train loss:0.0001480843869027827\n",
      "train loss:0.0007723222764614463\n",
      "train loss:0.0003871027667885657\n",
      "train loss:4.6109989017632916e-05\n",
      "train loss:4.519739540871982e-05\n",
      "train loss:0.00012196211413707594\n",
      "train loss:0.00010859093683038133\n",
      "train loss:0.00019625305261456922\n",
      "train loss:0.0013975625029644032\n",
      "train loss:0.0004149254831510668\n",
      "train loss:0.0001780527784454643\n",
      "train loss:0.0010180577045948384\n",
      "train loss:1.210232933928223e-05\n",
      "train loss:0.001879660362829301\n",
      "train loss:0.0005929746312364953\n",
      "train loss:6.249922880885526e-05\n",
      "train loss:0.0005175626579913683\n",
      "train loss:0.00041863014870764485\n",
      "train loss:0.00023574868261176822\n",
      "train loss:5.853911568895865e-05\n",
      "train loss:0.0009345311996880761\n",
      "train loss:2.698062490390789e-06\n",
      "train loss:0.00018719966564760844\n",
      "train loss:1.6717340073377464e-05\n",
      "train loss:5.8338230786517375e-05\n",
      "train loss:0.000639818511265716\n",
      "train loss:6.888447787965094e-05\n",
      "train loss:0.0004768521118759897\n",
      "train loss:5.1068706809342804e-05\n",
      "train loss:3.795172467583623e-05\n",
      "train loss:0.00018434770244929466\n",
      "train loss:0.0002787188386968754\n",
      "train loss:0.0001706364475286082\n",
      "train loss:0.000137610353414069\n",
      "train loss:0.013400680404821348\n",
      "train loss:0.00025775362476977064\n",
      "train loss:2.667315469374022e-05\n",
      "train loss:0.000521661305414161\n",
      "train loss:1.0836469808101169e-05\n",
      "train loss:6.959088785417274e-05\n",
      "train loss:0.0007654071470973981\n",
      "train loss:0.002437010196056953\n",
      "train loss:5.618702792287644e-06\n",
      "train loss:3.405790508339613e-05\n",
      "train loss:2.4353155339328435e-05\n",
      "train loss:0.0009933309050638049\n",
      "train loss:2.6748609819891577e-05\n",
      "train loss:5.958325705127069e-05\n",
      "train loss:6.5773842240778934e-06\n",
      "train loss:0.0013031531221898587\n",
      "train loss:0.00017700247695891374\n",
      "train loss:0.0004014670419314983\n",
      "train loss:0.0006348003617835939\n",
      "train loss:0.0024184743335821128\n",
      "train loss:0.00010461247340378614\n",
      "train loss:0.00022205657272483654\n",
      "train loss:0.00029448171848850944\n",
      "train loss:0.00019912553996246233\n",
      "train loss:0.0006345703184249497\n",
      "train loss:0.0009356788658391374\n",
      "train loss:3.470694309355891e-06\n",
      "train loss:4.312417908126318e-05\n",
      "=== epoch:17, train acc:1.0, test acc:0.988 ===\n",
      "train loss:0.06803091521169\n",
      "train loss:9.161811969912292e-07\n",
      "train loss:0.00019468978363073035\n",
      "train loss:2.979627992711474e-06\n",
      "train loss:5.807936281346472e-06\n",
      "train loss:0.00026916883461290203\n",
      "train loss:5.731748937120551e-05\n",
      "train loss:7.544587282395685e-05\n",
      "train loss:1.2608425830362992e-06\n",
      "train loss:0.0006830680145352838\n",
      "train loss:5.297124530085984e-06\n",
      "train loss:0.00043849586857735317\n",
      "train loss:0.0017776652806534912\n",
      "train loss:0.0002890558477186875\n",
      "train loss:3.217918722075839e-05\n",
      "train loss:3.812763176177649e-05\n",
      "train loss:4.3729251799322726e-05\n",
      "train loss:1.5856492074736939e-06\n",
      "train loss:0.0006948916115632574\n",
      "train loss:3.6853945960323536e-05\n",
      "train loss:3.733991090690046e-05\n",
      "train loss:6.213064654353048e-06\n",
      "train loss:4.052667192440658e-05\n",
      "train loss:3.129370524780857e-05\n",
      "train loss:0.0007031544534516957\n",
      "train loss:0.0005662582415704803\n",
      "train loss:6.796773408647547e-05\n",
      "train loss:0.00032684443587526515\n",
      "train loss:0.00010675770039708821\n",
      "train loss:3.258605402029288e-06\n",
      "train loss:0.00037978047359585093\n",
      "train loss:4.235308641993937e-05\n",
      "train loss:3.257136286882346e-05\n",
      "train loss:0.0003306052161277107\n",
      "train loss:8.56688030533952e-05\n",
      "train loss:5.4317207712823756e-05\n",
      "train loss:2.7567796209602043e-05\n",
      "train loss:0.0007256892057854851\n",
      "train loss:0.0013488252054796959\n",
      "train loss:2.1577667329879256e-05\n",
      "train loss:2.5940066161262748e-06\n",
      "train loss:0.00010291237447017853\n",
      "train loss:4.87721804447312e-05\n",
      "train loss:0.0002610093364632766\n",
      "train loss:0.0004291456534724776\n",
      "train loss:0.0005205063522368474\n",
      "train loss:4.187830450827759e-06\n",
      "train loss:1.4267513298252218e-05\n",
      "train loss:2.0487042264147372e-06\n",
      "train loss:2.8215785474068963e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.2006182790931312e-07\n",
      "train loss:0.0004655568046214671\n",
      "train loss:0.0005264279469735965\n",
      "train loss:1.0507827828904611e-07\n",
      "train loss:0.0002433860058863053\n",
      "train loss:6.328493429149336e-05\n",
      "train loss:0.0002359499992717253\n",
      "train loss:1.383738422488322e-06\n",
      "train loss:0.0003617663369143865\n",
      "train loss:2.5445235955167325e-06\n",
      "train loss:0.00038346244433440797\n",
      "train loss:4.440627104941146e-05\n",
      "train loss:5.554042687321563e-05\n",
      "train loss:3.0367192975602514e-06\n",
      "train loss:7.207980157304124e-05\n",
      "train loss:0.0011852631429440706\n",
      "train loss:0.0004555695901524238\n",
      "train loss:0.0003226585210404565\n",
      "train loss:0.00041788693119255445\n",
      "train loss:6.124354404139554e-06\n",
      "train loss:8.900029649542467e-05\n",
      "train loss:8.159016138017732e-05\n",
      "train loss:3.500316094065241e-07\n",
      "train loss:4.668533885675003e-06\n",
      "train loss:1.044127849782184e-05\n",
      "train loss:0.00022494013695359007\n",
      "train loss:0.00014009122593419207\n",
      "train loss:2.6787564913159687e-05\n",
      "train loss:6.513262640823395e-06\n",
      "train loss:0.00046455582819740395\n",
      "train loss:8.529641640775921e-06\n",
      "train loss:2.7807313472681137e-06\n",
      "train loss:0.00013714507307218174\n",
      "train loss:2.140312518188471e-06\n",
      "train loss:5.958699047207494e-05\n",
      "train loss:0.0007993280284323348\n",
      "train loss:3.3246366690206804e-05\n",
      "train loss:1.1619219688607997e-05\n",
      "train loss:1.696452238370736e-05\n",
      "train loss:2.1070521612113354e-07\n",
      "train loss:0.00011845757701248365\n",
      "train loss:4.747003040679664e-05\n",
      "train loss:0.0001743847160190249\n",
      "train loss:7.274850652388469e-05\n",
      "train loss:0.00015466871845328347\n",
      "train loss:0.0014678042784501593\n",
      "train loss:0.00018989479701951364\n",
      "train loss:6.167090873643431e-05\n",
      "train loss:0.00012609996807504172\n",
      "train loss:0.0004018739484870813\n",
      "train loss:4.829072198966067e-05\n",
      "train loss:1.6703674475613082e-05\n",
      "train loss:0.0002603694099958238\n",
      "train loss:0.0003307522220632389\n",
      "train loss:0.0002275218056428238\n",
      "train loss:0.00010104588637655296\n",
      "train loss:9.805597643400828e-06\n",
      "train loss:2.459129876038115e-05\n",
      "train loss:0.0009538000888957663\n",
      "train loss:1.5772491438311704e-05\n",
      "train loss:1.0239245966251532e-06\n",
      "train loss:0.0002418792299389176\n",
      "train loss:0.00015293198909269623\n",
      "train loss:4.908011714192002e-06\n",
      "train loss:0.00013120795576386637\n",
      "train loss:6.874340335640761e-06\n",
      "train loss:6.018000810118177e-06\n",
      "train loss:3.201366033286985e-05\n",
      "train loss:0.00024130616052082902\n",
      "train loss:3.602079163140413e-06\n",
      "=== epoch:18, train acc:1.0, test acc:0.995 ===\n",
      "train loss:-6.085582754053542e-10\n",
      "train loss:1.2633423172022772e-05\n",
      "train loss:2.6703952902532223e-07\n",
      "train loss:4.4435919480359125e-05\n",
      "train loss:3.995106142469901e-05\n",
      "train loss:8.318099325042385e-06\n",
      "train loss:3.7367083881606666e-06\n",
      "train loss:8.905623996856629e-08\n",
      "train loss:4.120976698021943e-06\n",
      "train loss:2.8356031494527517e-06\n",
      "train loss:6.465303744576285e-05\n",
      "train loss:0.0002678499890827487\n",
      "train loss:2.0620768811482422e-06\n",
      "train loss:8.18775130807303e-06\n",
      "train loss:2.233147925743715e-05\n",
      "train loss:0.00030135522172703566\n",
      "train loss:8.328658399141847e-05\n",
      "train loss:0.00026386039090787143\n",
      "train loss:0.00021791703774588404\n",
      "train loss:4.3519687852833734e-05\n",
      "train loss:9.721970562851576e-06\n",
      "train loss:4.6547265730910366e-05\n",
      "train loss:3.940106005596102e-05\n",
      "train loss:0.0007730485418276123\n",
      "train loss:0.0012069326320833145\n",
      "train loss:9.92505511005907e-06\n",
      "train loss:8.725462898289539e-05\n",
      "train loss:0.000255633584021198\n",
      "train loss:6.706979132739233e-06\n",
      "train loss:1.3557012297373564e-05\n",
      "train loss:4.8479116964121203e-05\n",
      "train loss:2.0649419507806216e-06\n",
      "train loss:2.4032652368968775e-05\n",
      "train loss:0.00016049125664390743\n",
      "train loss:1.9856167006642976e-06\n",
      "train loss:0.0002899687104155077\n",
      "train loss:1.277793065746122e-06\n",
      "train loss:1.7911101420537416e-05\n",
      "train loss:0.0010154518965399143\n",
      "train loss:1.3734822046944162e-07\n",
      "train loss:0.000230241293699081\n",
      "train loss:2.2934713564227983e-08\n",
      "train loss:9.392297005243466e-07\n",
      "train loss:9.605666870470445e-05\n",
      "train loss:1.046906333741e-06\n",
      "train loss:1.2370940001378634e-05\n",
      "train loss:1.773800669829905e-08\n",
      "train loss:6.385796257764711e-06\n",
      "train loss:2.167893667951979e-06\n",
      "train loss:1.4374187802041103e-06\n",
      "train loss:1.3252485750468712e-06\n",
      "train loss:3.5021219915402983e-06\n",
      "train loss:6.11392359770486e-05\n",
      "train loss:2.1640241433854813e-06\n",
      "train loss:3.394033279840878e-05\n",
      "train loss:0.00019046411021266937\n",
      "train loss:1.4789370401006976e-06\n",
      "train loss:1.3307745492484984e-06\n",
      "train loss:0.0017666978480178\n",
      "train loss:1.2066515717967518e-05\n",
      "train loss:0.00014857012891289186\n",
      "train loss:0.0009597967449943797\n",
      "train loss:9.921047111677513e-05\n",
      "train loss:2.780263210961173e-05\n",
      "train loss:0.0006530343829631253\n",
      "train loss:1.2514133357479081e-05\n",
      "train loss:2.617720687920809e-05\n",
      "train loss:0.0006323945551110001\n",
      "train loss:6.816461925767974e-05\n",
      "train loss:0.0001227570732600176\n",
      "train loss:0.0001729538251688774\n",
      "train loss:3.4475795861359784e-05\n",
      "train loss:0.0002802463790659391\n",
      "train loss:2.1887298463369537e-05\n",
      "train loss:0.00034868532150302044\n",
      "train loss:0.00011634428076171944\n",
      "train loss:9.760765592595014e-05\n",
      "train loss:6.508680083891244e-06\n",
      "train loss:2.4536253382137612e-05\n",
      "train loss:1.3565366756671746e-05\n",
      "train loss:0.00022563263064248734\n",
      "train loss:3.2418344527323385e-05\n",
      "train loss:0.00023835591950074423\n",
      "train loss:0.0003043374418154905\n",
      "train loss:6.654341560689871e-05\n",
      "train loss:0.0005649040337885232\n",
      "train loss:1.3719566011903827e-05\n",
      "train loss:1.6264613846885366e-05\n",
      "train loss:6.022547909880354e-05\n",
      "train loss:0.00014074862709183395\n",
      "train loss:0.0006997099737106004\n",
      "train loss:4.3029124762567455e-05\n",
      "train loss:0.0016420665586858365\n",
      "train loss:3.870171839681353e-06\n",
      "train loss:1.0658917185607831e-05\n",
      "train loss:0.00010398154543820722\n",
      "train loss:0.00012912917341984605\n",
      "train loss:0.00012875691189317436\n",
      "train loss:0.00026027208437719103\n",
      "train loss:2.6948201440005742e-05\n",
      "train loss:0.0003943673457328506\n",
      "train loss:2.2618421252603796e-05\n",
      "train loss:3.3562932801779325e-06\n",
      "train loss:6.838718022531267e-05\n",
      "train loss:3.482901296045564e-06\n",
      "train loss:0.00040520666386824463\n",
      "train loss:5.2752867261967196e-05\n",
      "train loss:0.00018421915278662364\n",
      "train loss:8.399055353635167e-07\n",
      "train loss:1.1746769038487003e-05\n",
      "train loss:7.592869409587501e-05\n",
      "train loss:0.0007503434638309951\n",
      "train loss:2.9428560112678964e-08\n",
      "train loss:2.1219324098879358e-05\n",
      "train loss:5.9679173506781814e-05\n",
      "train loss:4.8945311628347665e-05\n",
      "train loss:9.921946860696988e-06\n",
      "train loss:2.8990694208343732e-05\n",
      "train loss:0.00012151950337649976\n",
      "train loss:7.929921798559535e-05\n",
      "=== epoch:19, train acc:0.996, test acc:0.995 ===\n",
      "train loss:4.4410181484491826e-05\n",
      "train loss:1.5146217851401834e-05\n",
      "train loss:0.0006319324857201687\n",
      "train loss:0.0004846750009202591\n",
      "train loss:5.88918182234368e-06\n",
      "train loss:0.00046575281702654055\n",
      "train loss:0.0008326010278461478\n",
      "train loss:1.1845928952321097e-06\n",
      "train loss:1.8868713815331865e-05\n",
      "train loss:7.429238623318916e-07\n",
      "train loss:0.0002790521114979069\n",
      "train loss:1.0651233645645061e-05\n",
      "train loss:7.541008888052333e-07\n",
      "train loss:4.479434283488336e-06\n",
      "train loss:0.0001279399258896869\n",
      "train loss:1.4501156749928086e-07\n",
      "train loss:3.824270566656335e-05\n",
      "train loss:6.265337769704142e-06\n",
      "train loss:5.95182274562053e-06\n",
      "train loss:2.2381175175411846e-05\n",
      "train loss:2.570552659985057e-05\n",
      "train loss:0.0003305017442406435\n",
      "train loss:1.0559230867849474e-05\n",
      "train loss:2.986008762975963e-05\n",
      "train loss:1.7298931514947053e-06\n",
      "train loss:8.438136225581507e-05\n",
      "train loss:1.1198366111417145e-06\n",
      "train loss:1.3826285273005323e-06\n",
      "train loss:2.2483696556355264e-05\n",
      "train loss:5.479725155321494e-06\n",
      "train loss:0.00027910250936255884\n",
      "train loss:2.3617126990385168e-05\n",
      "train loss:2.8705162050173174e-05\n",
      "train loss:0.00010274820691606569\n",
      "train loss:0.00048380476207324485\n",
      "train loss:4.2734905772046676e-06\n",
      "train loss:-5.476468050532527e-08\n",
      "train loss:1.4225125607579852e-06\n",
      "train loss:0.0007700953435153736\n",
      "train loss:1.4979218147750113e-05\n",
      "train loss:1.7144400485784094e-06\n",
      "train loss:2.0134644838423385e-05\n",
      "train loss:0.0005901703098507141\n",
      "train loss:9.804616161005657e-06\n",
      "train loss:1.5572984499076357e-06\n",
      "train loss:7.576987787386407e-05\n",
      "train loss:0.0002289707861942096\n",
      "train loss:2.071343547361023e-06\n",
      "train loss:6.258849230286791e-07\n",
      "train loss:1.857795007046243e-05\n",
      "train loss:5.58927822825145e-07\n",
      "train loss:0.00017295978765836527\n",
      "train loss:0.00010677759331429575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.0702524900357005e-08\n",
      "train loss:3.823527153071643e-06\n",
      "train loss:0.0003346820317610416\n",
      "train loss:4.410754599121839e-06\n",
      "train loss:7.810621442506365e-06\n",
      "train loss:7.735725894834024e-05\n",
      "train loss:3.71275895826831e-06\n",
      "train loss:5.128575813601718e-05\n",
      "train loss:8.004655643248213e-06\n",
      "train loss:1.0795733228013437e-05\n",
      "train loss:5.75537513323174e-05\n",
      "train loss:6.816748924008035e-05\n",
      "train loss:9.22403980319443e-06\n",
      "train loss:0.0002878045521858771\n",
      "train loss:0.000388414188833697\n",
      "train loss:6.50915205717931e-05\n",
      "train loss:5.205936107891944e-05\n",
      "train loss:1.4903392849680201e-05\n",
      "train loss:9.715356853354124e-07\n",
      "train loss:9.159368510434142e-05\n",
      "train loss:4.146488261342566e-06\n",
      "train loss:2.9652217997983144e-07\n",
      "train loss:0.0003260963158662892\n",
      "train loss:2.3249311041780117e-06\n",
      "train loss:7.027044086205254e-05\n",
      "train loss:0.00030516553918056617\n",
      "train loss:1.8428518972027658e-06\n",
      "train loss:1.1934426444955835e-05\n",
      "train loss:3.0279709258159847e-07\n",
      "train loss:9.191373233607336e-05\n",
      "train loss:5.963518012999806e-05\n",
      "train loss:0.0007158385321571824\n",
      "train loss:9.207192636056788e-05\n",
      "train loss:0.0001848099678952449\n",
      "train loss:2.6118693586709374e-05\n",
      "train loss:6.949768594509526e-07\n",
      "train loss:2.282079234821822e-05\n",
      "train loss:2.0340975420066233e-06\n",
      "train loss:0.00026549454328759505\n",
      "train loss:1.3480843127831857e-05\n",
      "train loss:2.6513239902016196e-05\n",
      "train loss:8.710462423002091e-08\n",
      "train loss:6.456966574690396e-05\n",
      "train loss:2.2919623465741306e-05\n",
      "train loss:0.033717583904365266\n",
      "train loss:8.386042251121285e-06\n",
      "train loss:5.916006821685435e-05\n",
      "train loss:0.00046686961223173126\n",
      "train loss:0.0002625067865718738\n",
      "train loss:0.0007377069120837658\n",
      "train loss:0.0004751396057186689\n",
      "train loss:0.0001527880065785768\n",
      "train loss:0.0008833383810552141\n",
      "train loss:9.389974962139565e-05\n",
      "train loss:0.0005794746022531879\n",
      "train loss:2.6836641607151608e-06\n",
      "train loss:2.6074828726396312e-05\n",
      "train loss:0.00013429526346647088\n",
      "train loss:5.017146202348758e-07\n",
      "train loss:0.0002085877879023985\n",
      "train loss:1.9063542245259366e-06\n",
      "train loss:3.875975220113427e-05\n",
      "train loss:2.863606395814615e-05\n",
      "train loss:1.9473204638226183e-06\n",
      "train loss:1.682659314641021e-05\n",
      "train loss:0.00036577293124276334\n",
      "train loss:0.0001481195292066339\n",
      "=== epoch:20, train acc:1.0, test acc:0.995 ===\n",
      "train loss:5.378977679929147e-06\n",
      "train loss:0.007604404567605278\n",
      "train loss:1.4931555846583835e-05\n",
      "train loss:2.3485596639262222e-06\n",
      "train loss:0.0004206984569054338\n",
      "train loss:1.1489936944777356e-07\n",
      "train loss:0.04338191391747915\n",
      "train loss:0.00047171287594805064\n",
      "train loss:3.8590027177586546e-07\n",
      "train loss:2.9891035583962032e-05\n",
      "train loss:3.624614908789837e-05\n",
      "train loss:6.035445128628902e-06\n",
      "train loss:0.001453982622896325\n",
      "train loss:0.001609956788701587\n",
      "train loss:0.0006331944123736\n",
      "train loss:0.0037932263863726086\n",
      "train loss:0.0006439070881915604\n",
      "train loss:0.0010084109082033315\n",
      "train loss:4.364161395854157e-06\n",
      "train loss:1.854315737433678e-05\n",
      "train loss:1.5382224049714426e-05\n",
      "train loss:0.0006788203896921654\n",
      "train loss:0.00014343579563289979\n",
      "train loss:2.7896566858922108e-06\n",
      "train loss:0.0003827371430168646\n",
      "train loss:2.3461414300247342e-05\n",
      "train loss:8.472283273809387e-06\n",
      "train loss:3.987414892207817e-05\n",
      "train loss:3.326883816111107e-05\n",
      "train loss:0.0005222674581895025\n",
      "train loss:0.0002802777046045363\n",
      "train loss:0.00034883897887569667\n",
      "train loss:0.00014339163728776207\n",
      "train loss:0.003298859495704346\n",
      "train loss:4.724132231606242e-05\n",
      "train loss:4.550160836507663e-05\n",
      "train loss:8.017186729815893e-06\n",
      "train loss:0.0002026697697828232\n",
      "train loss:2.9370469763275075e-05\n",
      "train loss:4.063904259664882e-05\n",
      "train loss:0.00021701122156095507\n",
      "train loss:1.8418509308804388e-07\n",
      "train loss:0.0002962555087686963\n",
      "train loss:0.00016994747486172475\n",
      "train loss:0.015752478992394958\n",
      "train loss:0.0002270617300683267\n",
      "train loss:0.0014144500500783857\n",
      "train loss:0.00016534380426942483\n",
      "train loss:0.00017814019119527597\n",
      "train loss:0.002534965581783245\n",
      "train loss:0.0015540663146187226\n",
      "train loss:3.062746900814298e-05\n",
      "train loss:0.0005541661590521867\n",
      "train loss:6.0600222001926725e-05\n",
      "train loss:1.7738912358348883e-05\n",
      "train loss:2.7434972734549277e-05\n",
      "train loss:0.0006539207113941349\n",
      "train loss:3.0106912155536743e-06\n",
      "train loss:0.0006652206114628836\n",
      "train loss:0.00011203043529542469\n",
      "train loss:2.2858205970149305e-06\n",
      "train loss:0.00033914887890802286\n",
      "train loss:0.00033232301837179387\n",
      "train loss:7.855713931074787e-05\n",
      "train loss:1.1122011927949657e-05\n",
      "train loss:6.921691937042698e-05\n",
      "train loss:0.0006915426640570358\n",
      "train loss:0.00010254174606129863\n",
      "train loss:5.344409090497508e-05\n",
      "train loss:0.000849126101439533\n",
      "train loss:5.046334265403275e-05\n",
      "train loss:0.0006176544638308238\n",
      "train loss:0.0005635473552107664\n",
      "train loss:0.0005210753547688611\n",
      "train loss:0.00018823268680926683\n",
      "train loss:0.0001724270992433403\n",
      "train loss:1.013893022542225e-05\n",
      "train loss:0.0007369457061699001\n",
      "train loss:0.0013866087205607878\n",
      "train loss:6.743357999534105e-05\n",
      "train loss:0.00036219809902045166\n",
      "train loss:0.0002291211664514856\n",
      "train loss:0.008317404382319551\n",
      "train loss:0.005261403168717604\n",
      "train loss:1.3136471245541646e-05\n",
      "train loss:1.3584644200294526e-05\n",
      "train loss:6.526069071316967e-05\n",
      "train loss:0.00026920710396550617\n",
      "train loss:0.00039783086436651245\n",
      "train loss:0.00034319254024934675\n",
      "train loss:2.613743825715705e-06\n",
      "train loss:1.967487403542111e-05\n",
      "train loss:9.56688487612565e-08\n",
      "train loss:4.207548181751376e-05\n",
      "train loss:1.020963565417061e-05\n",
      "train loss:0.0034762880376510742\n",
      "train loss:0.0007426964687259508\n",
      "train loss:0.0005167062323494189\n",
      "train loss:0.00042210260382602715\n",
      "train loss:0.0006299446693458642\n",
      "train loss:1.2236743596013232e-06\n",
      "train loss:3.2456430441902705e-05\n",
      "train loss:0.00015880848548975742\n",
      "train loss:0.00012053091581635245\n",
      "train loss:0.0003770367782005421\n",
      "train loss:0.0013476044166410802\n",
      "train loss:0.0015740051304815744\n",
      "train loss:2.680775274057776e-05\n",
      "train loss:2.5247572091661422e-05\n",
      "train loss:7.173934635887292e-06\n",
      "train loss:5.193974887940458e-05\n",
      "train loss:0.00015533771121585142\n",
      "train loss:5.3688885793697904e-05\n",
      "train loss:0.00010780022282768399\n",
      "train loss:0.00020637685519368172\n",
      "train loss:1.523184635080866e-05\n",
      "train loss:0.0010874354366251686\n",
      "train loss:4.385422521695628e-06\n",
      "train loss:6.011420435382552e-06\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.994\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "#import sys, os\n",
    "#sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataset.img_oxt import load_oxt\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_oxt(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=3, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03690452 -0.02553347 -0.02966439]\n",
      " [-0.04931898  0.02581714  0.00199626]\n",
      " [-0.00385304 -0.00971032  0.00247456]\n",
      " [ 0.01723339 -0.00789018 -0.02008049]\n",
      " [ 0.01818325  0.00967089 -0.01005577]\n",
      " [ 0.01891191 -0.02157375  0.01440606]\n",
      " [ 0.00142917 -0.01088221  0.00320727]\n",
      " [ 0.00339241 -0.00830942 -0.00443222]\n",
      " [-0.01572326 -0.03055921  0.01140681]\n",
      " [-0.0237182   0.02981972 -0.01778364]\n",
      " [-0.02056431  0.0229232  -0.00356232]\n",
      " [ 0.00670577 -0.00418313 -0.01539559]\n",
      " [-0.0088091  -0.01350517  0.00089155]\n",
      " [ 0.00253032  0.0174236  -0.0196501 ]\n",
      " [ 0.00333909 -0.02016207  0.00327266]\n",
      " [-0.0153515   0.02221197 -0.02404533]\n",
      " [-0.0018285   0.01510665  0.01572447]\n",
      " [-0.01398969 -0.01159235  0.00690803]\n",
      " [ 0.02502679 -0.0218316  -0.03932124]\n",
      " [-0.01116856  0.01743012 -0.00847071]\n",
      " [ 0.00728166  0.01857383 -0.03540851]\n",
      " [-0.01451886  0.00535431  0.00806812]\n",
      " [ 0.00618735 -0.00242594 -0.01854552]\n",
      " [-0.00246742 -0.00862445  0.01927796]\n",
      " [ 0.0109034  -0.01581386 -0.0069982 ]\n",
      " [-0.02833144  0.02202723  0.01069436]\n",
      " [ 0.01254789  0.00639461 -0.00642409]\n",
      " [ 0.00201774  0.00235266 -0.0012804 ]\n",
      " [-0.0403888   0.02114662  0.01110142]\n",
      " [-0.02100689  0.02586298  0.00630807]\n",
      " [ 0.02131951 -0.00850433  0.00627725]\n",
      " [ 0.00227184 -0.01855731  0.00323165]\n",
      " [-0.01686953  0.0072003  -0.02969367]\n",
      " [-0.01350946  0.00754695 -0.00810267]\n",
      " [ 0.00981649 -0.00142735 -0.01315732]\n",
      " [-0.03381441 -0.00700796  0.02825942]\n",
      " [ 0.01454131  0.01143993 -0.00601068]\n",
      " [ 0.001331    0.01050397  0.00040441]\n",
      " [ 0.00467468  0.01932456 -0.02018037]\n",
      " [ 0.00452732  0.01813622 -0.01595482]\n",
      " [-0.01906063  0.01299576 -0.00648627]\n",
      " [ 0.00574197 -0.01706652  0.0093017 ]\n",
      " [-0.01444165 -0.00287812  0.01592106]\n",
      " [ 0.00432943 -0.00014202 -0.00456528]\n",
      " [-0.01428106 -0.00072269  0.0135523 ]\n",
      " [ 0.00159753  0.00781271 -0.01920614]\n",
      " [-0.05127701  0.00325232 -0.00086819]\n",
      " [-0.00385474 -0.01827116  0.02660697]\n",
      " [-0.01363482 -0.00909107 -0.01306384]\n",
      " [ 0.00215873  0.01209319 -0.0268747 ]\n",
      " [-0.02496549  0.00052388  0.0191088 ]\n",
      " [-0.0088058   0.01880882 -0.01288401]\n",
      " [ 0.00697095 -0.01661394  0.01669305]\n",
      " [ 0.01225932 -0.00417127 -0.02474911]\n",
      " [ 0.03263131  0.00381797 -0.02038444]\n",
      " [-0.03222028  0.00076081  0.03392017]\n",
      " [ 0.00615261 -0.01766394  0.0004227 ]\n",
      " [-0.01717149 -0.02335332  0.02316869]\n",
      " [ 0.00136033 -0.02235797  0.00721928]\n",
      " [-0.00340304  0.0425232  -0.02635454]\n",
      " [-0.0110482   0.00475049 -0.00395759]\n",
      " [-0.02261706  0.00254516  0.01768678]\n",
      " [ 0.03560796 -0.00252226 -0.00512301]\n",
      " [-0.03848024 -0.00155916  0.02264365]\n",
      " [ 0.01379541 -0.00781303  0.01597253]\n",
      " [ 0.00417684 -0.01916554  0.00092368]\n",
      " [ 0.0020262  -0.00114853 -0.01546185]\n",
      " [ 0.02204811  0.00433714 -0.00516056]\n",
      " [-0.01017268  0.00936837 -0.01738176]\n",
      " [ 0.02115816 -0.02354106 -0.02092245]\n",
      " [-0.00444906  0.00400913 -0.00417616]\n",
      " [ 0.02008828 -0.05916094 -0.01029234]\n",
      " [-0.00725134  0.00194415  0.0189325 ]\n",
      " [ 0.00516766  0.00311455  0.00146712]\n",
      " [-0.00829505  0.02432356 -0.00157559]\n",
      " [ 0.0165605  -0.01472899  0.00475131]\n",
      " [-0.00672443 -0.01463404  0.02340234]\n",
      " [ 0.0094268   0.00384016  0.00846703]\n",
      " [-0.01243096 -0.00382027  0.01749481]\n",
      " [ 0.0137381   0.01106998 -0.00711569]\n",
      " [ 0.02595629  0.00740932 -0.02903987]\n",
      " [-0.03723709 -0.01030655  0.01750967]\n",
      " [ 0.01387599 -0.03281679 -0.01336153]\n",
      " [ 0.0051415  -0.00077437  0.00618941]\n",
      " [-0.01167102  0.03443067 -0.01084998]\n",
      " [-0.01216997  0.00927981  0.00786755]\n",
      " [-0.00908369 -0.00248273  0.01196594]\n",
      " [-0.0206372  -0.00098903 -0.0026556 ]\n",
      " [-0.0138405   0.02265184 -0.02876845]\n",
      " [-0.00895991  0.01105213 -0.00255479]\n",
      " [-0.00578349  0.01334636 -0.0243973 ]\n",
      " [ 0.00969703 -0.01553726 -0.00727067]\n",
      " [-0.01051564 -0.01301493 -0.00695491]\n",
      " [ 0.0048567  -0.00773156  0.00015483]\n",
      " [ 0.00758972  0.0006806   0.00955935]\n",
      " [ 0.00679689  0.00249692 -0.00555531]\n",
      " [-0.01785038  0.01442871  0.02315232]\n",
      " [ 0.04176136 -0.0210679  -0.03727654]\n",
      " [-0.04160882  0.03865119 -0.05276251]\n",
      " [-0.00881164 -0.00580316  0.01085228]]\n"
     ]
    }
   ],
   "source": [
    "print(network.params['W3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IrFD5wl7MUs8",
    "outputId": "79e9e53e-442b-40c8-c701-ade92f7f2df2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.567, 0.961, 0.986, 0.995, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] [0.57, 0.9566666666666667, 0.9716666666666667, 0.9883333333333333, 0.9883333333333333, 0.985, 0.99, 0.9883333333333333, 0.9883333333333333, 0.9883333333333333, 0.9883333333333333, 0.9883333333333333, 0.9883333333333333, 0.9883333333333333, 0.9883333333333333, 0.9883333333333333, 0.9883333333333333, 0.9883333333333333, 0.9883333333333333, 0.9883333333333333]\n"
     ]
    }
   ],
   "source": [
    "print(trainer.train_acc_list, trainer.test_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0o2CUtoPMUs8",
    "outputId": "bae3c194-b570-4055-8a81-33bd16993786"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0.95, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'simple_convnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14279/3545441131.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moxt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_oxt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msimple_convnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleConvNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'simple_convnet'"
     ]
    }
   ],
   "source": [
    "## load_parameter를 이용해 학습을 시키지 않고 이전 파리미터값을 이용.\n",
    "\n",
    "# coding: utf-8\n",
    "#import sys, os\n",
    "#sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "#import numpy as np\n",
    "\n",
    "from dataset.oxt import load_oxt\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test)=load_oxt(flatten=False)\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=3, weight_init_std=0.01)\n",
    "\n",
    "network.load_params(\"params.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my data image is △\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 생성 o, x, t\n",
    "# 도형을 그리고 아무 키를 누르면 myImg.png로 저장\n",
    "# 그린 도형을 예측해서 결과 출력\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "oldx = oldy = -1\n",
    "\n",
    "def on_mouse(event, x, y, flags, param):\n",
    "    global oldx, oldy\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        oldx, oldy = x, y\n",
    "        # print('EVENT_LBUTTONDOWN: %d, %d' % (x, y))\n",
    "\n",
    "    # elif event == cv2.EVENT_LBUTTONUP:\n",
    "        # print('EVENT_LBUTTONUP: %d, %d' % (x, y))\n",
    "\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "            cv2.line(img, (oldx, oldy), (x, y), 0, 5, cv2.LINE_AA)\n",
    "            cv2.imshow('image', img)\n",
    "            oldx, oldy = x, y\n",
    "\n",
    "\n",
    "img = np.ones((280, 280), dtype=np.uint8) * 255\n",
    "\n",
    "cv2.namedWindow('image')\n",
    "cv2.setMouseCallback('image', on_mouse, img)\n",
    "\n",
    "cv2.imshow('image', img)\n",
    "cv2.waitKey()\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "img = cv2.resize(img, (28,28), interpolation=cv2.INTER_AREA)     # 28*28 resize\n",
    "img = ~img  # invert\n",
    "cv2.imwrite('myImg.png', img)\n",
    "\n",
    "#print(img.shape)\n",
    "img=img.reshape(-1,1,28,28)\n",
    "labels_view=['o', 'x', '△']\n",
    "y=network.predict(img)\n",
    "#print(y)\n",
    "pred_num=np.argmax(y)\n",
    "result = \"my data image is %s\"%(labels_view[pred_num])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "dcgFnBd2MUs9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from PIL import ImageTk, Image\n",
    "from tkinter import filedialog\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "window = Tk()\n",
    "window.title('oxt 예측하기')\n",
    "    \n",
    "oldx = oldy = -1\n",
    "\n",
    "def on_mouse(event, x, y, flags, param):\n",
    "    global oldx, oldy\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        oldx, oldy = x, y\n",
    "        # print('EVENT_LBUTTONDOWN: %d, %d' % (x, y))\n",
    "\n",
    "    # elif event == cv2.EVENT_LBUTTONUP:\n",
    "        # print('EVENT_LBUTTONUP: %d, %d' % (x, y))\n",
    "\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "            cv2.line(img, (oldx, oldy), (x, y), 0, 5, cv2.LINE_AA)\n",
    "            cv2.imshow('image', img)\n",
    "            oldx, oldy = x, y\n",
    "\n",
    "def crt():\n",
    "    global img, tmp_img\n",
    "    img = np.ones((280, 280), dtype=np.uint8) * 255\n",
    "\n",
    "    cv2.namedWindow('image')\n",
    "    cv2.setMouseCallback('image', on_mouse, img)\n",
    "\n",
    "    cv2.imshow('image', img)\n",
    "    cv2.waitKey(3000)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    img = cv2.resize(img, (28,28), interpolation=cv2.INTER_AREA)     # 28*28 resize\n",
    "    cv2.imwrite('tmp.png', img)\n",
    "    img = ~img  # invert\n",
    "    img=img.reshape(-1,1,28,28)\n",
    "    \n",
    "    labels_view=['o', 'x', '△']\n",
    "\n",
    "    y=network.predict(img)\n",
    "    pred_num=np.argmax(y)\n",
    "    result = \"my predict is %s\"%(labels_view[pred_num])\n",
    "    \n",
    "    tmp_img=Image.open('tmp.png')\n",
    "    tmp_img=ImageTk.PhotoImage(tmp_img)\n",
    "    \n",
    "    Label(window, text=\"파일경로: new\").grid(row=2) # 파일경로 view\n",
    "    Label(window, image=tmp_img).grid(row=3) #사진 view\n",
    "    Label(window, text=result).grid(row=4) # 예측 결과 출력    \n",
    "\n",
    "def open():\n",
    "    global my_image # 함수에서 이미지를 기억하도록 전역변수 선언 (안하면 사진이 안보임)\n",
    "    window.filename = filedialog.askopenfilename(initialdir='', title='파일선택', filetypes=(\n",
    "    ('png files', '*.png'), ('jpg files', '*.jpg'), ('all files', '*.*')))\n",
    " \n",
    "    Label(window, text=\"파일경로: \"+window.filename).grid(row=2) # 파일경로 view\n",
    "    \n",
    "    img = Image.open(window.filename)\n",
    "    my_image = ImageTk.PhotoImage(img)\n",
    "    \n",
    "    img=img.convert(\"L\")                         # gray 저장\n",
    "    img=np.invert(img)                           # 흑백을 반전\n",
    "    \n",
    "    # print(img.shape)                           # img의 shape 확인\n",
    "    img=img.reshape(-1, 1, 28, 28)\n",
    "    \n",
    "    Label(window, image=my_image).grid(row=3) #사진 view\n",
    "    \n",
    "    labels_view=['o', 'x', '△']\n",
    "\n",
    "    y=network.predict(img)\n",
    "    pred_num=np.argmax(y)\n",
    "    result = \"my predict is %s\"%(labels_view[pred_num])\n",
    "    Label(window, text=result).grid(row=4) # 예측 결과 출력\n",
    "    \n",
    "\n",
    "b_create=Button(window, text='그리기(아무키나 누르면 닫기)', command=crt).grid(row=0)\n",
    "b_open = Button(window, text='파일열기', command=open).grid(row=1)\n",
    "Label(window, text=\"파일 경로\").grid(row=2)\n",
    "Label(window).grid(row=3)\n",
    "Label(window, text=\"예측 결과\").grid(row=4)\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "executionInfo": {
     "elapsed": 432,
     "status": "error",
     "timestamp": 1660293915658,
     "user": {
      "displayName": "이태훈",
      "userId": "13105880102370027485"
     },
     "user_tz": -540
    },
    "id": "lgtE3sC2MUs9",
    "outputId": "69b616f0-2064-45ba-92ca-591bed060167"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera open failed!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "model = 'opencv_face_detector/res10_300x300_ssd_iter_140000_fp16.caffemodel'\n",
    "config = 'opencv_face_detector/deploy.prototxt'\n",
    "#model = 'opencv_face_detector/opencv_face_detector_uint8.pb'\n",
    "#config = 'opencv_face_detector/opencv_face_detector.pbtxt'\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print('Camera open failed!')\n",
    "    sys.exit()\n",
    "\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('Net open failed!')\n",
    "    sys.exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1, (300, 300), (104, 177, 123))\n",
    "    net.setInput(blob)\n",
    "    out = net.forward()\n",
    "\n",
    "    detect = out[0, 0, :, :]\n",
    "    (h, w) = frame.shape[:2]\n",
    "\n",
    "    for i in range(detect.shape[0]):\n",
    "        confidence = detect[i, 2]\n",
    "        if confidence < 0.5:\n",
    "            break\n",
    "\n",
    "        x1 = int(detect[i, 3] * w)\n",
    "        y1 = int(detect[i, 4] * h)\n",
    "        x2 = int(detect[i, 5] * w)\n",
    "        y2 = int(detect[i, 6] * h)\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0))\n",
    "\n",
    "        label = f'Face: {confidence:4.2f}'\n",
    "        cv2.putText(frame, label, (x1, y1 - 1), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "oxt_project_cnn_0729.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
