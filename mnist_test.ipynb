{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e76a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2998166309626122\n",
      "=== epoch:1, train acc:0.183, test acc:0.187 ===\n",
      "train loss:2.2966422248973\n",
      "train loss:2.2950768683797733\n",
      "train loss:2.2889545877804207\n",
      "train loss:2.2836303264558504\n",
      "train loss:2.275257691662823\n",
      "train loss:2.259594953843099\n",
      "train loss:2.2473446065262075\n",
      "train loss:2.229762677755931\n",
      "train loss:2.213209430267651\n",
      "train loss:2.17926023046891\n",
      "train loss:2.1363436386286736\n",
      "train loss:2.081710113515117\n",
      "train loss:2.09837971800852\n",
      "train loss:2.0071977649189816\n",
      "train loss:1.9644921430399578\n",
      "train loss:1.8958366674232265\n",
      "train loss:1.7720676329109026\n",
      "train loss:1.8226363883818617\n",
      "train loss:1.6929681875751867\n",
      "train loss:1.5790621596996155\n",
      "train loss:1.5053439779182078\n",
      "train loss:1.4975330027454559\n",
      "train loss:1.3597414947637796\n",
      "train loss:1.2566156014615464\n",
      "train loss:1.2160356471396507\n",
      "train loss:1.065554269037058\n",
      "train loss:0.9645756498276316\n",
      "train loss:0.9403017422977215\n",
      "train loss:0.9245096256657298\n",
      "train loss:0.8966168650031902\n",
      "train loss:0.8592419060275704\n",
      "train loss:0.8216821709897535\n",
      "train loss:0.7528424440390571\n",
      "train loss:0.7047208647556947\n",
      "train loss:0.7118292962841207\n",
      "train loss:0.6434098795994682\n",
      "train loss:0.7443609747012608\n",
      "train loss:0.8766650179061188\n",
      "train loss:0.6503104325153332\n",
      "train loss:0.6566846339049716\n",
      "train loss:0.5747088041382296\n",
      "train loss:0.6262839712284475\n",
      "train loss:0.42082719247498274\n",
      "train loss:0.569606201170563\n",
      "train loss:0.5368887840188749\n",
      "train loss:0.6146269547296723\n",
      "train loss:0.6462223874524184\n",
      "train loss:0.7879167926757447\n",
      "train loss:0.7631951540401407\n",
      "train loss:0.40789954979928067\n",
      "train loss:0.5213418994041163\n",
      "train loss:0.35890405034104816\n",
      "train loss:0.753404496998527\n",
      "train loss:0.7028903068556602\n",
      "train loss:0.47631179732591966\n",
      "train loss:0.43951269056182174\n",
      "train loss:0.5012125984974124\n",
      "train loss:0.4870331679729215\n",
      "train loss:0.4255593076823839\n",
      "train loss:0.5126181681992837\n",
      "train loss:0.4808492474303225\n",
      "train loss:0.5754636487343792\n",
      "train loss:0.577819778334953\n",
      "train loss:0.35171267779787735\n",
      "train loss:0.3945506610431187\n",
      "train loss:0.52133272881825\n",
      "train loss:0.5617619012743361\n",
      "train loss:0.46886793158338924\n",
      "train loss:0.37637033658338404\n",
      "train loss:0.5276314752635768\n",
      "train loss:0.5807882036171909\n",
      "train loss:0.24744009001512376\n",
      "train loss:0.5763411810958525\n",
      "train loss:0.2592761703858023\n",
      "train loss:0.4591751259245201\n",
      "train loss:0.6449704725833076\n",
      "train loss:0.4332629487946862\n",
      "train loss:0.468579805616292\n",
      "train loss:0.4668947417770814\n",
      "train loss:0.47238095546759845\n",
      "train loss:0.36387519208007274\n",
      "train loss:0.4459825165812112\n",
      "train loss:0.4032830525379746\n",
      "train loss:0.47312694661549626\n",
      "train loss:0.37063028854392305\n",
      "train loss:0.3271295473198901\n",
      "train loss:0.3956250146990573\n",
      "train loss:0.36389275158362866\n",
      "train loss:0.4907616660829179\n",
      "train loss:0.38340749670420643\n",
      "train loss:0.39844828639777796\n",
      "train loss:0.4575221562419298\n",
      "train loss:0.2111441779701805\n",
      "train loss:0.3555766391824523\n",
      "train loss:0.31925781628543765\n",
      "train loss:0.3556589237300682\n",
      "train loss:0.33153306123255366\n",
      "train loss:0.30407390820917063\n",
      "train loss:0.3417404621155023\n",
      "train loss:0.5701187043782815\n",
      "train loss:0.19168321326240184\n",
      "train loss:0.369058161317576\n",
      "train loss:0.296517168834646\n",
      "train loss:0.4364628971325236\n",
      "train loss:0.2300210711473921\n",
      "train loss:0.3584884992919592\n",
      "train loss:0.2937664271029989\n",
      "train loss:0.3401800069799205\n",
      "train loss:0.4516838568895439\n",
      "train loss:0.33727282006570786\n",
      "train loss:0.3997862909781683\n",
      "train loss:0.3278683496190609\n",
      "train loss:0.31041561465800027\n",
      "train loss:0.2172973166643553\n",
      "train loss:0.27242424804600646\n",
      "train loss:0.3634851962365413\n",
      "train loss:0.2909041512415559\n",
      "train loss:0.2945748745275761\n",
      "train loss:0.3946613231333695\n",
      "train loss:0.3513813472730082\n",
      "train loss:0.3663502241675923\n",
      "train loss:0.4883901126661711\n",
      "train loss:0.3815308306762282\n",
      "train loss:0.36215585213618273\n",
      "train loss:0.38083091667755087\n",
      "train loss:0.4046672995176781\n",
      "train loss:0.24564136412789203\n",
      "train loss:0.4144551180152357\n",
      "train loss:0.3101328057559664\n",
      "train loss:0.30464000571649413\n",
      "train loss:0.2755855174197888\n",
      "train loss:0.26343234681828753\n",
      "train loss:0.30094879401769303\n",
      "train loss:0.45404241415279206\n",
      "train loss:0.2900256182911519\n",
      "train loss:0.2901771203733979\n",
      "train loss:0.3601669337611757\n",
      "train loss:0.41348808878234045\n",
      "train loss:0.27880483823570124\n",
      "train loss:0.21586899143988106\n",
      "train loss:0.17601973771864596\n",
      "train loss:0.33463269704029214\n",
      "train loss:0.2385587474795615\n",
      "train loss:0.29340357391375316\n",
      "train loss:0.23766000898927755\n",
      "train loss:0.23533117479170512\n",
      "train loss:0.15968192821997457\n",
      "train loss:0.311799300143041\n",
      "train loss:0.27243111465142333\n",
      "train loss:0.22102143173613106\n",
      "train loss:0.40275658230956585\n",
      "train loss:0.2721479769552264\n",
      "train loss:0.29176780847016703\n",
      "train loss:0.1667750686824308\n",
      "train loss:0.2711637423266107\n",
      "train loss:0.2464039150844472\n",
      "train loss:0.2124222979893511\n",
      "train loss:0.41143235668118355\n",
      "train loss:0.3324129257207485\n",
      "train loss:0.2573668756426545\n",
      "train loss:0.20925041821967094\n",
      "train loss:0.23192024503145922\n",
      "train loss:0.253826431480942\n",
      "train loss:0.28015460408405807\n",
      "train loss:0.28896611152709145\n",
      "train loss:0.3692800656670832\n",
      "train loss:0.36589225522167224\n",
      "train loss:0.4116443294870706\n",
      "train loss:0.2945853983197158\n",
      "train loss:0.2970951646593592\n",
      "train loss:0.300303543223411\n",
      "train loss:0.3952248070253687\n",
      "train loss:0.4205042981138381\n",
      "train loss:0.3316311379405856\n",
      "train loss:0.26179930004373897\n",
      "train loss:0.20946489636905766\n",
      "train loss:0.34379329157894634\n",
      "train loss:0.41441548004891077\n",
      "train loss:0.3744368869825808\n",
      "train loss:0.20095069244110303\n",
      "train loss:0.2617285228879439\n",
      "train loss:0.16613556047820505\n",
      "train loss:0.24502095743668056\n",
      "train loss:0.18867717839638262\n",
      "train loss:0.25489304289690223\n",
      "train loss:0.25490409342387993\n",
      "train loss:0.21639381425940646\n",
      "train loss:0.2842643876310922\n",
      "train loss:0.2507600934968211\n",
      "train loss:0.41939159323717473\n",
      "train loss:0.20171901721969165\n",
      "train loss:0.30038745146313545\n",
      "train loss:0.25606929197431966\n",
      "train loss:0.30145499487163935\n",
      "train loss:0.15337759416994132\n",
      "train loss:0.22334705671081018\n",
      "train loss:0.2813628053184086\n",
      "train loss:0.22268548253204112\n",
      "train loss:0.43453473041813545\n",
      "train loss:0.2098674358970038\n",
      "train loss:0.36483407739493917\n",
      "train loss:0.2158491806186403\n",
      "train loss:0.30974613554348923\n",
      "train loss:0.2597093047408594\n",
      "train loss:0.20569305111291591\n",
      "train loss:0.17374056111202668\n",
      "train loss:0.32314265056036845\n",
      "train loss:0.24276382667191923\n",
      "train loss:0.15306978013005246\n",
      "train loss:0.3291100346020524\n",
      "train loss:0.224368932813697\n",
      "train loss:0.40070999865444273\n",
      "train loss:0.4862991980792195\n",
      "train loss:0.1640557005877003\n",
      "train loss:0.29933643029934986\n",
      "train loss:0.23336625377820971\n",
      "train loss:0.16632619605207896\n",
      "train loss:0.2390898997661108\n",
      "train loss:0.28741481258339036\n",
      "train loss:0.31273470226349775\n",
      "train loss:0.2851300009007866\n",
      "train loss:0.19166428708487157\n",
      "train loss:0.23686538907580826\n",
      "train loss:0.15206979094807144\n",
      "train loss:0.28545129157648363\n",
      "train loss:0.20987446529209444\n",
      "train loss:0.3229393979858723\n",
      "train loss:0.1865146788969104\n",
      "train loss:0.21336619559269854\n",
      "train loss:0.24125636144186455\n",
      "train loss:0.1621794564841463\n",
      "train loss:0.2303006193480905\n",
      "train loss:0.3295569983612337\n",
      "train loss:0.2006266652049914\n",
      "train loss:0.21452104367048322\n",
      "train loss:0.25002243538345964\n",
      "train loss:0.22913297520216658\n",
      "train loss:0.17913271974179937\n",
      "train loss:0.1735471813593273\n",
      "train loss:0.20354566098989432\n",
      "train loss:0.34887299795034654\n",
      "train loss:0.3408978312961415\n",
      "train loss:0.20396857854698333\n",
      "train loss:0.15288711408449504\n",
      "train loss:0.16738757588999523\n",
      "train loss:0.23360777179252995\n",
      "train loss:0.36602645937607814\n",
      "train loss:0.16981269754547662\n",
      "train loss:0.22161569802506248\n",
      "train loss:0.2651176127761129\n",
      "train loss:0.15253733424807128\n",
      "train loss:0.23010910475114715\n",
      "train loss:0.19295173468328622\n",
      "train loss:0.1849343317314474\n",
      "train loss:0.1800073224749494\n",
      "train loss:0.23162886540912112\n",
      "train loss:0.24222428050944658\n",
      "train loss:0.2673658018217749\n",
      "train loss:0.3447828164215745\n",
      "train loss:0.39940835397007385\n",
      "train loss:0.3612582595083261\n",
      "train loss:0.13471157639303719\n",
      "train loss:0.24218239514048398\n",
      "train loss:0.241413081902567\n",
      "train loss:0.24235548631351736\n",
      "train loss:0.2388459917089386\n",
      "train loss:0.20227148978823392\n",
      "train loss:0.2377472351793039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.1807881104761269\n",
      "train loss:0.2203021465153267\n",
      "train loss:0.14749214725750537\n",
      "train loss:0.2289562368683439\n",
      "train loss:0.21568278220622433\n",
      "train loss:0.14486771276727126\n",
      "train loss:0.20159508414478236\n",
      "train loss:0.24936455944384986\n",
      "train loss:0.13510530980304852\n",
      "train loss:0.247320993910061\n",
      "train loss:0.1815632617943366\n",
      "train loss:0.31485758595541263\n",
      "train loss:0.1577569187984503\n",
      "train loss:0.25714270015979396\n",
      "train loss:0.1634320584081633\n",
      "train loss:0.27653570206025513\n",
      "train loss:0.29096778599760653\n",
      "train loss:0.19795170592476058\n",
      "train loss:0.21040182607043273\n",
      "train loss:0.06639971497069205\n",
      "train loss:0.22578451959404877\n",
      "train loss:0.28349695363557087\n",
      "train loss:0.19524062295695638\n",
      "train loss:0.17110001556062876\n",
      "train loss:0.18305644758323647\n",
      "train loss:0.15738805555293348\n",
      "train loss:0.2299929727917423\n",
      "train loss:0.1926397893195049\n",
      "train loss:0.17673863027309875\n",
      "train loss:0.1256920520502097\n",
      "train loss:0.25887160764452755\n",
      "train loss:0.20009096585537217\n",
      "train loss:0.2638457435111872\n",
      "train loss:0.1404435362049969\n",
      "train loss:0.20536698884512053\n",
      "train loss:0.254951351388822\n",
      "train loss:0.13799710175055774\n",
      "train loss:0.22814373595439358\n",
      "train loss:0.11020439344521217\n",
      "train loss:0.12448075694038468\n",
      "train loss:0.23345528755557302\n",
      "train loss:0.10882684060114307\n",
      "train loss:0.23472776040653529\n",
      "train loss:0.2418128339008979\n",
      "train loss:0.21724887642555318\n",
      "train loss:0.1641750235154395\n",
      "train loss:0.20785266225418528\n",
      "train loss:0.15727167658864571\n",
      "train loss:0.15629732827698842\n",
      "train loss:0.1631261123370659\n",
      "train loss:0.13070388774981026\n",
      "train loss:0.2570383967973593\n",
      "train loss:0.14615974016679634\n",
      "train loss:0.178145769191755\n",
      "train loss:0.17013522385371235\n",
      "train loss:0.21477295889521217\n",
      "train loss:0.11517552926481382\n",
      "train loss:0.14766907148862327\n",
      "train loss:0.21950420301047896\n",
      "train loss:0.27403111585884454\n",
      "train loss:0.13507805491395025\n",
      "train loss:0.19069562499634943\n",
      "train loss:0.20657799664857718\n",
      "train loss:0.20551822492702967\n",
      "train loss:0.12243756128789177\n",
      "train loss:0.230321477498055\n",
      "train loss:0.13149885215413654\n",
      "train loss:0.11875343970466402\n",
      "train loss:0.1349304903168456\n",
      "train loss:0.15438660754941577\n",
      "train loss:0.17043491402567804\n",
      "train loss:0.11149351475898127\n",
      "train loss:0.19211839657524338\n",
      "train loss:0.1557497968534066\n",
      "train loss:0.16123416806369192\n",
      "train loss:0.23766005186609937\n",
      "train loss:0.116272792733951\n",
      "train loss:0.25018521368183044\n",
      "train loss:0.14314158053918288\n",
      "train loss:0.10283754261963211\n",
      "train loss:0.25155467981903923\n",
      "train loss:0.22180769966346012\n",
      "train loss:0.16456089592284134\n",
      "train loss:0.13124650246553388\n",
      "train loss:0.2867610469013243\n",
      "train loss:0.17883402830830372\n",
      "train loss:0.17792207014890146\n",
      "train loss:0.16452051260744793\n",
      "train loss:0.28461671385369747\n",
      "train loss:0.15129664016907032\n",
      "train loss:0.14199112664386426\n",
      "train loss:0.14599516522963304\n",
      "train loss:0.18466360084704653\n",
      "train loss:0.15842745250435447\n",
      "train loss:0.19076236475406297\n",
      "train loss:0.14313934862459846\n",
      "train loss:0.15441321236418112\n",
      "train loss:0.2160062025071616\n",
      "train loss:0.16110809403599197\n",
      "train loss:0.24652504527954572\n",
      "train loss:0.16472720680444974\n",
      "train loss:0.17764714409383026\n",
      "train loss:0.17870690807866782\n",
      "train loss:0.1760998404175644\n",
      "train loss:0.1561319980029949\n",
      "train loss:0.14370606354232912\n",
      "train loss:0.14040372331966727\n",
      "train loss:0.25547394182479344\n",
      "train loss:0.2137297485075373\n",
      "train loss:0.1426716409519891\n",
      "train loss:0.11765683953249578\n",
      "train loss:0.21975830392514356\n",
      "train loss:0.08697555651123015\n",
      "train loss:0.09672234398426023\n",
      "train loss:0.12144735079299881\n",
      "train loss:0.16230033117530807\n",
      "train loss:0.10904587000655035\n",
      "train loss:0.09171579262543242\n",
      "train loss:0.10523319406627701\n",
      "train loss:0.1426036657338871\n",
      "train loss:0.13852886910471943\n",
      "train loss:0.16567813816671276\n",
      "train loss:0.20189631999148336\n",
      "train loss:0.1178360469683682\n",
      "train loss:0.14548394464495437\n",
      "train loss:0.24836662739717025\n",
      "train loss:0.1649312574386949\n",
      "train loss:0.14773601460916683\n",
      "train loss:0.17442633470045385\n",
      "train loss:0.14054038694318383\n",
      "train loss:0.13430779390462125\n",
      "train loss:0.11169861214203579\n",
      "train loss:0.1057929304374265\n",
      "train loss:0.04720123643781715\n",
      "train loss:0.1811378935182544\n",
      "train loss:0.10954533783705542\n",
      "train loss:0.09946537307986339\n",
      "train loss:0.060131668613837445\n",
      "train loss:0.16439554494812145\n",
      "train loss:0.19985387660740125\n",
      "train loss:0.1326803022250096\n",
      "train loss:0.06760763781255369\n",
      "train loss:0.12519241420223348\n",
      "train loss:0.0761567120868573\n",
      "train loss:0.1877665398109107\n",
      "train loss:0.33834651970983654\n",
      "train loss:0.12366965331675542\n",
      "train loss:0.16223091116018606\n",
      "train loss:0.17128887464144232\n",
      "train loss:0.1440656703158473\n",
      "train loss:0.19285712485046236\n",
      "train loss:0.2061252254541061\n",
      "train loss:0.22583943800530587\n",
      "train loss:0.2237086107814216\n",
      "train loss:0.1277304102407594\n",
      "train loss:0.20813965267622273\n",
      "train loss:0.13700137385650235\n",
      "train loss:0.13000104582308583\n",
      "train loss:0.09971774966420162\n",
      "train loss:0.21751820424771087\n",
      "train loss:0.12094658767861874\n",
      "train loss:0.13626243651864398\n",
      "train loss:0.12471808183386335\n",
      "train loss:0.20408520200655947\n",
      "train loss:0.11987153019807083\n",
      "train loss:0.08073581694605815\n",
      "train loss:0.13896177175602953\n",
      "train loss:0.12057262993446649\n",
      "train loss:0.06197387371120114\n",
      "train loss:0.14621256984959255\n",
      "train loss:0.09947080077534466\n",
      "train loss:0.11995409486107231\n",
      "train loss:0.07939199251910414\n",
      "train loss:0.11042940121227061\n",
      "train loss:0.12405703814996369\n",
      "train loss:0.1065736734488815\n",
      "train loss:0.1900195750674395\n",
      "train loss:0.14020751059460881\n",
      "train loss:0.2206936391217968\n",
      "train loss:0.23173415257230293\n",
      "train loss:0.1121955928217795\n",
      "train loss:0.15623034735459088\n",
      "train loss:0.06976141371603878\n",
      "train loss:0.09853952441414314\n",
      "train loss:0.14438590446813396\n",
      "train loss:0.12999670561994967\n",
      "train loss:0.11520942619631235\n",
      "train loss:0.10257381929622765\n",
      "train loss:0.10930213802384901\n",
      "train loss:0.08912412954881638\n",
      "train loss:0.10006028450909876\n",
      "train loss:0.2037133553538851\n",
      "train loss:0.23264206231273182\n",
      "train loss:0.09891924670239055\n",
      "train loss:0.1261318012592909\n",
      "train loss:0.1359978782665441\n",
      "train loss:0.17168322097227248\n",
      "train loss:0.18768823407080132\n",
      "train loss:0.16314468631843565\n",
      "train loss:0.08243719993624242\n",
      "train loss:0.1156676891159233\n",
      "train loss:0.1408144352966313\n",
      "train loss:0.043087775034946735\n",
      "train loss:0.09186652656756984\n",
      "train loss:0.16635029684215077\n",
      "train loss:0.13995460526866238\n",
      "train loss:0.19206390038944093\n",
      "train loss:0.0719746006037309\n",
      "train loss:0.2365077915404761\n",
      "train loss:0.08412449972453451\n",
      "train loss:0.08075885061124052\n",
      "train loss:0.22882902849476808\n",
      "train loss:0.13062114817585005\n",
      "train loss:0.13555246784299044\n",
      "train loss:0.15091099435548203\n",
      "train loss:0.05177473435660307\n",
      "train loss:0.22188558491632854\n",
      "train loss:0.06904635861810983\n",
      "train loss:0.16950203309663633\n",
      "train loss:0.18656853402176596\n",
      "train loss:0.16173393271160988\n",
      "train loss:0.08991941283552746\n",
      "train loss:0.16086371801176175\n",
      "train loss:0.05896055834682205\n",
      "train loss:0.12333744375069569\n",
      "train loss:0.051120755154826905\n",
      "train loss:0.11423618590212833\n",
      "train loss:0.16760584805536702\n",
      "train loss:0.11351470886616012\n",
      "train loss:0.1678728501048176\n",
      "train loss:0.18465604440633598\n",
      "train loss:0.10571187598363399\n",
      "=== epoch:2, train acc:0.96, test acc:0.958 ===\n",
      "train loss:0.07160507435488027\n",
      "train loss:0.16671130443762086\n",
      "train loss:0.15185677297040417\n",
      "train loss:0.11597239602382436\n",
      "train loss:0.1842318113826846\n",
      "train loss:0.11637185647851808\n",
      "train loss:0.15285903646415114\n",
      "train loss:0.15277880818327863\n",
      "train loss:0.11833505908151261\n",
      "train loss:0.10151886571205686\n",
      "train loss:0.08030006059097873\n",
      "train loss:0.07285954703514193\n",
      "train loss:0.15354668408085312\n",
      "train loss:0.21249808500480075\n",
      "train loss:0.09578886798630618\n",
      "train loss:0.1610949582690603\n",
      "train loss:0.1892371530450065\n",
      "train loss:0.12378822803174057\n",
      "train loss:0.13985795391443812\n",
      "train loss:0.08909158353281721\n",
      "train loss:0.08935168541233418\n",
      "train loss:0.18259453635169862\n",
      "train loss:0.13316468968453954\n",
      "train loss:0.10367720025054658\n",
      "train loss:0.09412630856406592\n",
      "train loss:0.1633406290755711\n",
      "train loss:0.1531655165649987\n",
      "train loss:0.15167072247803243\n",
      "train loss:0.13588001666221852\n",
      "train loss:0.06913374535948699\n",
      "train loss:0.09254332549570655\n",
      "train loss:0.12944760670688743\n",
      "train loss:0.11194379924282333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.1212605925990169\n",
      "train loss:0.17145694667380793\n",
      "train loss:0.07315668994553111\n",
      "train loss:0.10407064700462713\n",
      "train loss:0.043534384435236065\n",
      "train loss:0.15110222269025306\n",
      "train loss:0.17907755645690682\n",
      "train loss:0.1853572669446562\n",
      "train loss:0.12084442452649863\n",
      "train loss:0.12696914331851575\n",
      "train loss:0.07067237763098004\n",
      "train loss:0.2479429821963444\n",
      "train loss:0.08599736350229455\n",
      "train loss:0.10022905511167922\n",
      "train loss:0.09263210457439645\n",
      "train loss:0.1129960297040301\n",
      "train loss:0.19549779409488127\n",
      "train loss:0.16911189721345185\n",
      "train loss:0.08947470722984859\n",
      "train loss:0.12127651493735478\n",
      "train loss:0.1008633777193063\n",
      "train loss:0.06236858751463143\n",
      "train loss:0.09359861323281957\n",
      "train loss:0.10058454778204576\n",
      "train loss:0.07733169005523563\n",
      "train loss:0.09356181941407685\n",
      "train loss:0.11828108013665031\n",
      "train loss:0.08182590189328423\n",
      "train loss:0.11088176151517391\n",
      "train loss:0.1079549493927206\n",
      "train loss:0.15651079273553314\n",
      "train loss:0.08089476828045593\n",
      "train loss:0.04097039399885141\n",
      "train loss:0.071872625107791\n",
      "train loss:0.06935004653627187\n",
      "train loss:0.11828928120289821\n",
      "train loss:0.06749580583005864\n",
      "train loss:0.08076475625081003\n",
      "train loss:0.09219723680289828\n",
      "train loss:0.05317421001378639\n",
      "train loss:0.06206295797888507\n",
      "train loss:0.08260085929002832\n",
      "train loss:0.13702130926182515\n",
      "train loss:0.07834495802855085\n",
      "train loss:0.047477258714212135\n",
      "train loss:0.0747038439919443\n",
      "train loss:0.13666904100426447\n",
      "train loss:0.136099628084425\n",
      "train loss:0.07542237725613796\n",
      "train loss:0.07412599232961012\n",
      "train loss:0.05000349405610223\n",
      "train loss:0.13949413640811828\n",
      "train loss:0.08122566545346183\n",
      "train loss:0.1470274098309651\n",
      "train loss:0.1220383103284399\n",
      "train loss:0.0897997972861427\n",
      "train loss:0.1258692120577169\n",
      "train loss:0.11235336773546087\n",
      "train loss:0.13779998217674602\n",
      "train loss:0.0729568676770101\n",
      "train loss:0.05957178208897939\n",
      "train loss:0.02843912482603966\n",
      "train loss:0.10374850217939503\n",
      "train loss:0.171555966589775\n",
      "train loss:0.06853542645494949\n",
      "train loss:0.11386264528087944\n",
      "train loss:0.16909371283385954\n",
      "train loss:0.1949823101451336\n",
      "train loss:0.052056816596784156\n",
      "train loss:0.2243161762757304\n",
      "train loss:0.08892696011521917\n",
      "train loss:0.06984805019785731\n",
      "train loss:0.20179713238238467\n",
      "train loss:0.08428970037431262\n",
      "train loss:0.08334909821256739\n",
      "train loss:0.14937859649658428\n",
      "train loss:0.0446428392425676\n",
      "train loss:0.09302862153493517\n",
      "train loss:0.037557863147954\n",
      "train loss:0.09676536017302013\n",
      "train loss:0.08028935711998136\n",
      "train loss:0.18517633810181638\n",
      "train loss:0.033272204105700746\n",
      "train loss:0.07679753764154154\n",
      "train loss:0.05539418094450591\n",
      "train loss:0.0882497660766212\n",
      "train loss:0.11509222647416349\n",
      "train loss:0.105567839489233\n",
      "train loss:0.12203128908755036\n",
      "train loss:0.1479345235699128\n",
      "train loss:0.044891753478149984\n",
      "train loss:0.14474224243013692\n",
      "train loss:0.07333035103604797\n",
      "train loss:0.09285573862882407\n",
      "train loss:0.13298625603739914\n",
      "train loss:0.14329015898439745\n",
      "train loss:0.07729881271639248\n",
      "train loss:0.11887010775218758\n",
      "train loss:0.1362104946993515\n",
      "train loss:0.099905755222552\n",
      "train loss:0.17847803986758773\n",
      "train loss:0.09085101367306224\n",
      "train loss:0.0549823494553184\n",
      "train loss:0.07457194748523091\n",
      "train loss:0.18334699136645144\n",
      "train loss:0.07456735659208524\n",
      "train loss:0.05285673728520154\n",
      "train loss:0.08004126826050738\n",
      "train loss:0.046203909509837625\n",
      "train loss:0.20527546874339833\n",
      "train loss:0.06890807729405266\n",
      "train loss:0.06710382206154089\n",
      "train loss:0.10989140452969502\n",
      "train loss:0.06845585378298692\n",
      "train loss:0.10968080661003111\n",
      "train loss:0.12894948593772457\n",
      "train loss:0.03995857597545064\n",
      "train loss:0.10990963460505082\n",
      "train loss:0.07818279351135242\n",
      "train loss:0.10697613970161383\n",
      "train loss:0.05250378644067215\n",
      "train loss:0.1308763448029457\n",
      "train loss:0.10805340934652176\n",
      "train loss:0.1101115724715888\n",
      "train loss:0.11531334557988814\n",
      "train loss:0.12170900281672843\n",
      "train loss:0.034118883762124845\n",
      "train loss:0.1853777878815193\n",
      "train loss:0.0546270685517303\n",
      "train loss:0.06260928485922658\n",
      "train loss:0.10870365243694581\n",
      "train loss:0.16264484782864547\n",
      "train loss:0.07246784407949798\n",
      "train loss:0.044528396229108175\n",
      "train loss:0.13673611638459818\n",
      "train loss:0.07525624114755083\n",
      "train loss:0.05350666748872333\n",
      "train loss:0.08499820587156262\n",
      "train loss:0.14034100891343007\n",
      "train loss:0.11543042297981006\n",
      "train loss:0.145130026073405\n",
      "train loss:0.07480525492850862\n",
      "train loss:0.04538108661901784\n",
      "train loss:0.08491030111050961\n",
      "train loss:0.13963768185993797\n",
      "train loss:0.07243310995575003\n",
      "train loss:0.08602503201430323\n",
      "train loss:0.10479301648167996\n",
      "train loss:0.15187938708366613\n",
      "train loss:0.04847671943414231\n",
      "train loss:0.1997324205042936\n",
      "train loss:0.04545268841957701\n",
      "train loss:0.08194920434939505\n",
      "train loss:0.08523545491237915\n",
      "train loss:0.13522591086651747\n",
      "train loss:0.07482229743597663\n",
      "train loss:0.11431548316898024\n",
      "train loss:0.08521076574931082\n",
      "train loss:0.03955998961266109\n",
      "train loss:0.11941200921733384\n",
      "train loss:0.11939729303590586\n",
      "train loss:0.16125193063610915\n",
      "train loss:0.06810167166986357\n",
      "train loss:0.11762068066725512\n",
      "train loss:0.08491323184821242\n",
      "train loss:0.12244999236247635\n",
      "train loss:0.041784574780778204\n",
      "train loss:0.04613941650010408\n",
      "train loss:0.09595533674310298\n",
      "train loss:0.09211309034542525\n",
      "train loss:0.08058908249336033\n",
      "train loss:0.04629434944387592\n",
      "train loss:0.05896456465077153\n",
      "train loss:0.09640187227775639\n",
      "train loss:0.14232637704499804\n",
      "train loss:0.04847858636768532\n",
      "train loss:0.0704482972243936\n",
      "train loss:0.09784357836102492\n",
      "train loss:0.12161591583452416\n",
      "train loss:0.06700086736080994\n",
      "train loss:0.03557651228014317\n",
      "train loss:0.040865405289483285\n",
      "train loss:0.05796374808819229\n",
      "train loss:0.06524332460300968\n",
      "train loss:0.06906622747028572\n",
      "train loss:0.03616681686606579\n",
      "train loss:0.10953610782143913\n",
      "train loss:0.029228167208208547\n",
      "train loss:0.21276228227100624\n",
      "train loss:0.0452794674101652\n",
      "train loss:0.10488121091739491\n",
      "train loss:0.09708897578580396\n",
      "train loss:0.12550693002632166\n",
      "train loss:0.12631299846899938\n",
      "train loss:0.06672913286856441\n",
      "train loss:0.06697660479026815\n",
      "train loss:0.21032311836760129\n",
      "train loss:0.06681901389180413\n",
      "train loss:0.11612467070099267\n",
      "train loss:0.12676353798872764\n",
      "train loss:0.08189295128545895\n",
      "train loss:0.10319790082698582\n",
      "train loss:0.16911614190002833\n",
      "train loss:0.12924588374743698\n",
      "train loss:0.09987360714177583\n",
      "train loss:0.04146683879162617\n",
      "train loss:0.08843179235539246\n",
      "train loss:0.0772129580886953\n",
      "train loss:0.10253254400581624\n",
      "train loss:0.13496696915957454\n",
      "train loss:0.06940287440509998\n",
      "train loss:0.03238927434293903\n",
      "train loss:0.02215021920784695\n",
      "train loss:0.1516239643597916\n",
      "train loss:0.11556730273829206\n",
      "train loss:0.059229670226991514\n",
      "train loss:0.0892094810480814\n",
      "train loss:0.03229775755883822\n",
      "train loss:0.10680421987952067\n",
      "train loss:0.0728498779684673\n",
      "train loss:0.04976031456355604\n",
      "train loss:0.06863627627476535\n",
      "train loss:0.20563509416075226\n",
      "train loss:0.09309305515240501\n",
      "train loss:0.0786522725642865\n",
      "train loss:0.07868238859765543\n",
      "train loss:0.08601777637207178\n",
      "train loss:0.0579077535652168\n",
      "train loss:0.17449103447504033\n",
      "train loss:0.1423919024554336\n",
      "train loss:0.0678441892518997\n",
      "train loss:0.05787175884508003\n",
      "train loss:0.11595023313741087\n",
      "train loss:0.024359560759122274\n",
      "train loss:0.11894921940368842\n",
      "train loss:0.16651506157611032\n",
      "train loss:0.10194719576932562\n",
      "train loss:0.08435375383837848\n",
      "train loss:0.055622964956986515\n",
      "train loss:0.05136959304545648\n",
      "train loss:0.06858508030531743\n",
      "train loss:0.06635471297972792\n",
      "train loss:0.2169046704252556\n",
      "train loss:0.07599266832643922\n",
      "train loss:0.10455822719088902\n",
      "train loss:0.0683400323718954\n",
      "train loss:0.04024144226328201\n",
      "train loss:0.20449683126872922\n",
      "train loss:0.048401080160479414\n",
      "train loss:0.04375504967238825\n",
      "train loss:0.10912367200189976\n",
      "train loss:0.08593957464053387\n",
      "train loss:0.04175245079924512\n",
      "train loss:0.06252509061698659\n",
      "train loss:0.11105504599516962\n",
      "train loss:0.09327129996437039\n",
      "train loss:0.061845985732056796\n",
      "train loss:0.05582535391037906\n",
      "train loss:0.11473161205515597\n",
      "train loss:0.05341531959919436\n",
      "train loss:0.07341098533066744\n",
      "train loss:0.12664081968283145\n",
      "train loss:0.1024347084723843\n",
      "train loss:0.04013549039209128\n",
      "train loss:0.08777570582801589\n",
      "train loss:0.06463911926217535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.08144371724151628\n",
      "train loss:0.04563351948337208\n",
      "train loss:0.0854731594709568\n",
      "train loss:0.15320281741844768\n",
      "train loss:0.06770754204309665\n",
      "train loss:0.036901392584924135\n",
      "train loss:0.09776767672249521\n",
      "train loss:0.11493847614119374\n",
      "train loss:0.10575830278740943\n",
      "train loss:0.04041905246864463\n",
      "train loss:0.06538029670177262\n",
      "train loss:0.04879025994418716\n",
      "train loss:0.08210792018497085\n",
      "train loss:0.12765956780643403\n",
      "train loss:0.06632330184505757\n",
      "train loss:0.09504895735923605\n",
      "train loss:0.12550143136829142\n",
      "train loss:0.12481284310582552\n",
      "train loss:0.0786628743285753\n",
      "train loss:0.3286992991388129\n",
      "train loss:0.12235427756876358\n",
      "train loss:0.06327038222507285\n",
      "train loss:0.032908198645215554\n",
      "train loss:0.032129520332219844\n",
      "train loss:0.0927218076469005\n",
      "train loss:0.07836568943061396\n",
      "train loss:0.04593786155459967\n",
      "train loss:0.0709719513517328\n",
      "train loss:0.1444544409246463\n",
      "train loss:0.07489054544969839\n",
      "train loss:0.056432129944166\n",
      "train loss:0.07531995511726179\n",
      "train loss:0.10535219362472815\n",
      "train loss:0.049465367405159374\n",
      "train loss:0.06319895628569012\n",
      "train loss:0.025826070464620487\n",
      "train loss:0.05712890443950336\n",
      "train loss:0.05914994262657318\n",
      "train loss:0.09724161262497127\n",
      "train loss:0.05125052733522984\n",
      "train loss:0.1790106037929341\n",
      "train loss:0.0703770393073159\n",
      "train loss:0.07921791688049555\n",
      "train loss:0.12637089605305\n",
      "train loss:0.11546173461354516\n",
      "train loss:0.11916160790360339\n",
      "train loss:0.11117591312223433\n",
      "train loss:0.10091164232742764\n",
      "train loss:0.041056223321519654\n",
      "train loss:0.20506387165819218\n",
      "train loss:0.04978285148951793\n",
      "train loss:0.0647938081002117\n",
      "train loss:0.07221725714542213\n",
      "train loss:0.09184031323464922\n",
      "train loss:0.02809253012557046\n",
      "train loss:0.13170006966589723\n",
      "train loss:0.08232534203009356\n",
      "train loss:0.07664283778192506\n",
      "train loss:0.023297653061731118\n",
      "train loss:0.05059992273007211\n",
      "train loss:0.05823888991447013\n",
      "train loss:0.046401241472638806\n",
      "train loss:0.114926953051682\n",
      "train loss:0.12998193042338407\n",
      "train loss:0.0339386991091671\n",
      "train loss:0.04906718251686106\n",
      "train loss:0.15296905346952805\n",
      "train loss:0.08439392499762534\n",
      "train loss:0.0679872146714913\n",
      "train loss:0.08856649858755529\n",
      "train loss:0.09366087830235485\n",
      "train loss:0.13038666121361359\n",
      "train loss:0.060973435777719645\n",
      "train loss:0.10668390266370327\n",
      "train loss:0.0657483608846139\n",
      "train loss:0.03208676632405869\n",
      "train loss:0.05311062892104244\n",
      "train loss:0.11341097371195337\n",
      "train loss:0.05556835900180909\n",
      "train loss:0.03911454477383871\n",
      "train loss:0.04640707796661685\n",
      "train loss:0.06000464076540584\n",
      "train loss:0.12150827879904891\n",
      "train loss:0.07984838182943568\n",
      "train loss:0.11286775408160968\n",
      "train loss:0.12092471895741162\n",
      "train loss:0.08637773797607616\n",
      "train loss:0.07958771533007875\n",
      "train loss:0.0624333101450641\n",
      "train loss:0.06349181090346148\n",
      "train loss:0.07272758870368391\n",
      "train loss:0.11410389606294431\n",
      "train loss:0.0930267289971287\n",
      "train loss:0.034825945534887555\n",
      "train loss:0.1883399672906926\n",
      "train loss:0.05981400478178123\n",
      "train loss:0.058914515389168635\n",
      "train loss:0.07111410005930367\n",
      "train loss:0.11583577332961936\n",
      "train loss:0.10752469902385557\n",
      "train loss:0.07957854887089862\n",
      "train loss:0.0677010547404191\n",
      "train loss:0.09317248733645975\n",
      "train loss:0.11466448442736388\n",
      "train loss:0.06936792128748455\n",
      "train loss:0.0470680460702663\n",
      "train loss:0.048222045848858294\n",
      "train loss:0.1083678805947699\n",
      "train loss:0.11103458658683257\n",
      "train loss:0.07847253616303583\n",
      "train loss:0.10812377497632628\n",
      "train loss:0.12042905917092178\n",
      "train loss:0.033870455035305926\n",
      "train loss:0.07214759107694893\n",
      "train loss:0.09812942102682509\n",
      "train loss:0.12709288186421755\n",
      "train loss:0.03783986708699454\n",
      "train loss:0.03607954677873033\n",
      "train loss:0.03819237111839078\n",
      "train loss:0.11561235331333544\n",
      "train loss:0.08952996161118405\n",
      "train loss:0.10008158743856335\n",
      "train loss:0.1059226473276724\n",
      "train loss:0.042806582814722355\n",
      "train loss:0.09290797471454368\n",
      "train loss:0.06605194168754136\n",
      "train loss:0.055657941980274295\n",
      "train loss:0.11152497087215937\n",
      "train loss:0.06400548927532443\n",
      "train loss:0.09537589907498259\n",
      "train loss:0.09234771527198891\n",
      "train loss:0.07529560021742232\n",
      "train loss:0.05286882312055561\n",
      "train loss:0.09542422856304951\n",
      "train loss:0.08981731510354765\n",
      "train loss:0.06562661803860134\n",
      "train loss:0.09095397681146948\n",
      "train loss:0.13106407129096673\n",
      "train loss:0.05269047242307946\n",
      "train loss:0.08712516162899518\n",
      "train loss:0.0998685952860789\n",
      "train loss:0.04521325357546965\n",
      "train loss:0.047229365524066945\n",
      "train loss:0.0482329675155801\n",
      "train loss:0.07973226116549288\n",
      "train loss:0.048786801254793545\n",
      "train loss:0.04079805391841166\n",
      "train loss:0.09326603752052329\n",
      "train loss:0.04101387244891095\n",
      "train loss:0.08079284002745817\n",
      "train loss:0.08073765411791906\n",
      "train loss:0.04938534622461277\n",
      "train loss:0.04100238332712203\n",
      "train loss:0.07083872445888278\n",
      "train loss:0.09033616495214435\n",
      "train loss:0.045304415498501706\n",
      "train loss:0.13468529608872848\n",
      "train loss:0.13936163147191705\n",
      "train loss:0.037030674535678404\n",
      "train loss:0.09342886949866666\n",
      "train loss:0.039360162673336516\n",
      "train loss:0.1282856529336218\n",
      "train loss:0.11164969778761218\n",
      "train loss:0.0902328356401731\n",
      "train loss:0.05275246537231162\n",
      "train loss:0.04237857907984955\n",
      "train loss:0.06980106662429847\n",
      "train loss:0.0798382218108947\n",
      "train loss:0.13310356238288876\n",
      "train loss:0.07896069262506304\n",
      "train loss:0.09074252129861422\n",
      "train loss:0.07273691730126935\n",
      "train loss:0.15008659180621536\n",
      "train loss:0.047169124390251144\n",
      "train loss:0.10588395968609692\n",
      "train loss:0.10709350415127948\n",
      "train loss:0.11431115861168335\n",
      "train loss:0.08865104886089516\n",
      "train loss:0.022492201034446126\n",
      "train loss:0.053381414648149175\n",
      "train loss:0.052208967906103985\n",
      "train loss:0.08418238906247964\n",
      "train loss:0.11887799411099544\n",
      "train loss:0.07114708591952643\n",
      "train loss:0.10073738540698242\n",
      "train loss:0.08316982839336219\n",
      "train loss:0.08219619440545557\n",
      "train loss:0.08768833349164518\n",
      "train loss:0.11425481364694426\n",
      "train loss:0.05770223529078831\n",
      "train loss:0.05884244345605336\n",
      "train loss:0.09848125266993356\n",
      "train loss:0.034100832363496786\n",
      "train loss:0.10003132330005356\n",
      "train loss:0.04802977952593933\n",
      "train loss:0.054346697759386164\n",
      "train loss:0.06873777577225644\n",
      "train loss:0.06280280568258324\n",
      "train loss:0.07801742403755234\n",
      "train loss:0.12329190999464418\n",
      "train loss:0.13816262175881894\n",
      "=== epoch:3, train acc:0.975, test acc:0.975 ===\n",
      "train loss:0.15073147548784263\n",
      "train loss:0.02816151493197624\n",
      "train loss:0.06787698679795956\n",
      "train loss:0.0865176700968387\n",
      "train loss:0.04762248473898049\n",
      "train loss:0.06268052020551639\n",
      "train loss:0.03151076494281634\n",
      "train loss:0.16127157094213446\n",
      "train loss:0.06767324744081585\n",
      "train loss:0.03770907843942572\n",
      "train loss:0.042824818654949005\n",
      "train loss:0.10579164730691047\n",
      "train loss:0.05599229220159234\n",
      "train loss:0.07200526407252011\n",
      "train loss:0.06038401594073265\n",
      "train loss:0.10341200221564292\n",
      "train loss:0.04697910285258094\n",
      "train loss:0.08509373930275126\n",
      "train loss:0.04221395250111284\n",
      "train loss:0.06869485386222975\n",
      "train loss:0.06503587283358507\n",
      "train loss:0.03420176656206217\n",
      "train loss:0.031910714237735734\n",
      "train loss:0.06901932674685751\n",
      "train loss:0.05349254487244288\n",
      "train loss:0.09207989193658929\n",
      "train loss:0.13824033455923876\n",
      "train loss:0.1075899952609626\n",
      "train loss:0.06447039045299761\n",
      "train loss:0.14135184403545678\n",
      "train loss:0.03614900949088378\n",
      "train loss:0.09486834810894368\n",
      "train loss:0.05805543769983705\n",
      "train loss:0.06579280611667317\n",
      "train loss:0.02725880863692023\n",
      "train loss:0.16408613231058133\n",
      "train loss:0.10077005688354837\n",
      "train loss:0.04000130624792793\n",
      "train loss:0.04426156592648564\n",
      "train loss:0.13304080098695154\n",
      "train loss:0.03997100059571968\n",
      "train loss:0.14031674221938525\n",
      "train loss:0.09389819194195073\n",
      "train loss:0.02193787186183927\n",
      "train loss:0.061318745935498284\n",
      "train loss:0.04456713757188968\n",
      "train loss:0.13755182768502117\n",
      "train loss:0.07518467228082475\n",
      "train loss:0.10248799886038072\n",
      "train loss:0.1372039367119976\n",
      "train loss:0.1979656608770067\n",
      "train loss:0.12464744702611755\n",
      "train loss:0.08175993199841444\n",
      "train loss:0.06502763661716728\n",
      "train loss:0.03558187848328749\n",
      "train loss:0.036185034191446384\n",
      "train loss:0.039880961507798615\n",
      "train loss:0.04558318723278611\n",
      "train loss:0.09953204918924673\n",
      "train loss:0.12065562143954074\n",
      "train loss:0.07833928777375734\n",
      "train loss:0.029745083634343566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.06739570667001811\n",
      "train loss:0.07346769891197909\n",
      "train loss:0.06142238604473504\n",
      "train loss:0.061202070884606635\n",
      "train loss:0.01501573750336624\n",
      "train loss:0.04544226330514652\n",
      "train loss:0.044008713371870276\n",
      "train loss:0.02740032805714758\n",
      "train loss:0.032559001259954895\n",
      "train loss:0.1615259094519945\n",
      "train loss:0.06399948359778002\n",
      "train loss:0.04265467100575889\n",
      "train loss:0.04832525622891972\n",
      "train loss:0.055396125017352485\n",
      "train loss:0.10747495181586367\n",
      "train loss:0.06366055389630662\n",
      "train loss:0.09448693135269094\n",
      "train loss:0.06653011537513749\n",
      "train loss:0.08287229809128378\n",
      "train loss:0.15054377677812095\n",
      "train loss:0.185800744861391\n",
      "train loss:0.08515116313922547\n",
      "train loss:0.05653516663971778\n",
      "train loss:0.07080360804314853\n",
      "train loss:0.04582092815883436\n",
      "train loss:0.038614159462321565\n",
      "train loss:0.08732606258425527\n",
      "train loss:0.020073217189612434\n",
      "train loss:0.07418648283503247\n",
      "train loss:0.05395280102530204\n",
      "train loss:0.06975589644488832\n",
      "train loss:0.030351469964362977\n",
      "train loss:0.059950935574822686\n",
      "train loss:0.07526964535725793\n",
      "train loss:0.09149877954923501\n",
      "train loss:0.033178402269179\n",
      "train loss:0.08683725360255018\n",
      "train loss:0.09644267300200828\n",
      "train loss:0.049906306384869065\n",
      "train loss:0.10608446960692834\n",
      "train loss:0.07610972683089585\n",
      "train loss:0.10455443837913436\n",
      "train loss:0.10130056070397452\n",
      "train loss:0.06947499731138781\n",
      "train loss:0.08553569724614644\n",
      "train loss:0.13418142775143263\n",
      "train loss:0.08083471460227372\n",
      "train loss:0.04181384268259417\n",
      "train loss:0.04937939462556487\n",
      "train loss:0.1089803615550793\n",
      "train loss:0.05070229160165087\n",
      "train loss:0.03795953183593552\n",
      "train loss:0.044665619749748114\n",
      "train loss:0.07710495670533932\n",
      "train loss:0.09526206583026085\n",
      "train loss:0.0459921423135777\n",
      "train loss:0.10142042004995896\n",
      "train loss:0.05468108304627838\n",
      "train loss:0.04253265066983498\n",
      "train loss:0.08512150492205468\n",
      "train loss:0.12896679517088114\n",
      "train loss:0.15273213936017976\n",
      "train loss:0.06220695033649511\n",
      "train loss:0.06698086128829435\n",
      "train loss:0.06255614769500967\n",
      "train loss:0.23846929588942223\n",
      "train loss:0.018252674642164176\n",
      "train loss:0.06378959644787402\n",
      "train loss:0.04556678581632372\n",
      "train loss:0.022495499080892743\n",
      "train loss:0.06714459119889646\n",
      "train loss:0.050238153195695474\n",
      "train loss:0.09176652618718527\n",
      "train loss:0.06165203768030083\n",
      "train loss:0.05485754324303424\n",
      "train loss:0.07463929912612631\n",
      "train loss:0.0830974630588833\n",
      "train loss:0.03848320306036646\n",
      "train loss:0.06807375882594527\n",
      "train loss:0.03828218915287045\n",
      "train loss:0.03219482456180392\n",
      "train loss:0.14531893358623174\n",
      "train loss:0.11976473066195184\n",
      "train loss:0.05050884023414814\n",
      "train loss:0.1263267451545084\n",
      "train loss:0.03418589491012011\n",
      "train loss:0.042015477461031146\n",
      "train loss:0.04532139759387532\n",
      "train loss:0.06771043474589768\n",
      "train loss:0.04260832135890966\n",
      "train loss:0.07034604650884302\n",
      "train loss:0.07991843193508696\n",
      "train loss:0.12782615193935198\n",
      "train loss:0.05345681946234283\n",
      "train loss:0.02708148851976962\n",
      "train loss:0.0739228550472775\n",
      "train loss:0.09218895952002025\n",
      "train loss:0.06581052734282992\n",
      "train loss:0.03499650123768161\n",
      "train loss:0.11636516627966052\n",
      "train loss:0.05393961277444225\n",
      "train loss:0.08235410923034124\n",
      "train loss:0.06260601735436715\n",
      "train loss:0.11220864652687755\n",
      "train loss:0.019413978373644246\n",
      "train loss:0.02624550178326235\n",
      "train loss:0.022165583252789344\n",
      "train loss:0.076794924799161\n",
      "train loss:0.0941933457243754\n",
      "train loss:0.09203487280044167\n",
      "train loss:0.04838496270437103\n",
      "train loss:0.08667823359161536\n",
      "train loss:0.02657091239961636\n",
      "train loss:0.04182512314530706\n",
      "train loss:0.053306377491748054\n",
      "train loss:0.044185559337739586\n",
      "train loss:0.029243885832702654\n",
      "train loss:0.035298606441325306\n",
      "train loss:0.08876767845855517\n",
      "train loss:0.05560273282274987\n",
      "train loss:0.09701415863486842\n",
      "train loss:0.07356414932211351\n",
      "train loss:0.09583633461073916\n",
      "train loss:0.036004635804007125\n",
      "train loss:0.09802384688860909\n",
      "train loss:0.06864217118891078\n",
      "train loss:0.07264317730635744\n",
      "train loss:0.029377036957039828\n",
      "train loss:0.10712388429486162\n",
      "train loss:0.020451314396204377\n",
      "train loss:0.0684551482276245\n",
      "train loss:0.05779831770527879\n",
      "train loss:0.016155215567790523\n",
      "train loss:0.03601333256446983\n",
      "train loss:0.024292489597534557\n",
      "train loss:0.01895810111028228\n",
      "train loss:0.13084641517986773\n",
      "train loss:0.07903205329032476\n",
      "train loss:0.03685136043356616\n",
      "train loss:0.05089589862537796\n",
      "train loss:0.08415095844388605\n",
      "train loss:0.03580544593843301\n",
      "train loss:0.038106930538736775\n",
      "train loss:0.0643029560289698\n",
      "train loss:0.12526821685180325\n",
      "train loss:0.0761520957023009\n",
      "train loss:0.11886501208190567\n",
      "train loss:0.0442080977171557\n",
      "train loss:0.08207380661417968\n",
      "train loss:0.08171302950371175\n",
      "train loss:0.020636681681417485\n",
      "train loss:0.033986067554889275\n",
      "train loss:0.016572129641680897\n",
      "train loss:0.14220398327390332\n",
      "train loss:0.1036933071928987\n",
      "train loss:0.12461381050457003\n",
      "train loss:0.051577325673043184\n",
      "train loss:0.07953070011555148\n",
      "train loss:0.10395408340274855\n",
      "train loss:0.04356998156180805\n",
      "train loss:0.04961801799297561\n",
      "train loss:0.07162745789142926\n",
      "train loss:0.08801586349191594\n",
      "train loss:0.08122047257766224\n",
      "train loss:0.06229865949043304\n",
      "train loss:0.12587529993292332\n",
      "train loss:0.03636379181967821\n",
      "train loss:0.06395488133829029\n",
      "train loss:0.01917039700403146\n",
      "train loss:0.0895524648968494\n",
      "train loss:0.034817801031705814\n",
      "train loss:0.05342105610598122\n",
      "train loss:0.06125791772577907\n",
      "train loss:0.04658868186269131\n",
      "train loss:0.02952469832153731\n",
      "train loss:0.040857969290851885\n",
      "train loss:0.07203279206820201\n",
      "train loss:0.06587036334352599\n",
      "train loss:0.08752737149165232\n",
      "train loss:0.07324306902163388\n",
      "train loss:0.05412166978572444\n",
      "train loss:0.13603239859725935\n",
      "train loss:0.019685931609906334\n",
      "train loss:0.11592546605130048\n",
      "train loss:0.11279906948963947\n",
      "train loss:0.03195145788159941\n",
      "train loss:0.06697179161160323\n",
      "train loss:0.06158931482755757\n",
      "train loss:0.03317164840943342\n",
      "train loss:0.06118362438317854\n",
      "train loss:0.06081452875650986\n",
      "train loss:0.0544953276807236\n",
      "train loss:0.09621075029412536\n",
      "train loss:0.08603949880390623\n",
      "train loss:0.1269395396882478\n",
      "train loss:0.032197797988083046\n",
      "train loss:0.04627098113022722\n",
      "train loss:0.03167948560255681\n",
      "train loss:0.0996646598058944\n",
      "train loss:0.034521248911472686\n",
      "train loss:0.0475531325919287\n",
      "train loss:0.07165146777842388\n",
      "train loss:0.07903039851615111\n",
      "train loss:0.060538832268912546\n",
      "train loss:0.023477690115189433\n",
      "train loss:0.047486742489891834\n",
      "train loss:0.051553485706779305\n",
      "train loss:0.028476994864313283\n",
      "train loss:0.03746305444023654\n",
      "train loss:0.02552467040412506\n",
      "train loss:0.0595205180142231\n",
      "train loss:0.08465162109025423\n",
      "train loss:0.03224505896280334\n",
      "train loss:0.06380458751960963\n",
      "train loss:0.027024464846046703\n",
      "train loss:0.036676000370323356\n",
      "train loss:0.09296789196350626\n",
      "train loss:0.0548630620831079\n",
      "train loss:0.06081596546060388\n",
      "train loss:0.06585854214319767\n",
      "train loss:0.039122652344808447\n",
      "train loss:0.036277201629729056\n",
      "train loss:0.03990545949472858\n",
      "train loss:0.0156508379940399\n",
      "train loss:0.031066982529087885\n",
      "train loss:0.06992315415522578\n",
      "train loss:0.06266636178550426\n",
      "train loss:0.05605489282707194\n",
      "train loss:0.09120212754959314\n",
      "train loss:0.04451224561129843\n",
      "train loss:0.05586554879466169\n",
      "train loss:0.06720754221387906\n",
      "train loss:0.07171152947609787\n",
      "train loss:0.08011650355119496\n",
      "train loss:0.01635799724106069\n",
      "train loss:0.0828710703161248\n",
      "train loss:0.06905493927045608\n",
      "train loss:0.054506659575938005\n",
      "train loss:0.09625108878653423\n",
      "train loss:0.018970827789733646\n",
      "train loss:0.0331987213052172\n",
      "train loss:0.056173030792577926\n",
      "train loss:0.044115338614754515\n",
      "train loss:0.036534998022053206\n",
      "train loss:0.1186914021949108\n",
      "train loss:0.03291045572490108\n",
      "train loss:0.10670400449077677\n",
      "train loss:0.019248056757258263\n",
      "train loss:0.05001668915151146\n",
      "train loss:0.027929076995901924\n",
      "train loss:0.05466904807011837\n",
      "train loss:0.10172188642904385\n",
      "train loss:0.046204044339372044\n",
      "train loss:0.04700620666961255\n",
      "train loss:0.04250356502450701\n",
      "train loss:0.06464588088492242\n",
      "train loss:0.06016885782013756\n",
      "train loss:0.05123660454353792\n",
      "train loss:0.05039312704446179\n",
      "train loss:0.051638309179765564\n",
      "train loss:0.12057220541747564\n",
      "train loss:0.021463410671424966\n",
      "train loss:0.060059528017338784\n",
      "train loss:0.031926894986061674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0353157156500307\n",
      "train loss:0.07953370881887034\n",
      "train loss:0.05734887587593739\n",
      "train loss:0.03904725848130141\n",
      "train loss:0.1110635452601844\n",
      "train loss:0.04509794190548035\n",
      "train loss:0.08185777457637856\n",
      "train loss:0.026385780709447863\n",
      "train loss:0.06886033577294572\n",
      "train loss:0.07911069402037597\n",
      "train loss:0.05962406874967665\n",
      "train loss:0.040963040980401086\n",
      "train loss:0.1161874781759099\n",
      "train loss:0.05592817447838043\n",
      "train loss:0.052054838690951274\n",
      "train loss:0.053492130967882606\n",
      "train loss:0.08741273781722726\n",
      "train loss:0.0249594828068618\n",
      "train loss:0.03625072023137363\n",
      "train loss:0.04455593223269713\n",
      "train loss:0.06655854033057036\n",
      "train loss:0.06147503247190548\n",
      "train loss:0.049727905699081955\n",
      "train loss:0.04368459869835616\n",
      "train loss:0.06780678837067625\n",
      "train loss:0.0306604822058104\n",
      "train loss:0.08518733676852813\n",
      "train loss:0.020304945179317784\n",
      "train loss:0.10218156471766092\n",
      "train loss:0.04741494070912932\n",
      "train loss:0.04535398272545715\n",
      "train loss:0.07842919861696769\n",
      "train loss:0.0437220054821148\n",
      "train loss:0.014084269821719342\n",
      "train loss:0.061881014760769494\n",
      "train loss:0.06579197989565344\n",
      "train loss:0.07615096532665837\n",
      "train loss:0.025216899949092226\n",
      "train loss:0.03677258518877762\n",
      "train loss:0.034760091748502565\n",
      "train loss:0.09273373144333087\n",
      "train loss:0.0206236841290645\n",
      "train loss:0.038061144976580155\n",
      "train loss:0.020701614035867413\n",
      "train loss:0.030430200027957384\n",
      "train loss:0.028467384311644993\n",
      "train loss:0.02737975202585409\n",
      "train loss:0.024874980011756555\n",
      "train loss:0.04158374480433567\n",
      "train loss:0.04654152885080303\n",
      "train loss:0.08408905874799076\n",
      "train loss:0.03829211545083395\n",
      "train loss:0.027763777811659025\n",
      "train loss:0.0338052688664098\n",
      "train loss:0.03250655324144093\n",
      "train loss:0.05138285825627809\n",
      "train loss:0.06258177522532025\n",
      "train loss:0.027333787230597074\n",
      "train loss:0.04004505057635911\n",
      "train loss:0.15056575343355838\n",
      "train loss:0.07800231900751711\n",
      "train loss:0.018097395867303575\n",
      "train loss:0.07489944057514673\n",
      "train loss:0.032177474085571076\n",
      "train loss:0.04576300731320724\n",
      "train loss:0.05865739607929952\n",
      "train loss:0.029718276330816296\n",
      "train loss:0.02494590888394283\n",
      "train loss:0.07444826904586384\n",
      "train loss:0.050244618656066255\n",
      "train loss:0.04143936039686147\n",
      "train loss:0.02047302593095721\n",
      "train loss:0.03498441015657979\n",
      "train loss:0.0539855279916056\n",
      "train loss:0.014816051363010058\n",
      "train loss:0.06786522792551412\n",
      "train loss:0.03570409128187623\n",
      "train loss:0.04955006903569186\n",
      "train loss:0.07043826113418904\n",
      "train loss:0.02497030266608185\n",
      "train loss:0.0984098431376986\n",
      "train loss:0.058590751356334896\n",
      "train loss:0.019411469836241625\n",
      "train loss:0.044035160190328826\n",
      "train loss:0.04220802271889391\n",
      "train loss:0.03463948184280392\n",
      "train loss:0.038359547307433406\n",
      "train loss:0.0622448150941147\n",
      "train loss:0.01206141271766686\n",
      "train loss:0.07728118062980865\n",
      "train loss:0.07459052823393716\n",
      "train loss:0.1291020413335335\n",
      "train loss:0.039600420372761395\n",
      "train loss:0.03277204640453726\n",
      "train loss:0.04251894950366868\n",
      "train loss:0.029298675586611433\n",
      "train loss:0.04104531451841852\n",
      "train loss:0.021191467825047588\n",
      "train loss:0.039672137640119516\n",
      "train loss:0.02426318443367218\n",
      "train loss:0.022399462101302704\n",
      "train loss:0.03670888759383236\n",
      "train loss:0.06356585272337717\n",
      "train loss:0.034920143005566985\n",
      "train loss:0.17614684138859352\n",
      "train loss:0.08613899101944968\n",
      "train loss:0.012871107335642888\n",
      "train loss:0.05511845065806114\n",
      "train loss:0.08475405512503234\n",
      "train loss:0.13674003315232294\n",
      "train loss:0.01650113074434522\n",
      "train loss:0.08961937482865864\n",
      "train loss:0.19999361270766186\n",
      "train loss:0.04376813154427408\n",
      "train loss:0.07373213506948777\n",
      "train loss:0.05890343930499107\n",
      "train loss:0.03272724807091834\n",
      "train loss:0.040763033082566326\n",
      "train loss:0.07180594532222077\n",
      "train loss:0.08908872914621813\n",
      "train loss:0.09181976848740256\n",
      "train loss:0.05265263187053451\n",
      "train loss:0.08203143822640252\n",
      "train loss:0.04508450554261652\n",
      "train loss:0.0883678482837209\n",
      "train loss:0.06938437897853741\n",
      "train loss:0.009450419689351432\n",
      "train loss:0.019977395744501754\n",
      "train loss:0.07425655955903468\n",
      "train loss:0.05411639984511316\n",
      "train loss:0.058931509850013004\n",
      "train loss:0.0736658893024225\n",
      "train loss:0.029091593557029182\n",
      "train loss:0.05110176564954817\n",
      "train loss:0.04543532672620961\n",
      "train loss:0.10626104611643239\n",
      "train loss:0.07512598838304957\n",
      "train loss:0.02855955672919536\n",
      "train loss:0.03864309958297953\n",
      "train loss:0.04670764082341499\n",
      "train loss:0.08797543410727306\n",
      "train loss:0.11074198649572763\n",
      "train loss:0.06133271628690405\n",
      "train loss:0.03954204469797642\n",
      "train loss:0.018800199965715397\n",
      "train loss:0.021339780072271813\n",
      "train loss:0.0380631904483374\n",
      "train loss:0.08624118118710969\n",
      "train loss:0.030394232458594236\n",
      "train loss:0.05066567919953312\n",
      "train loss:0.048470620518883685\n",
      "train loss:0.04713061409370142\n",
      "train loss:0.07092657512706177\n",
      "train loss:0.04137890121616953\n",
      "train loss:0.06880999668436517\n",
      "train loss:0.05261691486750226\n",
      "train loss:0.025877973390055944\n",
      "train loss:0.06758907991010939\n",
      "train loss:0.02916100510100885\n",
      "train loss:0.05492521319116539\n",
      "train loss:0.033133612913704875\n",
      "train loss:0.028973922089552837\n",
      "train loss:0.032670618461710994\n",
      "train loss:0.04813756651105107\n",
      "train loss:0.06449617580472057\n",
      "train loss:0.016937425779977772\n",
      "train loss:0.03771825017604388\n",
      "train loss:0.025018521041299157\n",
      "train loss:0.0817939411467309\n",
      "train loss:0.025423421715265123\n",
      "train loss:0.03977319468663017\n",
      "train loss:0.03360450179373413\n",
      "train loss:0.025284364298071408\n",
      "train loss:0.04214307557445244\n",
      "=== epoch:4, train acc:0.981, test acc:0.978 ===\n",
      "train loss:0.07019878082082037\n",
      "train loss:0.03136319011756835\n",
      "train loss:0.017415800008470882\n",
      "train loss:0.0770633229168232\n",
      "train loss:0.08735965559723405\n",
      "train loss:0.07243824636688292\n",
      "train loss:0.03591488953923169\n",
      "train loss:0.020848047049376418\n",
      "train loss:0.09584851234891387\n",
      "train loss:0.029982971685769723\n"
     ]
    }
   ],
   "source": [
    "## simple convolutional network(cnn)(chap07)\n",
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 5\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=120,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0.9, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9f81f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep convolutional Network(chap08)\n",
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.deep_convnet import DeepConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "network.load_params(\"params.pkl\")\n",
    "\n",
    "from tkinter import *\n",
    "from PIL import ImageTk, Image\n",
    "from tkinter import filedialog\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "window = Tk()\n",
    "window.title('숫자 예측하기')\n",
    "    \n",
    "oldx = oldy = -1\n",
    "\n",
    "def on_mouse(event, x, y, flags, param):\n",
    "    global oldx, oldy\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        oldx, oldy = x, y\n",
    "        # print('EVENT_LBUTTONDOWN: %d, %d' % (x, y))\n",
    "\n",
    "    # elif event == cv2.EVENT_LBUTTONUP:\n",
    "        # print('EVENT_LBUTTONUP: %d, %d' % (x, y))\n",
    "\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "            cv2.line(img, (oldx, oldy), (x, y), 0, 5, cv2.LINE_AA)\n",
    "            cv2.imshow('image', img)\n",
    "            oldx, oldy = x, y\n",
    "\n",
    "def crt():\n",
    "    global img, tmp_img\n",
    "    img = np.ones((280, 280), dtype=np.uint8) * 255\n",
    "\n",
    "    cv2.namedWindow('image')\n",
    "    cv2.setMouseCallback('image', on_mouse, img)\n",
    "\n",
    "    cv2.imshow('image', img)\n",
    "    cv2.waitKey(3000)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    img = cv2.resize(img, (28,28), interpolation=cv2.INTER_AREA)     # 28*28 resize\n",
    "    cv2.imwrite('tmp.png', img)\n",
    "    img = ~img  # invert\n",
    "    img=img.reshape(-1,1,28,28)\n",
    "    \n",
    "    labels_view=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "    y=network.predict(img)\n",
    "    pred_num=np.argmax(y)\n",
    "    result = \"my predict is %s\"%(labels_view[pred_num])\n",
    "    \n",
    "    tmp_img=Image.open('tmp.png')\n",
    "    tmp_img=ImageTk.PhotoImage(tmp_img)\n",
    "    \n",
    "    Label(window, text=\"파일경로: new\").grid(row=2) # 파일경로 view\n",
    "    Label(window, image=tmp_img).grid(row=3) #사진 view\n",
    "    Label(window, text=result).grid(row=4) # 예측 결과 출력    \n",
    "\n",
    "def open():\n",
    "    global my_image # 함수에서 이미지를 기억하도록 전역변수 선언 (안하면 사진이 안보임)\n",
    "    window.filename = filedialog.askopenfilename(initialdir='', title='파일선택', filetypes=(\n",
    "    ('png files', '*.png'), ('jpg files', '*.jpg'), ('all files', '*.*')))\n",
    " \n",
    "    Label(window, text=\"파일경로: \"+window.filename).grid(row=2) # 파일경로 view\n",
    "    \n",
    "    img = Image.open(window.filename)\n",
    "    my_image = ImageTk.PhotoImage(img)\n",
    "    \n",
    "    img=img.convert(\"L\")                         # gray 저장\n",
    "    img=np.invert(img)                           # 흑백을 반전\n",
    "    \n",
    "    # print(img.shape)                           # img의 shape 확인\n",
    "    img=img.reshape(-1, 1, 28, 28)\n",
    "    \n",
    "    Label(window, image=my_image).grid(row=3) #사진 view\n",
    "    \n",
    "    labels_view=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "    y=network.predict(img)\n",
    "    pred_num=np.argmax(y)\n",
    "    result = \"my predict is %s\"%(labels_view[pred_num])\n",
    "    Label(window, text=result).grid(row=4) # 예측 결과 출력\n",
    "    \n",
    "\n",
    "b_create=Button(window, text='그리기(아무키나 누르면 닫기)', command=crt).grid(row=0)\n",
    "b_open = Button(window, text='파일열기', command=open).grid(row=1)\n",
    "Label(window, text=\"파일 경로\").grid(row=2)\n",
    "Label(window).grid(row=3)\n",
    "Label(window, text=\"예측 결과\").grid(row=4)\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f2ef71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
